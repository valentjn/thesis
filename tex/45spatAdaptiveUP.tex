\longsection{%
  Hierarchization on Spatially Adaptive Sparse Grids with the
  Unidirectional Principle%
}{%
  Hierarchization with the Unidirectional Principle%
}
\label{sec:45spatAdaptiveUP}

In this final section of the chapter,
we study two algorithms based on the \up
for applying the linear operator $\linop$ on spatially adaptive sparse grids.
\todo{mention Vazipfl if published}



\subsection{%
  Iteratively Applying the Unidirectional Principle with Iterative Refinement%
}
\label{sec:451iterativeRefinement}

The first algorithm can be applied if two requirements are met:
\begin{itemize}
  \item
  The inverse $\linop^{-1}$ is known and can be applied fast.
  
  \item
  There is an operator $\linop'$
  that is ``sufficiently close'' to $\linop$ and can be applied fast.
\end{itemize}
For hierarchization with B-splines on sparse grids,
we choose $\linop$ to be the hierarchization
operator given in \cref{eq:hierarchizationSLE} and
$\linop'$ to be the \up directly applied on the
sparse grid.
Both of the assumptions are then satisfied,
as $\linop^{-1}$ is known
(interpolation matrix $\intpmat$ of basis function evaluations)
and $\linop'$ can be applied fast.
The \up $\linop'$ generally produces wrong
results for hierarchical B-splines due to missing coupling points.
However, especially for low B-spline degrees,
$\linop'$ does not deviate too much from the true operator $\linop$.
Below, we will specify a sufficient criterion for the ``closeness''.

\paragraph{Iterative refinement}

Under these assumptions, we can apply the procedure given in
\cref{alg:iterativeRefinement}.
The algorithm is equivalent to the well-known method of
\term{iterative refinement}, which has been developed to
stabilize the numerical solution of a linear system when solving
with rounding errors \cite{Higham02Accuracy}.
The operator $\linop'$ acts like a preconditioner,
which is why it is required to be close to $\linop$.
Note that the algorithm is similar to the repeated application
of the method of residual interpolation
(see \cref{sec:433residualInterpolation}) on the whole sparse grid.

\begin{algorithm}
  \begin{algorithmic}[1]
    \Function{$\vlinout =$ iterativeRefinement}{%
      $\vlinin$, $\vlinout[(0)]$%
    }
      \State{$\*r^{(0)} \gets \vlinin - \linop^{-1}[\vlinout[(0)]]$}
      \Comment{initial residual}%
      \For{$m = 0, 1, 2, \dotsc$}
        \State{$\vlinout[(m+1)] \gets \vlinout[(m)] + \linop' \*r^{(m)}$}
        \Comment{update solution}%
        \State{$\*r^{(m+1)} \gets \*r^{(m)} - \linop^{-1} \linop' \*r^{(m)}$}
        \Comment{update residual}%
      \EndFor{}
      \State{$\vlinout \gets \text{last computed } \vlinout[(m)]$}
    \EndFunction{}
  \end{algorithmic}
  \caption[%
    Iterative refinement%
  ]{%
    Application of a tensor product operator $\linop$
    on spatially adaptive sparse grids with iterative refinement,
    where $\linop'$ is an approximation of $\linop$.
    Inputs are the vector $\vlinin = (\linin{\*l,\*i})_{(\*l,\*i) \in \liset}$
    of input data (function values $\fcnval{\*l,\*i}$ at the grid points) and
    an initial solution $\vlinout[(0)]$.
    The output is the vector
    $\vlinout = (\linout{\*l,\*i})_{(\*l,\*i) \in \liset}$
    of output data (hierarchical surpluses $\surplus{\*l,\*i}$).%
  }%
  \label{alg:iterativeRefinement}%
\end{algorithm}

The loop in \cref{alg:iterativeRefinement} has to be terminated
after some iterations.
The following lemma allows to use a stopping criterion based on the
size of the residual $\*r^{(m)}$.

\begin{shortlemma}[equivalent convergence in \cref{alg:iterativeRefinement}]
  \label{lemma:iterativeRefinementEquivalent}
  In \cref{alg:iterativeRefinement}, we have
  $\vlinout[(m)] \to \vlinout[\ast] \iff \*r^{(m)} \to \*0$ for
  $m \to \infty$.
\end{shortlemma}

\begin{proof}
  It suffices to prove $\linop \*r^{(m)} = \vlinout[\ast] - \vlinout[(m)]$
  for $m \in \nat$ by induction.
  For $m = 0$, we have
  $\linop \*r^{(0)}
  = \linop \vlinin - \linop^{-1} \vlinout[(0)]
  = \vlinout[\ast] - \vlinout[(0)]$.
  For $m \to m+1$, it holds
  $\linop \*r^{(m+1)}
  = \linop \*r^{(m)} - \linop^{-1} \linop' \*r^{(m)}
  = (\vlinout[\ast] - \vlinout[(m)]) - \linop' \*r^{(m)}
  = \vlinout[\ast] - \vlinout[(m+1)]$.
\end{proof}

Next, we give a sufficient condition for the
convergence of \cref{alg:iterativeRefinement} to the true
solution, which we denote with $\vlinout[\ast] := \linop \vlinin$.

\begin{proposition}[%
  sufficient criterion for the convergence of \ref{alg:iterativeRefinement}%
]
  \label{prop:iterativeRefinementSufficient}
  If we have $\limsup_{m \to \infty}
  \sqrt[m]{\norm{(\idop - \linop^{-1} \linop')^m}} < 1$
  with an arbitrary operator matrix norm $\norm{\cdot}$ and the
  identity operator $\idop$,
  then $\vlinout[(m)] \to \vlinout[\ast]$ for $m \to \infty$
  in \cref{alg:iterativeRefinement}.
\end{proposition}

\begin{proof}
  A short induction proof shows that
  %the explicit representations of $\vlinout[(m)]$ and $\*r^{(m)}$:
  %\begin{subequations}
  %  \begin{align}
  %    \vlinout[(m)]
  %    &= \vlinout[(0)] + \linop' \cdot \sum_{m'=0}^m \*r^{(m')},\\
  %    \*r^{(m)}
  %    &= (\idop - \linop^{-1} \linop')^m \*r^{(0)},
  %  \end{align}
  %\end{subequations}
  %which implies
  \begin{equation}
    \label{eq:proofPropIterativeRefinementSufficient}
    \vlinout[(m)]
    = \vlinout[(0)] + \linop' \cdot
    \sum_{m'=0}^m (\idop - \linop^{-1} \linop')^{m'} \*r^{(0)},
  \end{equation}
  where $(\idop - \linop^{-1} \linop')^{m'} \*r^{(0)} = \*r^{(m')}$.
  For $m \to \infty$ and the assumption on
  $\norm{(\idop - \linop^{-1} \linop')^m}$,
  the sum converges to the Neumann series
  $\sum_{m'=0}^\infty (\idop - \linop^{-1} \linop')^{m'}
  = (\idop - (\idop - \linop^{-1} \linop'))^{-1} = (\linop')^{-1} \linop$
  (see, e.g., \cite{Werner11Funktionalanalysis}).
  In this case, we infer that the limit of $\vlinout[(m)]$ is given by
  \begin{equation}
    \vlinout[(0)] + \linop' \cdot (\linop')^{-1} \linop \*r^{(0)}
    = \vlinout[(0)] + \linop \vlinin - \linop^{-1} \vlinout[(0)]
    = \linop \vlinin
    = \vlinout[\ast],
  \end{equation}
  as claimed.
\end{proof}

The sufficient condition given in \cref{prop:iterativeRefinementSufficient}
is quite strong, as it can be shown that $\limsup_{m \to \infty}
\sqrt[m]{\norm{(\idop - \linop^{-1} \linop')^m}} \le 1$ is necessary for
convergence.
Unfortunately, in the case of hierarchization with B-splines,
numerical experiments showed that
this condition is only met for low dimensionalities $d$ and low
B-spline degrees $p$.
\Cref{alg:iterativeRefinement} generally diverges
for higher dimensionalities or higher degrees.



\subsection{Duality of the Unidirectional Principle}
\label{sec:452duality}

To find the second algorithm that we present in this section,
we study why we cannot directly apply the \up
(as introduced in \cref{sec:42fullGrids}) on spatially adaptive sparse grids.
As before, we denote with $\liset$ the level-index set of
the spatially adaptive sparse grid (see \cref{sec:41problem}).

The \up, as stated in
\cref{alg:unidirectionalPrinciple} for the full grid case,
subsequently applies one-dimensional operators
$\upopuv{t_j}{\lisetpole}\colon \real^{\setsize{\lisetpole}} \to
\real^{\setsize{\lisetpole}}$
on the poles $\lisetpole$ of the sparse grid at hand,
looping over a permutation $t_1, \dotsc, t_d$
of the dimensions $1, \dotsc, d$.
Let us recall the pole equivalence relation $\samepole{t_j}$
from \cref{eq:poleEquivalenceRelation}, defined on $\liset$:
\begin{equation}
  \*k' \samepole{t_j} \*k'' \iff \*k'_{-t_j} = \*k''_{-t_j},\quad
  \*k', \*k'' \in \liset.
\end{equation}
Two points $\*k', \*k'' \in \liset$ are equivalent with respect to
$\samepole{t_j}$, if $\*k'$ is contained in the pole through $\*k''$
with respect to the $t_j$-th dimension.

\paragraph{Operators for the unidirectional principle}

The applications of the one-dimensional operators
$\upopuv{t_j}{\lisetpole}$
($\lisetpole \in \eqclasses{\liset}{\samepole{t_j}}$)
in the $j$-th iteration of \cref{alg:unidirectionalPrinciple}
are equivalent to a single application of the following operator
$\upop{t_j}\colon \real^{\setsize{\liset}} \to \real^{\setsize{\liset}}$:
\begin{equation}
  \label{eq:upopEntries}
  (\upop{t_j})_{\*k'',\*k'}
  :=
  \begin{cases}
    (\upopuv{t_j}{\lisetpole})_{k''_{t_j},k'_{t_j}},&
    \ex{\lisetpole \in \eqclasses{\liset}{\samepole{t_j}}}{
      \*k', \*k'' \in \lisetpole
    },\\
    0,&\*k' \not\samepole{t_j} \*k'',
  \end{cases}
\end{equation}
where $(\upop{t_j})_{\*k'',\*k'}$ denotes the entry of row $\*k''$
and column $\*k'$ of the matrix corresponding to $\upop{t_j}$
(similar for $(\upopuv{t_j}{\lisetpole})_{k''_{t_j},k'_{t_j}}$).
This is because the poles $\lisetpole$ are pairwise disjoint
as equivalence classes.
Consequently, every point $\*k$ is only acted upon by a single
one-dimensional operator $\upopuv{t_j}{\lisetpole}$,
namely the one where $\lisetpole$ is the
pole $\eqclass{\*k}{\samepole{t_j}}$ through $\*k$.
This leads to the block-diagonal structure
of $\upop{t_j}$ given in \eqref{eq:upopEntries},
if the rows of the matrix of $\upop{t_j}$
are grouped by poles $\lisetpole$ and the columns are arranged accordingly.

\paragraph{Assumptions}

We make two assumptions on $\linop$ and $\upopuv{t_j}{\lisetpole}$
for the remaining considerations:
\begin{itemize}
  \item
  The operator $\linop$ has tensor product structure.
  This means that
  \begin{equation}
    \label{eq:tensorProductOperator}
    (\linop)_{\*k'',\*k'}
    =
    (\upopuv{t_1}{\eqclass{\chain{1}}{\samepole{t_1}}})_{k''_{t_1},k'_{t_1}}
    \dotsm
    (\upopuv{t_d}{\eqclass{\chain{d}}{\samepole{t_d}}})_{k''_{t_d},k'_{t_d}},
    \quad
    \*k', \*k'' \in \liset.
  \end{equation}
  
  \item
  The operators $\linop$ and are $\upopuv{t_j}{\lisetpole}$ invertible.
  In this case, $\upop{t_j}$ is also invertible and
  $\upopinv{t_j}$ is given by the block-diagonal matrix of
  the inverses of the blocks $\upopuv{t_j}{\lisetpole}$ of $\upop{t_j}$.
\end{itemize}
Both assumptions are satisfied by dehierarchization
operators $\intpmat$ due to the tensor product structure and the
linear independence of the hierarchical basis functions.
However, we formulate the following results for arbitrary
linear invertible tensor product operators $\linop$.

\paragraph{Correctness and duality of the unidirectional principle}

Now, we can describe the whole \up of
\cref{alg:unidirectionalPrinciple} as the operator
$\upop{t_1,\dotsc,t_d}\colon \real^{\setsize{\liset}} \to
\real^{\setsize{\liset}}$ given by:
\begin{equation}
  \label{eq:upopProduct}
  \upop{t_1,\dotsc,t_d}
  := \upop{t_d} \dotsm \upop{t_1}.
\end{equation}
The right-most operator is $\upop{t_1}$, since it is applied first.
We say that the \up is \term{correct} for $\linop$ and
$(t_1, \dotsc, t_d)$, if
\begin{equation}
  \upop{t_1,\dotsc,t_d}
  \overset{?}{=} \linop.
\end{equation}
Note that this relation is not satisfied in general,
especially for B-spline hierarchization with the operator
$\linop = \intpmatinv$.
However, for operators like these, whose inverse
$\linopinv = \intpmat$ can be much easier described and applied,
we can make use of the so-called \term{duality of the \up}:

\begin{lemma}[duality of the unidirectional principle]
  \label{lemma:dualityUnidirectionalPrinciple}
  The \up is correct for $\linop$ and $(t_1, \dotsc, t_d)$
  if and only if the \up is correct for $\linopinv$ and $(t_d, \dotsc, t_1)$.
\end{lemma}

\begin{proof}
  The correctness of the \up for $\linop$ and $(t_1, \dotsc, t_d)$
  is by definition equivalent to
  \begin{equation}
    \upop{t_d} \dotsm \upop{t_1} = \linop.
  \end{equation}
  By inverting both sides, we obtain the definition of the
  correctness of the \up for $\linopinv$ and $(t_d, \dotsc, t_1)$.
\end{proof}

The duality given in \cref{lemma:dualityUnidirectionalPrinciple}
means that in order to establish the correctness of $\linop$
for some arbitrary permutation $(t_1, \dotsc, t_d)$ of $1, \dotsc, d$,
it suffices to establish the \up's correctness for the
inverse operator $\linopinv$ and the reverse permutation $(t_d, \dotsc, t_1)$.
This is especially of interest for our main application,
the hierarchization operator $\linop = \intpmatinv$ for B-splines.



\subsection{Chains and Equivalent Correctness Conditions}
\label{sec:453chains}

Let us first define the notion of a chain between two grid points
$\*k'$ and $\*k''$.

\begin{definition}[chain]
  \label{def:chain}
  Let $\*k', \*k'' \in \liset$ and
  $(t_1, \dotsc, t_j)$ be a permutation of $j$ of the
  dimensions $1, \dotsc, d$.
  The \term{chain} from $\*k'$ to $\*k''$ with respect to
  $T := (t_1, \dotsc, t_j)$ is defined as the sequence
  $(\chain{0}, \dotsc, \chain{j})$, where
  \begin{equation}
    \chain{j'}_{T_{j'}}
    := \*k''_{T_{j'}},\quad
    \chain{j'}_{-T_{j'}}
    := \*k'_{-T_{j'}},\quad
    T_{j'}
    := (t_1, \dotsc, t_{j'}),\quad
    j' = 0, \dotsc, j,
    \hspace*{-10mm}
  \end{equation}
  if $\chain{j} = \*k''$ and
  $\chain{j'} \in \liset$ for all $j' = 0, \dotsc, j$.
\end{definition}

It is easy to see that this definition is equivalent to
$\chain{j'-1} \samepole{t_{j'}} \chain{j'}$ for $j' = 1, \dotsc, j$.
We now show two lemmas.
First, we prove that $(\upop{t_1,\dotsc,t_j})_{\*k'',\*k'} \not= 0$
is sufficient for the existence of a chain from $\*k'$ to $\*k''$:

\begin{restatable}[sufficient condition for chain existence]{%
  lemma%
}{%
  lemmaChainExistenceSufficient%
}
  \label{lemma:chainExistenceSufficient}
  If $(\upop{t_1,\dotsc,t_j})_{\*k'',\*k'} \not= 0$
  for some $j = 0, \dotsc, d$,
  then the grid $\liset$ contains the chain from $\*k'$ to $\*k''$
  with respect to $(t_1, \dotsc, t_j)$.
\end{restatable}

\begin{proof}
  See \cref{sec:proofCorrectnessUnidirectionalPrincipleSASG}.
\end{proof}

Second, we show that the equality of
$(\upop{t_1,\dotsc,t_j})_{\chain{j},\*k'}$ and the product of
the unidimensional operators is necessary for the
existence of a chain from $\*k'$ to $\*k''$:

\begin{restatable}[necessary condition for chain existence]{%
  lemma%
}{%
  lemmaChainExistenceNecessary%
}
  \label{lemma:chainExistenceNecessary}
  If the grid $\liset$ contains the chain $(\chain{0}, \dotsc, \chain{j})$
  from $\*k'$ to $\*k''$ with respect to $(t_1, \dotsc, t_j)$
  for some $j = 0, \dotsc, d$, then
  \begin{equation}
    \label{eq:lemmaChainExistenceNecessary}
    (\upop{t_1,\dotsc,t_j})_{\chain{j},\*k'}
    = (\upopuv{t_1}{\eqclass{\chain{1}}{\samepole{t_1}}})_{k''_{t_1},k'_{t_1}}
    \dotsm
    (\upopuv{t_j}{\eqclass{\chain{j}}{\samepole{t_j}}})_{k''_{t_j},k'_{t_j}}.
  \end{equation}
\end{restatable}

\begin{proof}
  See \cref{sec:proofCorrectnessUnidirectionalPrincipleSASG}.
\end{proof}

The two lemmas can now be used to prove the following characterization
of the correctness of the UP:

\begin{restatable}[characterization of the correctness of the UP]{%
  proposition%
}{%
  propCorrectnessUPCharacterization%
}
  \label{prop:correctnessUPCharacterization}
  The \up is correct for $\linop$ and $(t_1, \dotsc, t_d)$
  if and only if the grid $\liset$ contains the chain from $\*k'$ to $\*k''$
  with respect to $(t_1, \dotsc, t_d)$ for all $\*k', \*k'' \in \liset$
  for which $(\linop)_{\*k'',\*k'} \not= 0$.
\end{restatable}

\begin{proof}
  See \cref{sec:proofCorrectnessUnidirectionalPrincipleSASG}.
\end{proof}

When applied to the hierarchization operator,
the combination of \cref{prop:correctnessUPCharacterization} with
\thmref{lemma:dualityUnidirectionalPrinciple} can be summarized in
the following corollary:

\begin{corollary}[%
  equivalent statements for correctness of UP for hierarchization%
]
  \label{cor:equivalentCorrectnessUPHierarchization}
  The following statements are equivalent:
  \begin{itemize}
    \item
    The \up is correct for $\intpmatinv$ and $(t_1, \dotsc, t_d)$.
    
    \item
    The \up is correct for $\intpmat$ and $(t_d, \dotsc, t_1)$.
    
    \item
    The grid $\liset$ contains the chain from $\*k'$ to $\*k''$
    with respect to $(t_1, \dotsc, t_d)$ for all $\*k', \*k'' \in \liset$
    for which $\basis{\*k'}(\gp{\*k''}) \not= 0$.
  \end{itemize}
\end{corollary}

\begin{proof}
  The corollary is a direct consequence of
  \cref{lemma:dualityUnidirectionalPrinciple} and
  \cref{prop:correctnessUPCharacterization},
  applied to the dehierarchization operator $\linop = \intpmat$.
\end{proof}

\paragraph{Inserting chain points}

This means that we can establish the correctness of the \up
for the hierarchization operator $\linop = \intpmatinv$,
if we insert all missing chain points that are specified by
\cref{prop:correctnessUPCharacterization} into the grid.
As it is necessary to insert these points recursively
(e.g., the inserted points may generate new chains,
for which other missing points have to be inserted),
the number of points to be inserted can be very large.
The worst case is that the final grid is a full grid, i.e.,
the Cartesian product of the union of the poles in the different dimensions:
\begin{equation}
  \left(\bigcup_{\*k \in \liset} \eqclass{\*k}{\samepole{1}}\right)
  \times \dotsb \times
  \left(\bigcup_{\*k \in \liset} \eqclass{\*k}{\samepole{d}}\right).
\end{equation}
In this case, we lose the advantage of sparse grids,
whose goal is to ease the curse of dimensionality.
For the standard hierarchical B-spline basis $\bspl{l,i}{p}$,
this worst case often occurs as there are many non-zero entries
in the corresponding interpolation matrices $\intpmat$
(see \cref{sec:41problem}).



\subsection{Weakly Fundamental Splines}
\label{sec:454wfs}

\paragraph{Motivation}

In order to reduce the number of chain points to be inserted,
we have to use other spline bases such that
the resulting interpolation matrices $\intpmat$ have more zero entries.
One possibility are the hierarchical fundamental splines
as introduced in \cref{sec:443fundamentalSplines}.
However, they are globally supported, which implies a number
of disadvantages concerning algorithms and implementations.
The most significant disadvantage is that although
we can use \bfs for the univariate interpolation operators
(see \cref{sec:441BFSFundamentalBases}),
the time complexity for the univariate interpolation is still quadratic.
We now want to construct a locally supported spline basis for which
the univariate interpolation can be done in linear time.

To meet these goals, we have to relax the fundamental property
to a weaker version, which results in the so-called
\term{weakly fundamental property}.
A univariate hierarchical basis
$\wfundbasis{l',i'}\colon \clint{0, 1} \to \real$
is called \term{weakly fundamental}, if
\begin{equation}
  \label{eq:weaklyFundamentalProperty}
  \wfundbasis{l',i'}(\gp{l,i}) = 0,\quad
  l < l',\;\;
  i \in \hiset{l}.
\end{equation}
This is exactly the first condition \eqref{eq:fundamentalProperty1}
of the fundamental property \eqref{eq:fundamentalProperty}.
In other words, we drop the requirement that the basis functions
should vanish at the other grid points of the same level.
The relation \eqref{eq:fundamentalPropertyImplicationMV} from the
fundamental case becomes
\begin{equation}
  \label{eq:weaklyFundamentalPropertyImplicationMV}
  \wfundbasis{l',i'}(\gp{\*l,\*i})
  \not= 0
  \implies
  \*l' \le \*l,
\end{equation}
i.e., every basis function $\wfundbasis{l',i'}$
can only be non-zero at grid points $\gp{\*l,\*i}$ with
finer or equal level $\*l$.

\paragraph{Definition of hierarchical weakly fundamental splines}

We construct the \term{weakly fundamental spline parent function}
$\parentwfundspl{p}\colon \real \to \real$
by forming a linear combination of as few neighboring
uniform B-splines as possible such that $\parentwfundspl{p}$
satisfies the weakly fundamental property
\eqref{eq:weaklyFundamentalProperty}:
\begin{subequations}
  \begin{gather}
    \parentwfundspl{p}(x)
    := \sum_{k=-(p-1)/2}^{(p-1)/2}
    \wfundsplcoeff{k}{p} \parentbspl{p}(x - k)
    \quad\text{such that}\\
    \wfundsplcoeff{0}{p} = 1,\quad
    \parentwfundspl{p}(k') = 0,\;\;
    k' = -p + 2,\; -p + 4,\; \dotsc,\; p - 2.
  \end{gather}
\end{subequations}
\usenotation{Ã‹wfs}
The parent function canonically defines
\term{hierarchical weakly fundamental splines}
$\bspl[\wfs]{l,i}{p}\colon \clint{0, 1} \to \real$
via an affine parameter transformation
\begin{equation}
  \bspl[\wfs]{l,i}{p}(x)
  := \parentwfundspl{p}(\tfrac{x}{\ms{l}} - i),\quad
  l \ge 1.
\end{equation}
For $l = 0$, we define $\bspl[\wfs]{l,i}{p}$ to be the
linear Lagrange polynomial of level $0$, i.e.,
\begin{equation}
  \bspl[\wfs]{0,i}{p}
  := \lagrangepoly{0,i},\quad
  i = 0, 1,
\end{equation}
to simplify the description of the
Hermite hierarchization algorithm in \cref{sec:455hermiteHierarchization}.
Note that the hierarchical weakly fundamental splines are,
starting with level $l \ge 1$,
translation-invariant by construction.
%(see \cref{eq:translationInvariance}).
As the weakly fundamental parent spline $\parentwfundspl{p}$
vanishes at all even integers and as support of $\bspl[\wfs]{l,i}{p}$ is local
($\supp \bspl[\wfs]{l,i}{p}
= \clint{\gp{l,i-p}, \gp{l,i+p}} \cap \clint{0, 1}$),
this implies that the weakly fundamental property
\eqref{eq:weaklyFundamentalProperty} is fulfilled.

\paragraph{Chain points for weakly fundamental splines}

The first advantage of the
weakly fundamental spline basis $\bspl[\wfs]{l,i}{p}$
over the standard uniform B-splines $\bspl{l,i}{p}$ is that
the condition $\basis{\*k'}(\gp{\*k''}) \not= 0$ in
\cref{cor:equivalentCorrectnessUPHierarchization} will be
satisfied much more rarely.
Consequently, fewer chain grid points have to be inserted to
ensure the correctness of the \up for hierarchization.

In the special case of regular sparse grids $\regsgset{n}{d}$,
%(i.e., $\regsgspace[\*p,\wfs]{n}{d}$ defined analogously to
%$\regsgspace{n}{d}$),
we do not have to insert any grid points for the correctness of the \up.
We can verify this statement with
\thmref{cor:equivalentCorrectnessUPHierarchization}:
Let $(\*l',\*i')$ and $(\*l'',\*i'')$ with
$\normone{\*l'}, \normone{\*l''} \le n$ and
$\*i' \in \hiset{\*l'}$, $\*i'' \in \hiset{\*l''}$,
such that $\bspl[\wfs]{\*l',\*i'}{\*p}(\gp{\*l'',\*i''}) \not= 0$.
Furthermore, let $((\chain[\*l]{0}, \chain[\*i]{0}), \dotsc,
(\chain[\*l]{d}, \chain[\*i]{d}))$ be the chain
from $\*k'$ to $\*k''$ with respect to $t_1, \dotsc, t_d$.
Note that $\chain[\*l]{j} \le \vecmax\{\*l', \*l''\}$ due to the
definition of chain points (\cref{def:chain}).
Therefore, we have for $j = 0, \dotsc, d$
by \eqref{eq:weaklyFundamentalPropertyImplicationMV}
\begin{equation}
  \*l' \le \*l''
  \implies
  \chain[\*l]{j} \le \vecmax\{\*l', \*l''\} \le \*l''
  \implies
  \normone{\chain[\*l]{j}} \le \normone{\*l''} \le n.
\end{equation}
Hence, $\regsgset{n}{d}$ contains the grid points corresponding to
$(\chain[\*l]{j}, \chain[\*i]{j})$ for all $j = 0, \dotsc, d$.
Consequently, the conditions of
\cref{cor:equivalentCorrectnessUPHierarchization} are satisfied without
inserting any additional chain points.
This statement is even valid for arbitrary
dimensionally adaptive sparse grids.



\subsection{Hermite Hierarchization}
\label{sec:455hermiteHierarchization}

\blindtext{}




\subsection{Hierarchical Weakly Fundamental Not-A-Knot Splines}
\label{sec:456wfsNotAKnot}

\blindtext{}
