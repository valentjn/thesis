\section{Homogenization and the Two-Scale Approach}
\label{sec:61homogenization}

We roughly follow the presentation given in
\multicite{%
  Huebner14Mehrdimensionale,%
  Valentin14Hierarchische,%
  Valentin16Hierarchical%
}.

\subsection{Homogenization}
\label{sec:611homogenization}

\paragraph{Density function}

Let $\domain \subset \real^{\dimdomain}$ be the optimization domain
(which later contains the optimal shape) and
$\dens\colon \domain \to \clint{0, 1}$ a function,
which is called \term{density function}.
Usually, we assume $\dimdomain = 2$ or $\dimdomain = 3$,
although the method can be generalized to
arbitrary dimensionalities $\dimdomain \in \nat$.
The function values $\dens(\tilde{\*x})$ tell if $\tilde{\*x}$
is contained in the object (value of one) or not (value of zero).
The approach of \term{homogenization} now also allows values between
zero and one, giving the density of the material in that point.

\paragraph{Optimization of compliance values}

Furthermore, for each density function $\dens$,
let $\compliance(\dens)$ be an objective function value.
In our setting, which is shown in \cref{fig:topoOptExampleScenario},
we exert a force $\force$ (or multiple forces),
measure the resulting deformation of the object, and
compute the \term{compliance} (i.e., the inverse of the stiffness) as
the objective function value $\compliance(\dens)$:
\begin{equation}
  \compliance(\dens)
  = \int_{\domain} \innerprod[2]{\force}{\displacement_{\dens}(\tilde{\*x})}
  \diff\tilde{\*x},
\end{equation}
where $\displacement_{\dens}\colon \domain \to \real^{\dimdomain}$
is the \term{displacement function}, depending on the density
\cite{Huebner14Mehrdimensionale}.
The problem of topology optimization is to find the density function
with minimal objective function value:
\begin{equation}
  \label{eq:topoOptProblemContinuous}
  \min_{\dens} \compliance(\dens).
\end{equation}
Often, there are trivial solutions, if we do not impose additional conditions.
For example, when minimizing compliance function values,
choosing $\dens :\equiv 1$ will most likely lead to the shape with the
highest stiffness and, thus, the smallest displacement and compliance value.
Therefore, we introduce the following volume constraint:
\begin{equation}
  \frac{\voldens{\dens}{\domain}}{\vol{\domain}} \le \densub,\quad
  \voldens{\dens}{\domain}
  := \int_{\domain} \dens(\tilde{\*x}) \diff\tilde{\*x},\quad
  \vol{\domain}
  := \voldens{1}{\domain},
\end{equation}
where $\vol{\domain} = \int_{\domain} 1 \diff\tilde{\*x}$
is the volume of the domain and
$\densub \in \clint{0, 1}$ is an upper bound on the volume fraction.

\begin{SCfigure}
  \includegraphics{topoOptScenarios_1}%
  \caption[%
    Example scenario for topology optimization%
  ]{%
    Example scenario for topology optimization.
    An object \emph{\textcolor{hellblau}{(light blue)}}
    is fixed on the left side
    of the domain $\domain$
    \emph{\textcolor{mittelblau!50}{(darker blue)}}
    and deformed by a force $\force$, resulting in a displaced object
    \emph{(dashed)}.
    The density function $\dens(\tilde{\*x})$ is one inside the object
    and zero outside.%
  }%
  \label{fig:topoOptExampleScenario}%
\end{SCfigure}

\subsection{Two-Scale Approach}
\label{sec:612twoScale}

\paragraph{Discretization and two-scale approach}

Of course, we cannot solve the problem \eqref{eq:topoOptProblemContinuous}
directly on computers,
as there are infinitely many density functions $\dens$.
To be able to discretize the domain $\domain \subset \real^{\dimdomain}$
more easily, we assume that $\domain$ is some hyper-rectangle
$\clint{\tilde{a}_1, \tilde{b}_1} \times \dotsb \times
\clint{\tilde{a}_{\dimdomain}, \tilde{b}_{\dimdomain}}$;
if it is not, we can replace $\domain$ with its bounding box.
We can then split $\domain$ into $M_1 \times \dotsb \times M_{\dimdomain}$
equally sized and axis-aligned sub-hyper-rectangles,
which we call \term{macro-cells}.
In the \term{two-scale approach},
we assume the material of the macro-cells to be
repetitions of infinitesimally small periodic structures
(i.e., they are identical for each macro-cell),
called \term{micro-cells}.
These micro-cells have a specific shape, which is parametrized by $d$ so-called
\term{micro-cell parameters} $x_1, \dotsc, x_d$.
The parameters are assumed to be normalized to values in the
unit interval $\clint{0, 1}$.
For instance, in $\dimdomain = 2$ dimensions,
this shape could be an axis-aligned cross of two bars
with thicknesses $x_1$ and $x_2$.

\paragraph{Elasticity tensors}

Note that while the shape of all micro-cells in one macro-cell is identical,
the micro-cell parameters corresponding to different macro-cells will differ.
This enables varying densities in different regions of $\domain$.
We denote the parameters of the $j$-th macro-cell
with $\*x^{(j)} = (x_1^{(j)}, \dotsc, x_d^{(j)}) \in \clint{0, 1}^d$,
where $j = 1, \dotsc, N$ and
$M := M_1 \dotsm M_{\dimdomain}$ is the number of macro-cells.
With linear elasticity,
one can compute so-called \term{elasticity tensors} $\etensor_j$,
which can be written as
symmetric $(3 \times 3)$ or $(6 \times 6)$ matrices
(for $\dimdomain = 2$ or $\dimdomain = 3$ dimensions, respectively).%
\footnote{%
  In general, the elasticity tensor is a fourth-order tensor in
  $\real^{\dimdomain \times \dimdomain \times \dimdomain \times \dimdomain}$.
  One can reduce the size of the tensor by exploiting various symmetries
  \cite{Huebner14Mehrdimensionale}
  to obtain $6$ or $21$ stiffness coefficients.%
}
The elasticity tensors encode information about the material properties
of the different macro-cells.
They are usually computed as the solution of a \fem problem,
the \term{micro-problem}.
Once all elasticity tensors $\etensor_j$ are known,
we can compute the compliance value corresponding to the macro-shape
by solving another \fem problem, the \term{macro-problem}.
We refer to \cite{Allaire04Topology} and \cite{Huebner14Mehrdimensionale}
for more details.

\paragraph{Discretized optimization problem}

The new optimization problem following from the
two-scale discretization process has the form
\begin{subequations}
  \label{eq:topoOptProblemDiscrete}
  \begin{gather}
    \min J(\*x^{(1)}, \dotsc, \*x^{(M)}),\quad
    \*x^{(1)}, \dotsc, \*x^{(M)} \in \clint{0, 1}^d
    \quad\text{s.t.}\quad
    \densmean(\*x^{(1)}, \dotsc, \*x^{(M)}) \le \densub,\\
    \densmean(\*x^{(1)}, \dotsc, \*x^{(M)})
    := \frac{1}{M} \sum_{j=1}^M \dens(\*x^{(j)}),
  \end{gather}
\end{subequations}
where $\dens(\*x^{(j)}) \in \clint{0, 1}$ is the
density of the $j$-th macro cell with micro-cell parameter $\*x^{(j)}$
(i.e., the fraction of material volume of one micro-cell
with respect to its total volume)
and $\densmean(\*x^{(1)}, \dotsc, \*x^{(M)})$ is the resulting
total mean density.
This discretized optimization problem can be implemented and
solved with computers.

\subsection{Optimization Process and Drawbacks of the Naive Approach}
\label{sec:613optimization}

\paragraph{Optimization process}

During the solution process, optimization algorithms typically
evaluate the objective function $\compliance(\*x^{(1)}, \dotsc, \*x^{(M)})$
at iteratively different \term{micro-cell parameter combinations}
$(\*x^{(1)}, \dotsc, \*x^{(M)}) \in (\real^d)^M$.
Every evaluation of $\compliance$ corresponds to the solution of a
macro-problem.
To solve the macro-problem once, however, the elasticity tensors
$\etensor_j$ of all $M$ macro-cells need to be known.
Hence, in every optimization iteration, it is necessary to solve
one macro-problem and $M$ micro-cell problems with \fem techniques.
This naive approach of solving \eqref{eq:topoOptProblemDiscrete}
has two major drawbacks:

\paragraph{Drawback 1: Computation time}

First, the naive approach is typically computationally infeasible.
The computation of a single elasticity tensor usually takes seconds to
minutes.
Although all $M$ micro-cell problems can be solved in parallel without any
communication (``embarrassingly parallel''), this might still be too long,
as $M$ is usually in the range of thousands and
as this has to be repeated for every optimization iteration.
The optimizer often needs thousands or tens of thousands iterations
(keep in mind that the optimization problem is $(d \cdot M)$-dimensional),
so that users have to wait long before their optimal result is computed.

\paragraph{Drawback 2: Approximation of gradients}

Second, most optimization algorithms require gradients of the
objective function and of the constraints.
This means that we would need the gradients%
\footnote{%
  By convention, the gradient $\nabla_{\*x} f(\*x)$ of a
  scalar-valued function $f$ is the column vector of partial derivatives.
  For vector-valued functions $\*f$, the gradient $\nabla_{\*x} \*f(\*x)$
  is defined as the transposed Jacobian, i.e., the matrix whose columns
  are the gradients $\nabla_{\*x} f_j(\*x)$ of single components of
  $\*f$.%
}
\begin{equation}
  \nabla_{\*x^{(j)}} \etensor_j(\*x^{(j)}),\quad
  \nabla_{\*x^{(j)}} \dens_j(\*x^{(j)})
\end{equation}
of the elasticity tensor $\etensor_j$ and
the density $\dens_j$ of the $j$-th macro-cell for $j = 1, \dotsc, M$.
However, in general, both gradients are not available,
such that they have to be approximated with finite differences.
This introduces new error sources and implies that
per optimization iteration and macro-cell,
$2d$ additional elasticity tensors have to be evaluated
(if central differences are employed), i.e.,
the time-to-solution increases by the factor of $2d$.
Additionally, if there are discontinuities in the objective function
or its gradient (which could already be caused by small numeric instabilities
due to the solution with the \fem),
the number of optimization iterations to achieve convergence might
increase.
These considerations do not take Hessians or other higher-order derivatives
into account, for which all the mentioned issues even worsen.
