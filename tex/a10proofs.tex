\chapter{Proofs}

This chapter contains proofs that
were too long or technical to include them in the main text.
For convenience, the corresponding propositions and theorems are repeated
before the proofs.



\section[Proof of Proposition~\ref*{prop:gridSizeCoarseBoundary}]{%
  Proof of \texorpdfstring{%
    \Cref{prop:gridSizeCoarseBoundary}%
  }{%
    Proposition \ref{prop:gridSizeCoarseBoundary}%
  }%
}
\label{sec:proofGridSizeCoarseBoundary}

\propGridSizeCoarseBoundary*

\begin{proof}
  Note that the outer union in the definition of $L_{n,d}^{\sparse(b)}$ in
  \eqref{eq:coarseBoundary} is indeed disjoint.
  Therefore,
  \begin{align}
    |\Omega_{n,d}^{\sparse(b)}|
    &= \sum_{\substack{\ßl \in \NN^d,\\\norm{\ßl}_1 \le n}} |I_\ßl| +
    \sum_{
      \substack{
        \ßl \in \NN_0^d \setminus \NN^d,\\
        (\norm{\vecmax(\ßl, \ß1)}_1 \le n - b + 1) \lor
        (\ßl = \ß0)
      }
    } |I_\ßl|.\\
    \intertext{%
      The first sum is the number $|\interior{\Omega}_{n,d}^\sparse|$
      of interior grid points in $\Omega_{n,d}^\sparse$.
      The second sum can be split into summands
      with the same number $q$ of zero entries,
      which we count with
      $N_\ßl := |\{t \mid l_t = 0\}|
      = \norm{\vecmax(\ßl, \ß1)}_1 - \norm{\ßl}_1$,
      and the same level sum $m = \norm{\ßl}_1$:%
    }
    &= |\interior{\Omega}_{n,d}^\sparse| + 2^d +
    \sum_{q=1}^{d-1} \sum_{m=d-q}^{n-b-q+1}
    \sum_{
      \substack{
        \ßl \in \NN_0^d,\\
        N_\ßl = q,\,\norm{\ßl}_1 = m}
    } |I_\ßl|
  \end{align}
  where $2^d$ is the summand for $\ßl = \ß0$
  (number $|I_\ß0|$ of corners of $[\ß0, \ß1]$).
  The limits of the $m$ sum are $d-q$,
  since there must be at least $d-q$ entries $\ge 1$ in a level vector
  with $q$ zero entries, and $n-b-q+1$,
  since $m = \norm{\ßl}_1
  = \norm{\vecmax(\ßl, \ß1)}_1 - N_\ßl
  \le n-b+1 - N_\ßl
  = n-b-q+1$.
  
  In general, the innermost summand $|I_\ßl|$ can be calculated as
  $|I_\ßl|
  = \prod_{l_t \ge 1} 2^{l_t-1} \cdot \prod_{l_t = 0} 2
  = 2^{\norm{\ßl}_1 - d + 2N_\ßl}$.
  The number of innermost summands is given by
  $|\{\ßl \in \NN_0^d \mid N_\ßl = q,\, \norm{\ßl}_1 = m\}|
  = \binom{d}{q} |\{\ßl \in \NN^{d-q} \mid \norm{\ßl}_1 = m\}|
  = \binom{d}{q} \binom{m-1}{d-q-1}$
  by first putting $q$ zeros in $d$ places,
  for which there are $\binom{d}{q}$ possibilities, and then
  counting all positive vectors of length $d - q$ with level sum $m$,
  which can be done in $\binom{m-1}{d-q-1}$ ways.
  Thus,
  \begin{align}
    |\Omega_{n,d}^{\sparse(b)}|
    &= |\interior{\Omega}_{n,d}^\sparse| + 2^d +
    \sum_{q=1}^{d-1} \binom{d}{q} \sum_{m=d-q}^{n-b-q+1}
    2^{m - d + 2q} \binom{m-1}{d-q-1}.\\
    \intertext{%
      Shifting the index $m \to (m + d - q)$ and rearranging the terms
      slightly, we obtain%
    }
    &= |\interior{\Omega}_{n,d}^\sparse| + 2^d +
    \sum_{q=1}^{d-1} 2^q \binom{d}{q} \sum_{m=0}^{n-d-b+1}
    2^m \binom{(d-q)+m-1}{(d-q)-1}.\\
    \intertext{%
      We can now use \cref{lemma:numberOfGridPointsInterior} to conclude that%
    }
    &= |\interior{\Omega}_{n,d}^\sparse| +
    \sum_{q=1}^d 2^q \binom{d}{q}
    |\interior{\Omega}_{n-q-b+1,d-q}^\sparse|,
  \end{align}
  which is the right-hand side of the assertion.
\end{proof}



\section[Proof of Proposition~\ref*{prop:invariantCoarseBoundary}]{%
  Proof of \texorpdfstring{%
    \Cref{prop:invariantCoarseBoundary}%
  }{%
    Proposition \ref{prop:invariantCoarseBoundary}%
  }%
}
\label{sec:proofInvariantCoarseBoundary}

\propInvariantCoarseBoundary*

\begin{proof}
  First, we show that every inserted level $\ßl' \in \NN_0^t$ in the inner loop
  can be found on the right-hand side of \eqref{eq:coarseInvariant}.
  If $\ßl' := (\ßl, 0)$
  is inserted for some $\ßl \in L^{(t-1)}$,
  then we have $\norm{\vecmax(\ßl, \ß1)}_1 \le n+d+t-b$ or
  $\ßl = \ß0$ by line~\ref{line:algCoarseBoundary1} of
  \cref{alg:coarseBoundary}.
  In the first case, we have
  \begin{equation}
    \norm{\vecmax(\ßl', \ß1)}_1
    = \norm{\vecmax(\ßl, \ß1)}_1 + 1
    \le n - d + t - b + 1,
  \end{equation}
  and in the second case $\ßl' = \vec{0}$.
  In either case, $\ßl'$ is contained in the RHS of \eqref{eq:coarseInvariant}.
  
  If $\ßl' := (\ßl, l_t)$ is inserted
  for some $\ßl \in L^{(t-1)}$ and
  $l_t \in \{1, \dotsc, l^\ast\}$, then there are,
  depending on whether $\ßl \in \NN^{t-1}$ or not, two cases:
  \begin{itemize}
    \item
    If $\ßl \in \NN^{t-1}$, then $\ßl' \in \NN^{t-1}$ as well and
    $\norm{\ßl'}_1 \le \norm{\ßl}_1 + l^\ast = n - d + t$
    due to line~\ref{line:algCoarseBoundary2},
    i.e., $\ßl'$ is contained in the first set of the RHS of
    \eqref{eq:coarseInvariant}.
    
    \item
    If $\ßl \notin \NN^{t-1}$, then $\ßl' \notin \NN^{t-1}$ as well and
    $\norm{\vecmax(\ßl', \ß1)}_1
    \le \norm{\vecmax(\ßl, \ß1)}_1 + l^\ast
    = n - d + t - b + 1$
    due to line~\ref{line:algCoarseBoundary3},
    i.e., $\ßl$ is contained in the second set of the RHS of
    \eqref{eq:coarseInvariant}.
  \end{itemize}
  Thus, all levels that the algorithm inserts into $L^{(t)}$
  can be found on the RHS of \eqref{eq:coarseInvariant}.
  
  It remains to prove that all levels on the RHS of \eqref{eq:coarseInvariant}
  are inserted by the algorithm into $L^{(t)}$ eventually.
  We prove this by induction over $t = 1, \dotsc, d$.
  For $t = 1$, the right-hand side (RHS) of \eqref{eq:coarseInvariant} equals
  $\{l \in \NN_0 \mid l \le n-d+1\}$, which is just $L^{(1)}$.
  For the induction step $(t - 1) \to t$, we assume
  the validity of the induction hypothesis
  \begin{equation}
    \label{eq:proofCoarseInductionHypothesis}
    \begin{aligned}
    L^{(t-1)}
    &= \{\ßl \in \NN^{t-1} \mid
    \norm{\ßl}_1 \le n-d+t-1\} \dotcup {}\\
    &\hphantom{{}={}}
    \big(\{\ßl \in \NN_0^{t-1} \setminus \NN^{t-1} \mid
    \norm{\vecmax(\ßl, \ß1)}_1 \le n-d+t-b\} \cup
    \{\vec{0}\}\big).
    \end{aligned}
  \end{equation}
  The RHS of \eqref{eq:coarseInvariant} has three parts,
  so we check for elements $\ßl' \in \NN_0^t$
  of each of the three sets that they are appended to $L^{(t)}$
  eventually.
  
  First, let $\ßl' = (\ßl, l_t)$ be in the first set of the RHS,
  i.e., $\ßl' \in \NN^t$ (in particular $l_t \ge 1$) and
  $\norm{\ßl'}_1 \le n - d + t$.
  Note that $\ßl$ will be encountered in the inner loop, as
  $\ßl \in \NN^{t-1}$ and
  $\norm{\ßl}_1 = \norm{\ßl'}_1 - l_t \le n - d + t - 1$,
  which implies $\ßl \in L^{(t-1)}$ by the induction
  hypothesis \eqref{eq:proofCoarseInductionHypothesis}.
  Since $1 \le l_t \le l^\ast$
  (due to
  $l_t = \norm{\ßl'}_1 - \norm{\ßl}_1 \le n-d+t - \norm{\ßl}_1 = l^\ast$),
  the level $\ßl'$ is inserted into $L^{(t)}$ during the innermost loop
  in line~\ref{line:algCoarseBoundary4}.
  
  Second, let $\ßl' = (\ßl, l_t)$
  be in the second set of the RHS, i.e.,
  $\ßl' \notin \NN^t$ and
  $\norm{\vecmax(\ßl', \ß1)}_1 \le n-d+t-b+1$.
  Here, there are three cases:
  \begin{enumerate}
    \item
    $l_t \ge 1$:
    This implies $\ßl \notin \NN^{t-1}$ and 
    $\norm{\vecmax(\ßl, \ß1)}_1
    = \norm{\vecmax(\ßl', \ß1)}_1 - l_t
    \le n-d+t-b$.
    This means $\ßl \in L^{(t-1)}$ by the induction hypothesis
    \eqref{eq:proofCoarseInductionHypothesis}.
    As $l_t \in \{1, \dotsc, l^\ast\}$
    (due to $l_t \le n-d+t-b -
    \norm{\vecmax(\ßl, \ß1)}_1 = l^\ast$),
    $l$ is added to $L^{(t)}$ in line~\ref{line:algCoarseBoundary4}.
    
    \item
    $l_t = 0$ and $\ßl \in \NN^{t-1}$:
    This implies $\norm{\ßl}_1 = \norm{\ßl'}_1
    = \norm{\vecmax(\ßl', \ß1)}_1 - 1
    \le n - d + t - b
    \le n - d + t - 1$ since $b \ge 1$.
    Again, by the induction hypothesis,
    \eqref{eq:proofCoarseInductionHypothesis} and
    $l$ is added to $L^{(t)}$ in line~\ref{line:algCoarseBoundary5} due to
    $\norm{\vecmax(\ßl, \ß1)}_1
    = \norm{\ßl}_1 \le n - d + t - b$.
    
    \item
    $l_t = 0$ and $\ßl \notin \NN^{t-1}$:
    This implies $\norm{\vecmax(\ßl, \ß1)}_1
    = \norm{\vecmax(\ßl', \ß1)}_1 - 1
    \le n - d + t - b$.
    Again, by the induction hypothesis,
    \eqref{eq:proofCoarseInductionHypothesis} and
    $l$ is added to $L^{(t)}$ in line~\ref{line:algCoarseBoundary5}.
  \end{enumerate}
  
  Third, let $\ßl = (\vec{0}, 0) \in \NN_0^t$
  be in the third set of the RHS.
  This level is appended in line~\ref{line:algCoarseBoundary5}
  to $L^{(t)}$, since $\ßl' = \vec{0} \in \NN_0^{t-1}$ is in $L^{(t-1)}$ by 
  induction hypothesis~\eqref{eq:proofCoarseInductionHypothesis}.
\end{proof}



\section[Proof of Proposition~\ref*{prop:hierBSplineLinearlyIndependent}]{%
  Proof of \texorpdfstring{%
    \Cref{prop:hierBSplineLinearlyIndependent}%
  }{%
    Proposition \ref{prop:hierBSplineLinearlyIndependent}%
  }%
}
\label{sec:proofHierBSplineLinearlyIndependent}

\propHierBSplineLinearlyIndependent*

\begin{proof}
  The proof is rigorous for the common low B-spline degrees of
  $p \in \{1, 3, 5, 7\}$.
  For higher degrees, the proof has to be viewed as a sketch.
  
  We follow the presentation in \cite{Valentin16Hierarchical} and
  prove the assertion by induction over $l \in \NN_0$.
  For $l = 0$, the B-splines $\varphi_{0,i'}^p$ with $i' \in \{0, 1\}$
  are linearly independent.
  For the induction step $(l-1) \to l$, let
  \begin{equation}
    \sum_{l'=0}^l \sum_{i' \in I_{l'}} \alpha_{l',i'} \varphi_{l',i'}^p
    \equiv 0
  \end{equation}
  be a linear combination of the zero function.
  We separate the summands of level $l$
  from the summands of coarser levels $l' < l$:
  \begin{equation}
    \label{eq:hierBSplineLinearlyIndependent3}
    \sum_{i \in I_l} \alpha_{l,i} \varphi_{l,i}^p
    =: g_1 \equiv g_2 :=
    -\sum_{l'=0}^{l-1} \sum_{i' \in I_{l'}} \alpha_{l',i'} \varphi_{l',i'}^p.
  \end{equation}
  The right-hand side $g_2$ is smooth in every grid point
  $x_{l,i}$ ($i \in I_l$) of level $l$,
  since these grid points are not knots of the hierarchical B-splines
  $\varphi_{l',i'}^p$ ($i' \in I_{l'}$) of level $l' < l$.
  This implies that the left-hand $g_1$ side must be smooth there as well:
  \begin{equation}
    \label{eq:hierBSplineLinearlyIndependent1}
    \partialdiff_-^p g_1(x_{l,i'})
    = \partialdiff_+^p g_1(x_{l,i'}),\quad
    i' \in I_l,
  \end{equation}
  where $\partialdiff_-^p$ and $\partialdiff_+^p$ denote the left and right
  derivative of order $p$, respectively.
  By repeated application of \eqref{eq:cardinalBSplineDerivative},
  one can show
  \begin{equation}
    \partialdiff_-^p b^p(k + 1)
    = (-1)^k \binom{p}{k}
    = \partialdiff_+^p b^p(k),\quad
    k \in \ZZ,
  \end{equation}
  where $\binom{p}{k} = 0$ for $k < 0$ or $k > p$
  \cite{Hoellig13Approximation}.
  We can insert this relation into
  \eqref{eq:hierBSplineLinearlyIndependent1}, i.e., into
  \begin{alignat}{2}
    %&&
    \sum_{i \in I_l} \alpha_{l,i}
    \partialdiff_-^p \varphi_{l,i}^p(x_{l,i'})
    &= \sum_{i \in I_l} \alpha_{l,i}
    \partialdiff_+^p \varphi_{l,i}^p(x_{l,i'}),\quad
    &&i' \in I_l,\\
    \intertext{and use \eqref{eq:uniformHierarchicalBSplineUV} to obtain}
    %\iff\quad&&
    %(h_l)^{-p} \sum_{i \in I_l} \alpha_{l,i}
    %\partialdiff_-^p b^p((p+1)/2 + i' - i)
    %&= (h_l)^{-p} \sum_{i \in I_l} \alpha_{l,i}
    %\partialdiff_+^p b^p((p+1)/2 + i' - i),\quad
    %i' \in I_l\\
    %\iff\quad&&
    \sum_{i \in I_l} \alpha_{l,i} (-1)^{k-1} \binom{p}{k-1}
    &= \sum_{i \in I_l} \alpha_{l,i} (-1)^k \binom{p}{k},\quad
    &&i' \in I_l,\quad
    k := \frac{p+1}{2} + i' - i.
  \end{alignat}
  As $\binom{p}{k-1} + \binom{p}{k} = \binom{p+1}{k}$
  and $(-1)^k$ is constant for $i \in I_l$ when $i'$ is fixed,
  this is equivalent to
  \begin{equation}
    \label{eq:hierBSplineLinearlyIndependent2}
    \sum_{i \in I_l} \alpha_{l,i}
    \binom{p+1}{\frac{p+1}{2} + i' - i} = 0,\quad
    i' \in I_l.
  \end{equation}
  
  This is a quadratic linear system of equations
  whose system matrix
  $A(p)$ is a banded symmetric Toeplitz matrix of size
  $2^{l-1} \times 2^{l-1}$ with bandwidth $\lceil(p-1)/4\rceil$.
  For $p = 1, 3, 5, 7$, $A(p)$ is as follows:
  \begin{equation}
    \begin{gathered}
      A(1) =
      \begin{pmatrix}
        2&&\\
        &\ddots&\\
        &&2
      \end{pmatrix},\quad
      A(3) =
      \begin{pmatrix}
        6&1&&\\
        1&\ddots&\ddots\\
        &\ddots&\ddots&1\\
        &&1&6
      \end{pmatrix},\\
      A(5) =
      \begin{pmatrix}
        20&6&&\\
        6&\ddots&\ddots\\
        &\ddots&\ddots&6\\
        &&6&20
      \end{pmatrix},\quad
      A(7) =
      \begin{pmatrix}
        70&28&1&&\\
        28&\ddots&\ddots&\ddots&\\
        1&\ddots&\ddots&\ddots&1\\
        &\ddots&\ddots&\ddots&28\\
        &&1&28&70
      \end{pmatrix},
    \end{gathered}
  \end{equation}
  These matrices are diagonally dominant and therefore regular.
  For higher B-spline degrees $p$, the regularity of $A(p)$ has
  to be shown differently.
  
  Due to the regularity of $A(p)$, we infer from
  \eqref{eq:hierBSplineLinearlyIndependent2} that
  $\alpha_{l,i'} = 0$ for $i' \in I_l$.
  Due to \eqref{eq:hierBSplineLinearlyIndependent3},
  we obtain
  a linear combination of the zero function with the hierarchical
  B-splines of level $< l$, i.e.,
  \begin{equation}
  \sum_{l'=0}^{l-1} \sum_{i' \in I_{l'}} \alpha_{l',i'} \varphi_{l',i'}^p
  = 0,
  \end{equation}
  which implies $\alpha_{l',i'} = 0$ for all $l' = 0, \dotsc, l - 1$
  by the induction hypothesis.
  Thus, the hierarchical functions $\varphi_{l',i'}^p$
  ($l' \le l$, $i' \in I_{l'}$) are linearly independent.
\end{proof}