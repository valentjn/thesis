\section{Sparse Grids}
\label{sec:23sparseGrids}

The idea of sparse grids is to use the
hierarchical splitting \eqref{eq:hierSplittingUV}
to keep only the most ``important'' hierarchical subspaces,
omitting the remaining ones.
There are three main ``flavors'' of sparse grids:
regular, dimensionally adaptive, and spatially adaptive.



\subsection{Regular Sparse Grids}
\label{sec:231regularSG}

\paragraph{Hierarchical contributions}

To assess the importance of a subspace, we consider again the
interpolant $f_\ßl \in \ns{\ßl}$ of a function $f\colon \clint{\ß0, \ß1} \to \RR$.
According to the splitting \eqref{eq:hierSplittingMV}, the interpolant can
be written as
\begin{equation}
  \label{eq:interpHierFullGrid}
  f_\ßl
  = \sum_{\ßl'=\ß0}^\ßl \sum_{\ßi'=\ß0}^{\ß2^{\ßl'}}
  \alpha_{\ßl',\ßi'} \basis{\ßl',\ßi'},\quad
  \fa{\ßi = \ß0, \dotsc, \ß2^\ßl}{f_\ßl(\vgp{\ßl,\ßi}) = f(\vgp{\ßl,\ßi})}.
\end{equation}
\newgsymbol{alphali!}{$\alpha_{\ßl,\ßi}$}{%
  Hierarchical surpluses (coefficients of a linear combination of hierarchical
  basis functions)%
}%
The coefficients $\alpha_{\ßl',\ßi'}$ with respect to the hierarchical basis
$\basis{\ßl',\ßi'}$ are the \term{hierarchical surpluses}.
When using the hat function basis $\basis{\ßl,\ßi}^1$,
one can prove the following representation
for the corresponding surpluses \cite{Bungartz04Sparse}:
\begin{equation}
  \alpha_{\ßl,\ßi}
  = (-1)^d 2^{-\normone{\ßl+\ß1}}
  \int_\ß0^\ß1 \basis{\ßl,\ßi}^1(\ßx)
  \frac{\partialdiff^{2d}}{\partialdiff x_1^2 \dotsb \partialdiff x_d^2}
  f(\ßx) \diff{}\ßx,
\end{equation}
if $\ßl \ge \ß1$ and
$f$ is twice continuously differentiable in every dimension simultaneously,
i.e.,
$\frac{\partialdiff^{2d}}{\partialdiff x_1^2 \dotsb \partialdiff x_d^2} f$
exists and is continuous.%
\footnote{%
  Again, the notation implies that the integration domain is
  the unit hypercube $\clint{\ß0, \ß1}$.%
}\multiplefootnoteseparator%
\footnote{%
  The statement is even valid for functions in the Sobolev space
  $H_\mathrm{mix}^2(\clint{\ß0, \ß1})$ with dominating mixed derivative,
  as its proof mainly relies on integration by parts
  \cite{Bungartz04Sparse}.%
}
%This equation provides a direct relation between the hat function surpluses
%and the second mixed derivative of the objective function, which has
%two consequences.
%First, the absolute value of the surpluses is large in regions where
%the absolute value of the second mixed derivative is large, i.e.,
%where the objective function oscillates strongly.
%Second, the absolute surpluses decay with
%increasing level $\ßl$ as both the factor
%$2^{-\normone{\ßl+\ß1}}$ and the size of the support of $\basis{\ßl,\ßi}$
%are decreasing.
Consequently, the contribution of the summand of level $\ßl$
can be estimated by
\begin{equation}
  \label{eq:componentEstimation}
  \bignorm{\sum_{\ßi'=\ß0}^{\ß2^{\ßl'}}
  \alpha_{\ßl',\ßi'} \basis{\ßl',\ßi'}^1}_{L^2}
  \le 3^{-d} \cdot 2^{-2 \normone{\ßl}} \cdot
  \bignorm{\frac{\partialdiff^{2d}}{
      \partialdiff x_1^2 \dotsb \partialdiff x_d^2
  } f}_{L^2}
\end{equation}
for the hat function surpluses $\alpha_{\ßl',\ßi'}$ \cite{Bungartz04Sparse}.

\paragraph{Definition of regular sparse grids}

Equation \eqref{eq:componentEstimation} motivates to omit those summands
from the sum \eqref{eq:interpHierFullGrid} whose level sum $\normone{\ßl}$
exceeds a certain value $n \in \NNz$.
More formally, the selection of the relevant subspaces can be formulated as a
continuous knapsack problem~\cite{Bungartz04Sparse}.
\newgsymbol{Vnds}{$V_{n,d}^\sparse$}{%
  Regular sparse grid space of level $n$ with dimensionality $d$%
}%
\newgsymbol{Omegands}{$\Omega_{n,d}^\sparse$}{%
  Set of regular sparse grid points of level $n$ with dimensionality $d$%
}%
\newgsymbol{.s}{$\cdot^\sparse$}{%
  Superscript for ``sparse grid (grid point set/function space/interpolant)''%
}%
The resulting function space and grid point set
\begin{equation}
  V_{n,d}^\sparse
  := \bigoplus_{\normone{\ßl} \le n} W_{\ßl},\qquad
  \Omega_{n,d}^\sparse
  := \bigdotcup_{\normone{\ßl} \le n}
  \{\vgp{\ßl,\ßi} \mid \ßi \in I_{\ßl}\}
\end{equation}
are called \term{regular sparse grid space} and
\term{regular sparse grid} of level $n$, respectively.
To better distinguish the different grids,
we call the nodal spaces and grids \term{full grids}.
Although sparse grids have been motivated using the hat function
basis $\basis{\ßl,\ßi}^1$,
we generalize the definition to arbitrary bases $\basis{\ßl,\ßi}$.
\Cref{fig:regularSG} shows the construction of a
regular sparse grid in two dimensions.

\begin{figure}
  \subcaptionbox{%
    Hierarchical splitting and subspace selection.
    The rectangles indicate the support of the
    bivariate hat basis functions.%
  }[85mm]{%
    \includegraphics{sg_1}%
  }%
  \hfill%
  \begin{minipage}[b]{65mm}
    \subcaptionbox{%
      Full grid obtained by adding all subspaces of level $\ßl \le n \cdot \ß1$.%
    }[65mm]{%
      \includegraphics{sg_2}%
    }\\[5mm]%
    \subcaptionbox{%
      Regular sparse grid obtained by adding all subspaces
      whose level $\ßl$ satisfies $\normone{\ßl} \le n$
      \emph{\textcolor{mittelblau}{(blue)}}.%
    }[65mm]{%
      \includegraphics{sg_3}%
    }%
  \end{minipage}
  \caption{Regular sparse grid of level $n = 3$ in two dimensions.}
  \label{fig:regularSG}
\end{figure}

\paragraph{Error and grid size}

\newgsymbol{Omegad}{$\bndry{\Omega}$}{%
  Boundary of the domain $\Omega \subset \RR^d$%
}%
One can prove that for homogeneous boundary conditions
$f|_{\bndry{\clint{\ß0,\ß1}}} \equiv 0$,
the number of required inner grid points
($\vgp{\ßl,\ßi} \in \Omega_{n,d}^\sparse$ where $\ßl \ge \ß1$)
grows like $\calO(\ms{n}^{-1} (\log_2 \ms{n}^{-1})^{d-1})$
\cite{Bungartz04Sparse}, which is much less than
the corresponding number $\calO((\ms{n}^{-1})^d)$ in the full grid case
(see \eqref{eq:dimensionFG}).
In particular, the exponential dependency on the dimensionality $d$
has vanished.
The corresponding $L^2$ error of the sparse grid interpolant
$f_{n,d}^{\sparse,1}$ using hat functions
(still assuming homogeneous boundary conditions) decays like
\begin{equation}
  \norm{f - f_{n,d}^{\sparse,1}}_{L^2} = \calO(\ms{n}^2 (\log_2 \ms{n}^{-1})^{d-1}),
\end{equation}
which is only slightly worse than the full grid error by the factor of
$(\log_2 \ms{n}^{-1})^{d-1}$ \cite{Bungartz04Sparse}.



\subsection{Dimensionally Adaptive Sparse Grids}
\label{sec:232dimensionallyAdaptiveSG}

The idea of dimensional adaptivity is to spend more grid
points along specific dimensions depending on the objective function.
Different criteria for the choice of dimensions exist,
for example the absolute value of the linear hierarchical surpluses
or the \anova decomposition.
\todo{not sure about that one}
To incorporate dimensional adaptivity into sparse grids,
one has to generalize the symmetric
choice of subspaces in the definition of regular sparse grids
to allow for an asymmetric preference.
\newgsymbol{Vs}{$V^\sparse$}{%
  Arbitrary sparse grid space (possibly spatially adaptive)%
}%
\newgsymbol{Omegas}{$\Omega^\sparse$}{%
  Arbitrary sparse grid (possibly spatially adaptive)%
}%
\newgsymbol{L}{$L$}{Finite subset $L \subset \NNz^d$ of levels}%
Generally, function spaces~$V^\sparse$ and grid sets $\Omega^\sparse$
of \term{dimensionally adaptive sparse grids} have the form
\begin{equation}
  V^\sparse
  = \bigoplus_{\ßl \in L} W_\ßl,\qquad
  \Omega^\sparse
  = \bigdotcup_{\ßl \in L} \{\vgp{\ßl,\ßi} \mid \ßi \in I_\ßl\},
\end{equation}
where $L$ is a \term{downward closed} set, i.e.,
a finite subset $L \subset \NNz^d$
for which $\forall_{\ßl \in L} \fa{\ßl' \le \ßl}{\ßl' \in L}$.
Regular sparse grids are a special case by setting
$L = \{\ßl \in \NNz^d \mid \normone{\ßl} \le n\}$.

\paragraph{Combination technique}

The key advantage of dimensionally adaptive sparse grids over
spatially adaptive approaches lies in the
so-called \term{combination technique}.
\newgsymbol{cli!}{$c_{\ßl,\ßi}$}{%
  Interpolation coefficients for the full grid $\ns{\ßl}$%
}%
For regular sparse grids, one can show that the sparse grid interpolant
$f_{n,d}^\sparse$ can be written as
\begin{equation}
  \label{eq:combiTechnique}
  f_{n,d}^\sparse
  = \sum_{q=0}^{d-1} (-1)^q \binom{d-1}{q} \sum_{\normone{\ßl} = n-q}
  \sum_{\ßi=\ß0}^{\ß2^\ßl} c_{\ßl,\ßi} \basis{\ßl,\ßi},
\end{equation}
where the $c_{\ßl,\ßi} \in \RR$ ($\ßi = \ß0, \dotsc, \ß2^\ßl$)
are the interpolation coefficients on the full grid of level~$\ßl$, i.e.,
$\fa{\ßi' = \ß0, \dotsc, \ß2^\ßl}{%
  \sum_{\ßi=\ß0}^{\ß2^\ßl} c_{\ßl,\ßi} \basis{\ßl,\ßi}(\vgp{\ßl,\ßi'})
  = f(\vgp{\ßl,\ßi'})%
}$ \multicite{Smolyak63Quadrature,Zenger91Sparse}.
For general dimensionally adaptive sparse grids, a similar formula exists
\cite{Wasilkowski95Explicit}.
The combination formula \eqref{eq:combiTechnique},
which is visualized in \cref{fig:combinationTechnique}, splits the
sparse grid interpolant into a weighted sum of full grid interpolants.
In applications, each grid can be processed in parallel,
drastically speeding up computations like the solution of \pdes.
In addition, existing code working on nodal bases does not have to be
rewritten in terms of implementing hierarchical functions,
which means that combination technique allows sparse grids to be employed
in a minimally invasive way.

\begin{SCfigure}
  \includegraphics{sg_4}%
  \caption{%
    The combination technique combines nodal subspaces in a weighted
    sum to form a regular sparse grid space of level $n = 3$ in two dimensions.
    The \textcolor{C1}{red subspaces} ($q = 1$ in \eqref{eq:combiTechnique})
    are subtracted from the sum of the
    \textcolor{C4}{green subspaces} ($q = 0$).%
  }%
  \label{fig:combinationTechnique}%
\end{SCfigure}



\subsection{Spatially Adaptive Sparse Grids}
\label{sec:233spatiallyAdaptiveSG}

Dimensional adaptivity does not suffice to resolve local features of the
objective function.
Especially in some applications, it is crucial for the
interpolant to be highly accurate in specific regions of the domain.
For instance in optimization, it is not necessary to have a small global
interpolation error, but rather high accuracy near the optima is important.

This can be achieved by \term{spatially adaptive sparse grids},
on which this thesis focuses.
\newgsymbol{K}{$K$}{Finite set of level-index pairs $(\ßl,\ßi)$}%
In the most general form, their function spaces $V^\sparse$
and grid sets $\Omega^\sparse$ have the form
\begin{equation}
  V^\sparse
  = \spn\{\basis{\ßl,\ßi} \mid (\ßl,\ßi) \in K\},\qquad
  \Omega^\sparse
  = \{\vgp{\ßl,\ßi} \mid (\ßl,\ßi) \in K\},
\end{equation}
where $K$ is a finite set of level-index pairs $(\ßl,\ßi)$
with $\ßl \in \NNz^d$ and $\ßi \in I_\ßl$.
Algorithms for sparse grids often make specific assumptions about $K$.
If they are not met, then the algorithms do not produce the correct results.
\newgsymbol{et!}{$\ße_t$}{$t$-th unit vector $(0, \dotsc, 0, 1, 0, \dotsc, 0)$}%
%For example for hat functions $\basis{\ßl,\ßi}^1$, the grid should contain
%the hierarchical ancestors of every grid point:
%\begin{equation}
%  \label{eq:hierAncestors}
%  \forall_{(\ßl,\ßi) \in K}
%  \fa{\{t = 1, \dotsc, d \mid l_t \ge 1\}}{(\ßl',\ßi') \in K},\quad
%  \ßl' := \ßl - \ße_t,\quad
%  i_{t'}' :=
%  \begin{cases}
%    2 \lfloor i_t/4 \rfloor + 1,&t = t',\\
%    i_{t'},&t \not= t',
%  \end{cases}
%\end{equation}
%where $\ße_t$ is the $t$-th unit vector and $t' = 1, \dotsc, d$.
%If \eqref{eq:hierAncestors} is not met, the so-called
%unidirectional principle, which is used for instance to efficiently calculate
%hierarchical surpluses, does not work.
%However, as we will see, the unidirectional principle cannot be applied
%to B-splines of general degree, even if \eqref{eq:hierAncestors} is satisfied.
%Therefore, for most of our considerations, we will not restrict the
%choice of $K$.
For example when working with hat functions $\basis{\ßl,\ßi}^1$,
the grid should contain the hierarchical ancestors of every grid point.
Otherwise, the so-called
unidirectional principle, which is used for instance to efficiently calculate
hierarchical surpluses, does not work.
However, as we will see, the unidirectional principle cannot be applied
to B-splines of general degree, even if the hierarchical ancestors exist.
Therefore, for most of our considerations, we will not restrict the
choice of $K$.

\begin{figure}
  \subcaptionbox{%
    Hierarchical splitting and grid point selection.
    The rectangles indicate again the support of the
    bivariate hat basis functions.%
  }[85mm]{%
    \includegraphics{sg_5}%
  }%
  \hfill%
  \subcaptionbox{%
    Resulting spatially adaptive sparse grid.%
  }[65mm]{%
    \includegraphics{sg_6}%
  }%
  \caption{%
    Spatially adaptive sparse grid in two dimensions.
    More grid points were generated in the top right corner,
    which can help to resolve fine oscillations of the objective function.%
  }
  \label{fig:spatiallyAdaptiveSG}
\end{figure}
