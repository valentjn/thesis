\section{The Hierarchization Problem}
\label{sec:41problem}

Let $\sgset \subset \clint{0, 1}^d$ be a general (sparse) grid that
may be spatially adaptive, i.e.,
of the form $\setsize{\sgset} = \{\vgp{\ßl,\ßi} \mid (\ßl, \ßi) \in \liset\}$,
where $\liset$ is a set of level-index pairs $(\ßl, \ßi)$ with $\ßl \in \NNz$
and $\ßi \in \hiset{\ßl}$ such that $N := \setsize{\sgset} = \setsize{\liset} < \infty$
(see \cref{sec:233spatiallyAdaptiveSG}).
The \term{hierarchization problem} is finding
\term{hierarchical surpluses}
$(\surplus{\ßl',\ßi'})_{(\ßl',\ßi') \in \liset} \in \RR^N$ such that
\begin{equation}
  \label{eq:hierarchizationProblem}
  \sum_{\mathclap{(\ßl', \ßi') \in \liset}} \surplus{\ßl',\ßi'}
  \basis{\ßl',\ßi'}(\vgp{\ßl,\ßi}) = f_{\ßl,\ßi}
  \quad\text{for all}\quad
  (\ßl, \ßi) \in \liset,
\end{equation}
where $(f_{\ßl,\ßi})_{(\ßl,\ßi) \in \liset} \in \RR^N$ is a given set of
function values $f(\vgp{\ßl,\ßi})$ at the grid points $\vgp{\ßl,\ßi}$.
The hierarchical surpluses then define the interpolant $f^\sparse$ as
\begin{equation}
  f^\sparse\colon \clint{\ß0, \ß1} \to \RR,\quad
  f^\sparse :=
  \sum_{\mathclap{(\ßl', \ßi') \in \liset}} \surplus{\ßl',\ßi'}
  \basis{\ßl',\ßi'},
\end{equation}
which interpolates $f$ at the grid points $\vgp{\ßl,\ßi}$ of $\sgset$.

The basis functions $\basis{\ßl',\ßi'}$ are
arbitrary tensor product functions.
We explicitly allow $\basis{\ßl',\ßi'}$ to be a non-hierarchical
nodal basis, in which case $\ßl'$ is constant and
$\sgset$ is usually a full grid.
Strictly speaking, the problem is then a \term{interpolation problem}
and the $\surplus{\ßl',\ßi'}$ are \term{interpolation coefficients}.
However, we still apply the terms
``hierarchization'' and ``hierarchical surpluses'' in this case
to keep the terminology consistent.

\paragraph{Hierarchization as a linear operator}

The example of hierarchization can be generalized
to arbitrary linear operators
\begin{equation}
  L\colon \RR^N \to \RR^N,\quad
  \ßu \mapsto \ßy = L[\ßu],
\end{equation}
where $L$ depends on the grid $\sgset$ at hand.
Input $\ßu$ and output $\ßy$ are scalar-valued data%
%\footnote{%
%  The setting could be generalized even more,
%  for example to vector-valued data.%
%  We restrict ourselves to the scalar-valued case,
%  keeping hierarchization as our main application in mind.%
%}
\begin{equation}
  \ßu = (u_{\ßl,\ßi})_{(\ßl,\ßi) \in \liset} \in \RR^N,\quad
  \ßy = (y_{\ßl,\ßi})_{(\ßl,\ßi) \in \liset} \in \RR^N,
\end{equation}
which give one scalar per grid point $\vgp{\ßl,\ßi} \in \sgset$.
For the case of hierarchization,
$L$ is the inverse of the \term{interpolation matrix} $A \in \RR^{N \times N}$:
\begin{subequations}
  \label{eq:hierarchizationSLE}
  \begin{equation}
    L = A^{-1},\quad
    A = (\basis{\ßl',\ßi'}(\vgp{\ßl,\ßi}))_{(\ßl,\ßi),(\ßl',\ßi') \in \liset},\quad
    \ßu = (f_{\ßl,\ßi})_{(\ßl,\ßi) \in \liset},\quad
    \ßy = (\surplus{\ßl',\ßi'})_{(\ßl',\ßi') \in \liset}.
  \end{equation}
  This means that we can determine the $\surplus{\ßl',\ßi'}$ by solving
  the $N \times N$ system of linear equations
  \begin{equation}
    \ßy = L[\ßu]
    \quad\iff\quad
    A \cdot (\surplus{\ßl',\ßi'})_{(\ßl',\ßi') \in \liset}
    = (f_{\ßl,\ßi})_{(\ßl,\ßi) \in \liset}.
  \end{equation}
\end{subequations}

\paragraph{Complexity of B-spline hierarchization}

As noted in \cite{Valentin18Fundamental},
hierarchization on sparse grids with hierarchical B-splines
$\basis{\ßl,\ßi}^\ßp$ of degree $\ßp$
as basis functions $\basis{\ßl,\ßi}$ is a tedious task.
The corresponding linear system \eqref{eq:hierarchizationSLE} is in general
non-symmetric
(i.e., $\basis{\ßl',\ßi'}^\ßp(\vgp{\ßl,\ßi}) \not=
\basis{\ßl,\ßi}^\ßp(\vgp{\ßl',\ßi'})$) and densely populated.
This is because the matrix entry in the $(\ßl,\ßi)$-th row and
$(\ßl',\ßi')$-th column vanishes if and only if
\begin{equation}
  \vgp{\ßl,\ßi} \notin \intsupp \basis{\ßl',\ßi'}^\ßp
  \iff
  \ex{t = 1, \dotsc, d}{
    \gp{l_t,i_t} \notin
    \opintscaled{
      \gp{l'_t,i'_t} - \tfrac{p_t+1}{2} \ms{l'_t},\,
      \gp{l'_t,i'_t} + \tfrac{p_t+1}{2} \ms{l'_t}
    }
  },
\end{equation}
where $\intsupp$ is the interior of the support
\cite{Valentin18Fundamental}.
For coarse levels $\ßl'$, the mesh size $\ms{l'_t}$ is large in
every dimension $t$, which implies that $\intsupp \basis{\ßl',\ßi'}^\ßp$
contains most of the grid points.
In contrast to the hat function case ($\ßp = \ß1$),
the value of $\surplus{\ßl',\ßi'}$ depends not only on
$f_{\ßl,\ßi}$ and the data of its $3^d - 1$ neighboring grid points
on the boundary of $\supp \basis{\ßl',\ßi'}^\ß1$,
but potentially on the data of the whole grid.

This prohibits the use of the unidirectional principle,
which will be discussed in \cref{sec:42fullGrids}
and which takes $\landauO{Nd}$ computing time%
\footnote{in the number of univariate basis evaluations},
on sparse grids with hierarchical B-splines.
Consequently, we have to solve the linear system
\eqref{eq:hierarchizationSLE}, which is significantly more time-consuming,
as it takes between $\Omega(N^2 d)$ and $\landauO{N^3 d}$ time
(via Gaussian elimination).
In addition, if we use an explicit solver for the linear system,
we additionally have to store an $N \times N$ matrix in memory.
However, a grid of size $N = \num{50000}$ would already exceed the memory
of a \SI{16}{\gibi\byte} workstation.

\paragraph{Notation}

We do not need the hierarchical level-index information $(\ßl, \ßi)$ in
$\sgset$, $\liset$, $\ßu$, and $\ßy$
for most of the considerations in this chapter.
Therefore, we assume that in each dimension $t$, the level-index pairs
$(l_t, i_t)$ ($l_t \in \NNz$, $i_t \in \hiset{l_t}$)
are continuously enumerated by a single index $k_t = k_t(l_t, i_t) \in \NNz$.
We identify $(\ßl, \ßi)$ with a single index $\ßk$,
whose $t$-th component is given by $k_t(l_t, i_t)$.
Consequently,
we can regard $\liset$ as a subset $\{\ßk \mid \ßx_\ßk \in \sgset\}$
of $\NN^d$.
We will switch between the notations whenever appropriate.
All statements that are formulated in the $\ßk$ notation are
valid for both the nodal and the hierarchical basis
(i.e., for all tensor product bases).

In the following, $k_t$ denotes the $t$-th component of a $d$-vector $\ßk$
as usual.
With $\ßk_{-t}$, we denote the $(d-1)$-vector that is obtained from $\ßk$
by omitting the $k$-th component,
i.e., $\ßk_{-t} := (k_1, \dotsc, k_{t-1}, k_{t+1}, \dotsc, k_d)$.
For a $j$-tuple $T = (t_1, \dotsc, t_j) \in \{1, \dotsc, d\}^j$,
we define $\ßk_T$ to be the $j$-vector $(k_{t_1}, \dotsc, k_{t_j})$
that only contains the entries of the dimensions listed in $T$.
Accordingly, $\ßk_{-T}$ is defined as the $(d-j)$-vector
that contains the entries of the remaining dimensions
(sorted by the dimension $t$).
We define $a:b := (a, a + 1, \dotsc, b)$ as an indexing shortcut ($a \le b$).
