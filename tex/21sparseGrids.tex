\section{Hierarchical Bases and Sparse Grids}

\todo{write}



\subsection{Nodal Spaces}

\todo{insert citations}

\subsubsection{Univariate Case}

Let us first consider univariate functions
that are defined on the unit interval $[0, 1]$.
\newgsymbol{l}{$l$}{Level $\in \NN_0$}%
We discretize this domain by splitting it into $2^l$ equidistant segments,
where $l \in \NN_0$ is the \term{level}.
\newgsymbol{xli}{$x_{l,i}$}{Grid point $:= i \cdot h_l$}%
\newgsymbol{i}{$i$}{Index $= 0, \dotsc, 2^l$}%
\newgsymbol{hl}{$h_l$}{Mesh size $:= 2^{-l}$}%
The resulting $2^l + 1$ \term{grid points} are given by
\begin{equation}
  x_{l,i} := i \cdot h_l,\quad
  i = 0, \dotsc, 2^l,
\end{equation}
where $i$ is the \term{index} and $h_l := 2^{-l}$ is the \term{mesh size}.
\newgsymbol{phili}{$\varphi_{l,i}$}{%
  Hierarchical basis function of level $l$, index $i$%
}%
Every grid point is associated with a \term{basis function}
\begin{equation}
  \varphi_{l,i}\colon [0, 1] \to \RR.
\end{equation}
In this thesis, we assume $\varphi_{l,i}$ to be arbitrary,
satisfying required assumptions when needed and stated.
However, it helps for both the theory and the intuition to have a
specific example of basis functions in mind.
\newgsymbol{phili1}{$\varphi_{l,i}^1$}{Hat function of level $l$, index $i$}%
The most common choice for $\varphi_{l,i}$ are the so-called \term{hat functions}
(linear B-splines), which are defined as
\begin{equation}
  \varphi_{l,i}^1(x)
  := \max(1 - |x/h_l - i|, 0).
\end{equation}
Here and in the following,
the superscript ``1'' is the degree of the linear B-spline and
is not to be read as an exponent.
We will generalize this notation to B-splines $\varphi_{l,i}^p$ of
arbitrary degrees $p$ (see \cref{sec:sgBspl}).

\newgsymbol{Vl}{$V_l$}{Nodal space of level $l$}%
\newgsymbol{span}{$\spn$}{Linear span (set of all linear combinations)}%
The \emph{nodal space} $V_l$ of level $l$
is defined as the linear span of all basis functions
$\varphi_{l,i}$, $i = 0, \dotsc, 2^l$:
\begin{equation}
  V_l := \spn\{\varphi_{l,i} \mid i = 0, \dotsc, 2^l\}.
\end{equation}
We assume that the functions $\varphi_{l,i}$ form a basis of $V_l$, i.e.,
every linear combination of these functions is unique.
\newgsymbol{Vl1}{$V_l^1$}{Nodal piecewise linear space of level $l$}%
In the case of hat functions $\varphi_{l,i}^1$,
the nodal space $V_l^1$ is defined analogously as the span of the
$\varphi_{l,i}^1$ functions.
The space $V_l^1$ is the space of the linear splines,
that is, the space of all continuous functions on $[0, 1]$ that are
piecewise linear polynomials on $[x_{l,i}, x_{l,i+1}]$ for
$i = 0, \dotsc, 2^l - 1$.

\subsubsection{Multivariate Case}

\newgsymbol{d}{$d$}{Dimensionality $\in \NN$}%
For the multivariate case with $d \in \NN$ dimensions,
we proceed with the usual tensor product approach,
for which we replace all indices, points, and functions with
multi-indices, Cartesian products, and tensor products, respectively.
\newgsymbol{0!}{$\ß0$}{$(0, \dotsc, 0) \in \NN_0^d$}%
\newgsymbol{1!}{$\ß1$}{$(1, \dotsc, 1) \in \NN^d$}%
\newgsymbol{01!}{$[\ß0, \ß1]$}{%
  Unit hypercube $:= [0, 1]^d$%
}
\newgsymbol{l!}{$\ßl$}{Multivariate level $\in \NN_0^d$}%
Therefore, the domain is now $[\ß0, \ß1] := [0, 1]^d$,
which can be partitioned into
$\prod_{t=1}^d (2^{l_t} + 1)$ equally sized hyper-cubes,
where $\ßl = (l_1, \dotsc, l_d) \in \NN_0^d$ is the $d$-dimensional level.
\newgsymbol{xli!}{$\ßx_{\ßl,\ßi}$}{Multivariate grid point $:= \ßi \cdot \ßh_\ßl$}%
\newgsymbol{i!}{$\ßi$}{Multivariate index $= \ß0, \dotsc, \ß2^l$}%
\newgsymbol{i02l!}{$\ßi = \ß0, \dotsc, \ß2^l$}{%
  For all $\ßi$ with $0 \le i_t \le 2^{l_t}$ for all $t = 1, \dotsc, d$%
}%
The corners of the hyper-cubes are given by the grid points
\begin{equation}
  \label{eq:gridPointMultivariate}
  \ßx_{\ßl,\ßi} := \ßi \cdot \ßh_\ßl,\quad
  \ßi = \ß0, \dotsc, \ß2^{\ßl}.
\end{equation}
In this thesis, relations and operations with vectors (in bold face)
are to be read coordinate-wise, unless stated otherwise.
Bold-faced numbers like $\ß0$ are defined to be the vector $(0, \dotsc, 0)$
in which every entry is equal to that number.
This allows for a somewhat intuitive and suggestive notation.
\newgsymbol{hl!}{$\ßh_\ßl$}{Multivariate mesh size $:= \ß2^{-\ßl}$}%
For example, \eqref{eq:gridPointMultivariate} is equivalent to
\begin{equation}
  \ßx_{\ßl,\ßi}
  := (i_1 h_{l_1},\; \dotsc,\; i_d h_{l_d}),\quad
  i_t = 0, \dotsc, 2^{l_t},\quad
  t = 1, \dotsc, d,
\end{equation}
with the $d$-dimensional mesh size
$\ßh_\ßl := \ß2^{-\ßl} = (h_{l_1}, \dotsc, h_{l_d})$.
\newgsymbol{phili!}{$\varphi_{\ßl,\ßi}$}{%
  Multivariate hierarchical basis function of level $\ßl$, index $\ßi$%
}%
Again, every grid point is associated with a basis function that is defined
as the tensor product of the univariate functions:%
\footnote{%
  Note that one could employ basis functions of different types in
  each dimension, for example B-splines of different degrees.
  For simplicity, we first restrict ourselves to the case of a single type
  for all dimensions, but we will treat the more general case in
  \todo{insert reference}.%
}
\begin{equation}
  \varphi_{\ßl,\ßi}\colon [\ß0, \ß1] \to \RR,\quad
  \varphi_{\ßl,\ßi}(\ßx)
  := \prod_{t=1}^d \varphi_{l_t,i_t}(x_t).
\end{equation}

\newgsymbol{Vl!}{$V_\ßl$}{Multivariate nodal space of level $\ßl$}%
The multivariate nodal space $V_\ßl$ is defined analogously to
the univariate case:
\begin{equation}
  V_\ßl
  := \spn\{\varphi_{\ßl,\ßi} \mid \ßi = \ß0, \dotsc, \ß2^{\ßl}\}.
\end{equation}
\newgsymbol{phili1!}{$\varphi_{\ßl,\ßi}^1$}{%
  Multivariate hat function of level $\ßl$, index $\ßi$%
}%
\newgsymbol{Vl1!}{$V_\ßl^1$}{%
  Multivariate nodal piecewise linear space of level $\ßl$%
}%
\newgsymbol{ab!}{$[\ßa, \ßb]$}{%
  Hypercube $:= [a_1, b_1] \times \dotsb \times [a_d, b_d]$%
}%
In the case of hat functions $\varphi_{\ßl,\ßi}^1$,
the nodal space $V_\ßl^1$ is the $d$-linear spline space, i.e.,
the space of all continuous functions
on $[\ß0, \ß1]$ that are piecewise $d$-linear polynomials on
all hyper-cubes
\begin{equation}
  [\ßx_{\ßl,\ßi}, \ßx_{\ßl,\ßi+\ß1}]
  := [x_{l_1,i_1}, x_{l_1,i_1+1}] \times \dotsb \times
  [x_{l_d,i_d}, x_{l_d,i_d+1}],\quad
  \ßi = \ß0, \dotsc, \ß2^\ßl - \ß1.
\end{equation}

We can now interpolate objective functions $f\colon [\ß0, \ß1] \to \RR$
in the nodal space $V_\ßl$.
\newgsymbol{fl!}{$f_\ßl$}{Interpolant of $f$ in $V_\ßl$}%
\newgsymbol{ci!}{$c_\ßi$}{Coefficients of a linear combination}%
A function $f_\ßl\colon [\ß0, \ß1] \to \RR$ is called \term{interpolant}
in $V_\ßl$, if
\begin{equation}
  \label{eq:interpFullGrid}
  f_\ßl
  = \sum_{\ßi=\ß0}^{\ß2^\ßl} c_\ßi \varphi_{\ßl,\ßi},\quad
  \fa{\ßi = \ß0, \dotsc, \ß2^\ßl}{f_\ßl(\ßx_{\ßl,\ßi}) = f(\ßx_{\ßl,\ßi})},
\end{equation}
where $c_\ßi \in \RR$ and
the sum is over all $\ßi = \ß0, \dotsc, \ß2^\ßl$.
\begin{lemma}[Linear independence of tensor products]
  \label{lemma:tensorProductLinearIndependence}
  The functions $\varphi_{\ßl,\ßi}$ ($\ßi = \ß0, \dotsc, \ß2^\ßl$)
  form a basis of $V_\ßl$, if the univariate functions
  $\varphi_{l,i}$ ($i = 0, \dotsc, 2^l$)
  form a basis of the univariate nodal space $V_l$.
\end{lemma}
\begin{proof}
  Assume that $c_\ßi \in \RR$ are chosen in \eqref{eq:interpFullGrid}
  such that $f_\ßl \equiv 0$.
  Then for all $\ßi' = \ß0, \dotsc, \ß2^\ßl$,
  we can evaluate \eqref{eq:interpFullGrid} at $\ßx_{\ßl,\ßi'}$ to obtain
  \begin{equation}
    \sum_{i_1=0}^{2^{l_1}}
    \left(\sum_{i_2=0}^{2^{l_2}} \dotsb
    \left(\sum_{i_d=0}^{2^{l_d}} c_\ßi \varphi_{l_d,i_d}(x_{l_d,i_d'})\right) \dotsb
    \varphi_{l_2,i_2}(x_{l_2,i_2'})\right) \varphi_{l_1,i_1}(x_{l_1,i_1'})
    = 0.
  \end{equation}
  We apply the linear independence in 1D ($x_1$ direction) to conclude that
  the sum over $i_2$ must vanish.
  Repeating this argument for all dimensions, we infer that $c_\ßi = 0$
  for all $\ßi = \ß0, \dotsc, \ß2^\ßl$,
  implying the linear independence of the functions $\varphi_{\ßl,\ßi}$.
\end{proof}
\newgsymbol{Omegal!}{$\Omega_\ßl$}{Set of full grid points of level $\ßl$}%
The lemma is equivalent to the statement that the coefficients $c_\ßi \in \RR$
exist for every objective function $f$ and are uniquely determined by
the values at the grid points
\begin{equation}
  \Omega_\ßl
  := \{\ßx_{\ßl,\ßi} \mid \ßi = \ß0, \dotsc, \ß2^{\ßl}\}.
\end{equation}
\newgsymbol{n}{$n$}{Level $\in \NN_0$ of full or sparse grid}
A common choice for the level $\ßl$ is $n \cdot \ß1$ for some $n \in \NN_0$.
\newgsymbol{Vnd}{$V_{n,d}$}{%
  Multivariate nodal space
  $:= V_{n \cdot \ß1}$ of level $n$ with dimensionality $d$%
}
\newgsymbol{Vnd1}{$V_{n,d}^1$}{%
  Multivariate nodal piecewise linear space
  $:= V_{n \cdot \ß1}^1$ of level $n$ with dimensionality $d$%
}
In this case, we replace ``$\ßl$'' in the subscripts with ``$n,d$''
(for example, $V_{n,d} := V_{n \cdot \ß1}$).
\newgsymbol{fnd1}{$f_{n,d}^1$}{Full grid interpolant $\in V_{n,d}^1$ of $f$}%
\newgsymbol{||.||L2}{$\norm{\cdot}_{L^2}$}{%
  $L^2$ norm $\norm{f}_{L^2} := \sqrt{\int_\Omega f(x)^2 \dx}$
  for a function $f\colon \Omega \to \RR$%
}
\newgsymbol{O}{$\calO(f(x))$}{Big-$\calO$ Landau notation}%
For the hat function basis $\varphi_{l,i}^1$,
it can be shown that the $L^2$ interpolation error of the interpolant
$f_{n,d}^1$ is given by
\begin{equation}
  \norm{f - f_{n,d}^1}_{L^2} = \calO(h_n^2),
\end{equation}
i.e., the order of the interpolation error is quadratic in the mesh size.
\todo{reference Höllig, as Acta is only for zero boundary}



\subsection{Hierarchical Subspaces}

\newgsymbol{dim}{$\dim$}{Vector space dimension}%
\newgsymbol{|.|}{$|\cdot|$}{%
  Absolute value of a scalar or the number of elements of a set%
}%
The dimension of the nodal space $V_\ßl$ is given by
\begin{equation}
  \label{eq:dimensionFG}
  \dim V_\ßl
  = |\Omega_\ßl|
  = \prod_{t=1}^d (2^{l_t} + 1).
\end{equation}
\newgsymbol{Omega}{$\Omega(f(x))$}{Big-$\Omega$ Landau notation}%
\newgsymbol{norm1}{$\norm{\cdot}_1$}{1-norm $\norm{\ßx}_1 := \sum_{t=1}^d x_t$}%
In asymptotic terms, this dimension is in $\Omega(2^{\norm{\ßl}_1})$,
where the 1-norm $\norm{\ßl}_1$ of $\ßl$ is defined as
$\norm{\ßl}_1 := \sum_{t=1}^d l_t$.
If we choose the same level $n \in \NN_0$ in all dimensions,
this means that the dimension of $V_{n,d}$ and the
number of grid points grow at least as fast as
$2^{nd} = (h_n^{-1})^d$.
This exponential dependency between $\dim V_{n,d}$ and $d$ is known as the
\term{curse of dimensionality}.
The curse makes interpolation on $V_\ßl$ computationally infeasible
for dimensionalities $d > 4$,
as we would have to calculate and store $\dim V_\ßl$ coefficients $c_\ßi$.%
\footnote{%
  The number of necessary basis evaluations to evaluate the interpolant once
  would not be as large, as most types of basis functions
  (like hat functions $\varphi_{l,i}^1$ and higher-order B-splines)
  are locally supported.%
}

\subsubsection{Hierarchical Splitting in the Univariate Case}

In order to reduce the computational effort,
we first split $V_\ßl$ into smaller subspaces and then identify
which subspaces are most important and which subspaces can be omitted
at the cost of a slightly larger error.
In the univariate case, the key observation is that a grid point of a level $l$
can be written as a grid point of a higher level $l'$:
\begin{equation}
  x_{l,i} = x_{l',i'},\quad
  l' \ge l,\quad
  i' = 2^{l'-l} i.
\end{equation}
\newgsymbol{xor}{$\xor$}{Bitwise ``exclusive or''}%
Conversely, this implies that every grid point $x_{l,i}$ can be uniquely written
as a grid point of a coarser level $l'$ and an odd index $i'$:
\begin{equation}
  x_{l,i} = x_{l',i'},\quad
  l' =
  \begin{cases}
    l - \left[\log_2(\xor(i, i-1) + 1) - 1\right],&l > 0,\\
    l,&l = 0,\\
  \end{cases}\quad
  i' = 2^{l'-l} i,
\end{equation}
% https://mathoverflow.net/a/29973
where $\xor$ is the bitwise ``exclusive or'' function.
The term in brackets is the exponent of the
highest power of two that divides $i$.
\newgsymbol{u!}{$\dotcup$}{%
  Disjoint union of sets (union where the pairwise intersection of
  the joined sets is empty)%
}%
\newgsymbol{Il}{$I_l$}{Set of (odd) indices for hierarchical basis functions}%
This implies that $\Omega_l$ decomposes as
\begin{equation}
  \Omega_l
  = \bigdotcup_{l'=0}^l \{x_{l',i'} \mid i' \in I_{l'}\},\quad
  I_l :=
  \begin{cases}
    \{i = 0, \dotsc, 2^l \mid \text{$i$ odd}\},&l > 0,\\
    \{0, 1\},&l = 0,
  \end{cases}
\end{equation}
where $\dotcup$ indicates the disjoint union.
\newgsymbol{Wl}{$W_l$}{Hierarchical subspace of level $l$}%
We call the spaces spanned by the basis functions that correspond to the
joined sets \term{hierarchical subspaces} $W_l$:
\begin{equation}
  W_l
  := \spn\{\varphi_{l,i} \mid i \in I_l\}.
\end{equation}

\newgsymbol{oplus}{$\oplus$}{%
  Direct sum of vector spaces (vector space sum in which the dimension of
  the sum equals the sum of the summands' dimensions)%
}%
For the hat function basis $\varphi_{l,i}^1$ and other basis types,
one can prove that the corresponding nodal space
decomposes into the direct sum of all
hierarchical subspaces of coarser levels or the same level, i.e.,
\begin{equation}
  \label{eq:hierSplittingUV}
  V_l
  = \bigoplus_{l'=0}^l W_{l'},
\end{equation}
Here, the direct sum $\oplus$ is
the normal vector space sum with the additional indication
that the dimension of the sum is the sum $\sum_{l'=0}^l W_{l'}$
of the dimensions of the summands
(analogously to $|\Omega_l| = \sum_{l'=0}^l |\Omega_{l'}|$,
where $\Omega_l$ is the disjoint union of the sets $\Omega_{l'}$).
In general, \eqref{eq:hierSplittingUV} may not be true.
The following lemma provides a characterization,
which can be used to prove \eqref{eq:hierSplittingUV} for hat functions.
\begin{lemma}[Hierarchical splitting characterization]
  \label{lemma:hierSplittingUV}
  Equivalent to relation \eqref{eq:hierSplittingUV} is the satisfaction of
  both of the following conditions:
  \begin{itemize}
    \item
    The hierarchical subspaces $W_{l'}$ ($l' \le l$) are subspaces of $V_l$.
    
    \item
    The basis functions $\varphi_{l',i'}$ ($l' \le l$, $i' \in I_{l'}$)
    are linearly independent.
  \end{itemize}
\end{lemma}
\begin{proof}
  The first condition is equivalent to $\sum_{l'=0}^l W_{l'} \subset V_l$.
  The second condition is equivalent to
  $\dim \sum_{l'=0}^l W_{l'} = \sum_{l'=0}^l \dim W_{l'}$,
  i.e., to the directness of the sum.
  Therefore, the logical conjunction of both is equivalent to
  $\bigoplus_{l'=0}^l W_{l'} \subset V_l$.
  If the sum is direct,
  the dimension of the sum is equal to $2 + \sum_{l'=1}^l 2^{l'-1} = 2^l + 1$
  (due to $\dim W_{l'} = |I_{l'}| = 2^{l'-1}$ for $l' > 0$ and
  $\dim W_{l'} = 2$ for $l' = 0$),
  which is also the dimension of $V_l$.
  The only subspace of $V_l$ that has the same dimension as $V_l$ is $V_l$ itself,
  so we infer $\bigoplus_{l'=0}^l W_{l'} = V_l$.
\end{proof}
\begin{corollary}
  The hierarchical splitting \eqref{eq:hierSplittingUV}
  holds for the hat function basis.
\end{corollary}
\begin{proof}
  The first condition of \cref{lemma:hierSplittingUV}
  is satisfied as piecewise linear splines of level $l'$
  are also piecewise linear splines of higher levels $l \ge l'$.
  The linear independence for the second condition can be proved by induction
  over $l$:
  If a linear combination of $\varphi_{l',i'}^1$ ($l' \le l$, $i' \in I_{l'}$)
  vanishes everywhere, then the coefficients of level $l$ must be zero,
  as otherwise the basis functions $\varphi_{l,i'}^1$ ($i \in I_l$) would
  introduce kinks at $x_{l,i'}$, which the zero function does not have.
  This means that we have a zero linear combination of $\varphi_{l',i'}^1$ for
  $l' \le l - 1$, $i' \in I_{l'}$,
  and by the induction hypothesis, the other coefficients also vanish.
\end{proof}

\subsubsection{Hierarchical Splitting in the Multivariate Case}

\newgsymbol{Wl!}{$W_\ßl$}{Multivariate hierarchical subspace of level $\ßl$}%
\newgsymbol{Il!}{$I_\ßl$}{%
  Set $:= I_{l_1} \times \dotsb \times I_{l_d}$ of
  (odd) multivariate indices for hierarchical basis functions%
}%
Multivariate hierarchical subspaces of level $\ßl$
are defined analogously to the univariate case:
\begin{equation}
  W_\ßl
  := \spn\{\varphi_{\ßl,\ßi} \mid \ßi \in I_\ßl\},\quad
  I_\ßl
  := I_{l_1} \times \dotsb \times I_{l_d}.
\end{equation}
The splitting \eqref{eq:hierSplittingUV} can now be generalized to the
multivariate case:
\begin{equation}
  \label{eq:hierSplittingMV}
  V_\ßl
  = \bigoplus_{\ßl'=0}^\ßl W_{\ßl'},
\end{equation}
Again, this relation does not hold in general.
A multivariate counterpart of \cref{lemma:hierSplittingUV} can be used
to prove that \eqref{eq:hierSplittingMV} holds if
the corresponding 1D relation \eqref{eq:hierSplittingUV} holds for all dimensions:
\begin{lemma}[Multivariate hierarchical splitting characterization]
  \label{lemma:hierSplittingMV}
  Equivalent to relation \eqref{eq:hierSplittingMV} is the satisfaction of
  both of the following conditions:
  \begin{itemize}
    \item
    The hierarchical subspaces $W_{\ßl'}$ ($\ßl' \le \ßl$) are subspaces of $V_\ßl$.
    
    \item
    The basis functions $\varphi_{\ßl',\ßi'}$ ($\ßl' \le \ßl$, $\ßi' \in I_{\ßl'}$)
    are linearly independent.
  \end{itemize}
\end{lemma}
\begin{proof}
  If the sum is direct, then its dimension is given by
  \begin{equation}
    \dim \sum_{\ßl'=0}^\ßl W_{\ßl'}
    = \sum_{l_1'=0}^{l_1} \dotsb \sum_{l_d'=0}^{l_d}
    \prod_{t=1}^d \dim W_{l_t'}
    = \prod_{t=1}^d \sum_{l_t'=0}^{l_t} \dim W_{l_t'}
    = \prod_{t=1}^d (2^{l_t} + 1)
    = \dim V_\ßl
  \end{equation}
  using \eqref{eq:dimensionFG}.
  The rest is analogous to the proof of \cref{lemma:hierSplittingUV}.
\end{proof}
\begin{proposition}
  If univariate splitting \eqref{eq:hierSplittingUV} holds for every dimension,
  then the multivariate splitting \eqref{eq:hierSplittingMV} holds as well.
\end{proposition}
\begin{proof}
  We check the two conditions of \cref{lemma:hierSplittingMV}
  given the two univariate conditions of \cref{lemma:hierSplittingUV}:
  \begin{enumerate}
    \item
    The hierarchical basis functions $\varphi_{\ßl',\ßi'}$
    of $W_{\ßl'}$ ($\ßl' \le \ßl$, $\ßi' \in I_{\ßl'}$) are tensor products
    of functions $\varphi_{l_t',i_t'}$.
    According to the first condition of \cref{lemma:hierSplittingUV},
    each of these functions can be written as a linear combination of
    the nodal basis $\varphi_{\ßl,\ßi}$ ($\ßi = \ß0, \dotsc, \ß2^\ßl$).
    The tensor product can be expanded to a linear combination
    of tensor products of nodal basis functions.
    Therefore, $\varphi_{\ßl',\ßi'}$ is a linear combination of
    multivariate nodal functions, i.e., $\varphi_{\ßl',\ßi'} \in V_\ßl$.
    As this is true for all $\ßi' \in I_{\ßl'}$, we obtain $W_{\ßl'} \subset V_\ßl$.
    
    \item
    The linear independence of the hierarchical functions $\varphi_{\ßl',\ßi'}$
    ($\ßl' \le \ßl$, $\ßi' \in I_{\ßl'}$) can be shown completely analogously
    to the proof of \cref{lemma:tensorProductLinearIndependence}.\qedhere
  \end{enumerate}
\end{proof}
\begin{corollary}
  The multivariate hierarchical splitting \eqref{eq:hierSplittingMV}
  holds for the hat function basis.\qed
\end{corollary}



\subsection{Sparse Grids}

The idea of sparse grids is to use the
hierarchical splitting \eqref{eq:hierSplittingUV}
to keep only the most ``important'' hierarchical subspaces,
omitting the remaining ones.
There are three main ``flavors'' of sparse grids:
regular, dimensionally adaptive, and spatially adaptive.

\subsubsection{Regular Sparse Grids}

To assess the importance of a subspace, we consider again the
interpolant $f_\ßl \in V_\ßl$ of a function $f\colon [\ß0, \ß1] \to \RR$.
According to the splitting \eqref{eq:hierSplittingUV}, the interpolant can
be written as
\begin{equation}
  \label{eq:interpHierFullGrid}
  f_\ßl
  = \sum_{\ßl'=0}^\ßl \sum_{\ßi'=\ß0}^{\ß2^{\ßl'}}
  \alpha_{\ßl',\ßi'} \varphi_{\ßl',\ßi'},\quad
  \fa{\ßi = \ß0, \dotsc, \ß2^\ßl}{f_\ßl(\ßx_{\ßl,\ßi}) = f(\ßx_{\ßl,\ßi})}.
\end{equation}
\newgsymbol{alphali!}{$\alpha_{\ßl,\ßi}$}{%
  Hierarchical surpluses (coefficients of a linear combination of hierarchical
  basis functions)%
}%
The coefficients $\alpha_{\ßl',\ßi'}$ with respect to the hierarchical basis
$\varphi_{\ßl',\ßi'}$ are the \term{hierarchical surpluses}.
When using the hat function basis $\varphi_{\ßl,\ßi}^1$,
one can prove the following representation
for the corresponding surpluses:
\begin{equation}
  \alpha_{\ßl,\ßi}
  = (-1)^d 2^{-\norm{\ßl+\ß1}_1}
  \int_\ß0^\ß1 \varphi_{\ßl,\ßi}^1(\ßx)
  \frac{\partial^{2d}}{\partial x_1^2 \dotsb \partial x_d^2} f(\ßx) \diff{}\ßx,
\end{equation}
if $\ßl \ge \ß1$ and
$f$ is twice continuously differentiable in every dimension simultaneously,
i.e., $\frac{\partial^{2d}}{\partial x_1^2 \dotsb \partial x_d^2} f$
exists and is continuous.%
\footnote{%
  Again, the notation implies that the integration domain is
  the hypercube $[\ß0, \ß1]$.%
}\multiplefootnoteseparator%
\footnote{%
  The statement is even valid for functions in the Sobolev space
  $H_\mathrm{mix}^2([\ß0, \ß1])$ with dominating mixed derivative,
  as its proof mainly relies on integration by parts.%
}
%This equation provides a direct relation between the hat function surpluses
%and the second mixed derivative of the objective function, which has
%two consequences.
%First, the absolute value of the surpluses is large in regions where
%the absolute value of the second mixed derivative is large, i.e.,
%where the objective function oscillates strongly.
%Second, the absolute surpluses decay with
%increasing level $\ßl$ as both the factor
%$2^{-\norm{\ßl+\ß1}_1}$ and the size of the support of $\varphi_{\ßl,\ßi}$
%are decreasing.
Consequently, the contribution of the summand of level $\ßl$
can be estimated by
\begin{equation}
  \label{eq:componentEstimation}
  \bignorm{\sum_{\ßi'=\ß0}^{\ß2^{\ßl'}}
  \alpha_{\ßl',\ßi'} \varphi_{\ßl',\ßi'}^1}_{L^2}
  \le 3^{-d} \cdot 2^{-2 \norm{\ßl}_1} \cdot
  \bignorm{\frac{\partial^{2d}}{\partial x_1^2 \dotsb \partial x_d^2} f}_{L^2}
\end{equation}
for the hat function surpluses $\alpha_{\ßl',\ßi'}$.

Equation \eqref{eq:componentEstimation} motivates to omit those summands
from the sum \eqref{eq:interpHierFullGrid} whose level sum $\norm{\ßl}_1$
exceeds a certain value $n \in \NN_0$.
The selection of the relevant subspaces can be formulated as a
continuous knapsack problem.
\newgsymbol{Vnds}{$V_{n,d}^\sparse$}{%
  Regular sparse grid space of level $n$ with dimensionality $d$%
}%
\newgsymbol{Omegands}{$\Omega_{n,d}^\sparse$}{%
  Set of sparse grid points of level $n$ with dimensionality $d$%
}%
The resulting function space and grid point set
\begin{equation}
  V_{n,d}^\sparse
  := \bigoplus_{\norm{\ßl}_1 \le n} W_{\ßl},\qquad
  \Omega_{n,d}^\sparse
  := \bigdotcup_{\norm{\ßl}_1 \le n}
  \{\ßx_{\ßl,\ßi} \mid \ßi \in I_{\ßl}\}
\end{equation}
are called \term{regular sparse grid space} and
\term{regular sparse grid} of level $n$, respectively.
To better distinguish the different grids,
we call the nodal spaces and grids \term{full grids}.
Although sparse grids have been motivated using the hat function
basis $\varphi_{\ßl,\ßi}^1$,
we generalize the definition to arbitrary bases $\varphi_{\ßl,\ßi}$.
\newgsymbol{dOmega}{$\partial \Omega$}{Boundary of the domain $\Omega \subset \RR^d$}%
One can prove that for homogeneous boundary conditions
$f|_{\partial[\ß0,\ß1]} \equiv 0$,
the number of required inner grid points
($\ßx_{\ßl,\ßi} \in \Omega_{n,d}^\sparse$ where $\ßl \ge \ß1$)
grows like $\calO(h_n^{-1} (\log_2 h_n^{-1})^{d-1})$, which is much less than
the corresponding number $\calO((h_n^{-1})^d)$ in the full grid case
(see \eqref{eq:dimensionFG}).
In particular, the exponential dependency on the dimensionality $d$
has vanished.
\newgsymbol{fnds1}{$f_{n,d}^{\sparse,1}$}{%
  Sparse grid interpolant $\in V_{n,d}^{\sparse,1}$ of $f$%
}%
The corresponding $L^2$ error of the sparse grid interpolant
$f_{n,d}^{\sparse,1}$ using hat functions
(still assuming homogeneous boundary conditions) decays like
\begin{equation}
  \norm{f - f_{n,d}^{\sparse,1}}_{L^2} = \calO(h_n^2 (\log_2 h_n^{-1})^{d-1}),
\end{equation}
which is only slightly worse than the full grid error by the factor of
$(\log_2 h_n^{-1})^{d-1}$.

\subsubsection{Dimensionally Adaptive Sparse Grids}

The idea of dimensional adaptivity is to spend more grid
points in specific dimensions depending on the objective function.
Different criteria for the choice of dimensions exists,
for example the absolute value of the linear hierarchical surpluses
or the ANOVA decomposition.
\todo{not sure about that one}
To incorporate dimensional adaptivity into sparse grids,
one has to generalize the symmetric
choice of subspaces in the definition of regular sparse grids
to allow for an asymmetric preference.
\newgsymbol{Vs}{$V^\sparse$}{%
  Arbitrary sparse grid space (possibly spatially adaptive)%
}%
\newgsymbol{Omegas}{$\Omega^\sparse$}{%
  Arbitrary sparse grid (possibly spatially adaptive)%
}%
\newgsymbol{L}{$L$}{Finite subset $L \subset \NN_0^d$ of levels}%
Generally, function spaces $V^\sparse$ and grid sets $\Omega^\sparse$
of \term{dimensionally adaptive sparse grids} have the form
\begin{equation}
  V^\sparse
  = \bigoplus_{\ßl \in L} W_\ßl,\qquad
  \Omega^\sparse
  = \bigdotcup_{\ßl \in L} \{\ßx_{\ßl,\ßi} \mid \ßi \in I_\ßl\},
\end{equation}
where $L$ is a \term{downward closed} set, i.e.,
a finite subset $L \subset \NN_0^d$
for which $\forall_{\ßl \in L} \fa{\ßl' \le \ßl}{\ßl' \in L}$.
Regular sparse grids are a special case by setting
$L = \{\ßl \in \NN_0^d \mid \norm{\ßl}_1 \le n\}$.

The key advantage of dimensionally adaptive sparse grids over
spatially adaptive approaches lies in the
so-called \term{combination technique}.
\newgsymbol{cli!}{$c_{\ßl,\ßi}$}{%
  Interpolation coefficients for the full grid $V_\ßl$%
}%
For regular sparse grids, one can show that the sparse grid interpolant
$f_{n,d}^\sparse$ can be written as
\begin{equation}
  \label{eq:combiTechnique}
  f_{n,d}^\sparse
  = \sum_{q=0}^{d-1} (-1)^q \binom{d-1}{q} \sum_{\norm{\ßl}_1 = n-q}
  \sum_{\ßi=\ß0}^{\ß2^\ßl} c_{\ßl,\ßi} \varphi_{\ßl,\ßi},
\end{equation}
where the $c_{\ßl,\ßi} \in \RR$ ($\ßi = \ß0, \dotsc, \ß2^\ßl$)
are the interpolation coefficients on the full grid of level $\ßl$, i.e.,
$\fa{\ßi' = \ß0, \dotsc, \ß2^\ßl}{%
  \sum_{\ßi=\ß0}^{\ß2^\ßl} c_{\ßl,\ßi} \varphi_{\ßl,\ßi}(\ßx_{\ßl,\ßi'})
  = f(\ßx_{\ßl,\ßi'})%
}$.
For general dimensionally adaptive sparse grids, a similar formula exists.
\todo{citation}
The combination formula \eqref{eq:combiTechnique} decomposes the
sparse grid interpolant into a weighted sum of full grid interpolants.
In applications, each of the grid can be processed in parallel,
allowing to drastically speed up computations like the solution of
\pde{}s.
In addition, existing software working with nodal bases do not have to be
rewritten in terms of hierarchical functions,
which means that combination technique allows sparse grids to be employed
in a minimally invasive way.

\subsubsection{Spatially Adaptive Sparse Grids}

Dimensional adaptivity does not suffice to resolve local features of the
objective function.
Especially in some applications, it is crucial for the
interpolant to have a high accuracy in specific regions of the domain
For instance in optimization, it is not necessary to have a small global
interpolation error, but rather high accuracy near the optima.

This can be achieved by \term{spatially adaptive sparse grids},
on which this thesis focuses.
\newgsymbol{K}{$K$}{Finite set of level-index pairs $(\ßl,\ßi)$}%
In the most general form, their function spaces and grid sets have the form
\begin{equation}
  V^\sparse
  = \spn\{\varphi_{\ßl,\ßi} \mid (\ßl,\ßi) \in K\},\qquad
  \Omega^\sparse
  = \{\ßx_{\ßl,\ßi} \mid (\ßl,\ßi) \in K\},
\end{equation}
where $K$ is a finite set of level-index pairs $(\ßl,\ßi)$
with $\ßl \in \NN_0^d$ and $\ßi \in I_\ßl$.
Often, one has to make specific assumptions about $K$ in order for
sparse grid algorithms to work correctly.
\newgsymbol{et!}{$\ße_t$}{$t$-th unit vector $(0, \dotsc, 0, 1, 0, \dotsc, 0)$}%
For example for hat functions $\varphi_{\ßl,\ßi}^1$, the grid should contain
the hierarchical ancestors of every grid point:
\begin{equation}
  \label{eq:hierAncestors}
  \forall_{(\ßl,\ßi) \in K}
  \fa{\{t = 1, \dotsc, d \mid l_t \ge 1\}}{(\ßl',\ßi') \in K},\quad
  \ßl' := \ßl - \ße_t,\quad
  i_{t'}' :=
  \begin{cases}
    2 \lfloor i_t/4 \rfloor + 1,&t = t',\\
    i_{t'},&t \not= t',
  \end{cases}
\end{equation}
where $\ße_t$ is the $t$-th unit vector and $t' = 1, \dotsc, d$.
If \eqref{eq:hierAncestors} is not met, the so-called
unidirectional principle, which is used for instance to efficiently calculate
hierarchical surpluses, does not work.
However, as we will see, the unidirectional principle cannot be applied
to B-splines of general degree.
Therefore, for most of our considerations, we will not restrict the
choice of $K$.

\subsection{Boundary Treatment}

One issue of regular sparse grids $\Omega_{n,d}^\sparse$ as we have defined them
is that the number of grid points still grows very fast
with the level $n$ and the dimensionality $d$,
This is mainly because the finest mesh size $h_n$ on the
boundary of the domain $[\ß0, \ß1]$ is finer than
the finest mesh size $h_{n-d+1}$ that can be found in the interior.
If we define $\interior{\Omega}_{n,d}^\sparse$ as the set of
interior grid points in $\Omega_{n,d}^\sparse$, i.e.,%
\footnote{%
  Note that in the literature,
  the regular sparse grid space of level $n$ without boundary points is often
  defined via $\norm{\ßl}_1 \le n + d - 1$ to ensure that the finest mesh size
  is given by $h_n$.
  In our notation, this corresponds to $\interior{\Omega}_{n+d-1,d}^\sparse$.%
}
\begin{equation}
  \interior{\Omega}_{n,d}^\sparse
  := \Omega_{n,d}^\sparse \cap \mathopen]\ß0, \ß1\mathclose[
  = \{\ßx_{\ßl,\ßi} \in \Omega_{n,d}^\sparse \mid \ßl \ge \ß1\},
\end{equation}
then the following relation about the number of grid points
of $\Omega_{n,d}^\sparse$ can be shown:
\begin{lemma}[Number of grid points of $\Omega_{n,d}^\sparse$]
  \label{lemma:numberOfGridPoints}
  \setlength{\abovedisplayskip}{0pt}
  \begin{equation}
    |\Omega_{n,d}^\sparse|
    = \sum_{q=0}^d 2^q \binom{d}{q} |\interior{\Omega}_{n+q,d-q}^\sparse|
  \end{equation}
\end{lemma}
\begin{proof}
  See \todo{insert Acta}.
  The proof will also be similar to the proof of \todo{insert reference}.
\end{proof}
Here, we define $0$-dimensional grids to contain exactly one grid point
(the empty tuple).

Intuitively, \cref{lemma:numberOfGridPoints} splits the sparse grid
$\Omega_{n,d}^\sparse$ into lower-dimensional sparse grids
$\interior{\Omega}_{n+q,d-q}^\sparse$ without boundary points.
The factor $2^q \binom{d}{q}$ is the number of $(d-q)$-dimensional faces
of the $d$-dimensional unit hypercube.
For example, the 3D cube decomposes into
$2^0 \binom{3}{0} = 1$ interior cube $\mathopen]0, 1\mathclose[^3$,
$2^1 \binom{3}{1} = 6$ sides ($2$-dimensional faces)
like $\mathopen]0, 1\mathclose[^2 \times \{0\}$,
$2^2 \binom{3}{2} = 12$ edges ($1$-dimensional faces)
like $\mathopen]0, 1\mathclose[ \times \{(0, 0)\}$, and
$2^3 \binom{3}{3} = 8$ corners ($0$-dimensional faces)
like $(0, 0, 0)$.
On each of these $(d-q)$-dimensional faces,
the sparse grid $\Omega_{n,d}^\sparse$ contains
the interior of a sparse grid of level $n + q$,
the size of which grows like $\calO(2^{n+q} n^{d-q-1})$.
As the number of boundary faces increases exponentially
with the dimensionality $d$,
the size of $\Omega_{n,d}^\sparse$ can exhaust computational memory
quite fast.
There are mainly two approaches to deal with this issue.

\subsubsection{Sparse Grids with Coarser Boundaries}

One solution is to insert the boundary level functions and grid points
at a finer level than zero.
A popular choice is the insertion at level one, which corresponds to
\begin{equation}
  \label{eq:sparseGridB1}
  \Omega_{n,d}^{\sparse(1)}
  := \bigdotcup_{\ßl \in L_{n,d}^{\sparse(1)}}
  \{\ßx_{\ßl,\ßi} \mid \ßi \in I_\ßl\},\quad
  L_{n,d}^{\sparse(1)}
  := \{\ßl \in \NN_0^d \mid \norm{\mathop{\vec{\max}}(\ßl, \ß1)}_1 \le n\},
\end{equation}
where $\vec{\max}$ is to be read coordinate-wise as usual.
This choice is equivalent to treating zero level components as level one
in the subspace selection.
This ensures that the finest mesh sizes in interior of $[\ß0, \ß1]$ and
its boundary coincide to be $h_{n-d+1}$, which reduces the number of grid points
on the boundary significantly.

Another solution that can be found in the literature about sparse grids with
hat functions is (in the univariate case)
to start with the constant function on level zero with
corresponding grid point $0.5$,
then employ the two boundary functions and points on level one,
and finally proceed as usual for the finer levels $\ge 2$.
Apart from a constant shift in the level of the resulting sparse grids,
this is equivalent to inserting the boundary functions and points at level two.
This solution leads to even less grid points than the previous approach,
as now the mesh size is finer in the interior of the domain than on the
boundary.
However, for very high dimensionalities this might still lead to
computationally infeasible sparse grids.

We generalize these two solutions to the definition of a
sparse grid $\Omega_{n,d}^{\sparse(b)}$ that is equivalent to inserting
the boundary functions and points at a level $b \in \NN$:
\begin{definition}
  The regular sparse grid of level $n \ge d$,
  dimensionality $d \in \NN$, and boundary level $b \in \NN$ is defined as
  \begin{equation}
    \begin{split}
      \Omega_{n,d}^{\sparse(b)}
      := \bigdotcup_{\ßl \in L_{n,d}^{\sparse(b)}}
      \{\ßx_{\ßl,\ßi} \mid \ßi \in I_\ßl\},&\quad
      L_{n,d}^{\sparse(b)}
      := \{\ßl \in \NN^d \mid \norm{\ßl}_1 \le n\} \dotcup {}\\[-1em]
      &\big(\{\ßl \in \NN_0^d \setminus \NN^d \mid
      \norm{\mathop{\vec{\max}}(\ßl, \ß1)}_1 \le n-b+1\} \cup \{\ß0\}\big).
    \end{split}
  \end{equation}
\end{definition}
The definition is motivated by partitioning the levels $\ßl \in \NN_0^d$
into interior levels ($\ßl \in \NN^d$)
and boundary levels ($\ßl \in \NN_0^d \setminus \NN^d$).
By including the levels of the interior grid $\interior{\Omega}_{n,d}^\sparse$,
the mesh size in the interior is the same as before ($h_{n-d+1}$).
Like in \eqref{eq:sparseGridB1}, we treat boundary levels as level one,
but we subtract $b - 1$ from the upper bound to ensure the correct
mesh size $h_{n-d-b+2}$ on the boundary.
We add $\ß0$ to the level set to ensure that at least the $2^d$ corner
points are included in the resulting sparse grid.
Note that this definition is consistent with \eqref{eq:sparseGridB1} as
$L_{n,d}^{\sparse(b)}
= \{\ßl \in \NN_0^d \mid \norm{\mathop{\vec{\max}}(\ßl, \ß1)}_1 \le n\}$
for $b = 1$.

The number of grid points of $\Omega_{n,d}^{\sparse(b)}$
can be calculated as follows:
\begin{proposition}[Number of grid points of $\Omega_{n,d}^{\sparse(b)}$]
  \setlength{\abovedisplayskip}{0pt}
  \begin{equation}
    |\Omega_{n,d}^{\sparse(b)}|
    = |\interior{\Omega}_{n,d}^\sparse| +
    \sum_{q=1}^d 2^q \binom{d}{q} |\interior{\Omega}_{n-b+1,d-q}^\sparse|
  \end{equation}
\end{proposition}
\begin{proof}
  See appendix.
  \todo{add proof}
\end{proof}

\Cref{alg:coarseBoundary} shows how to generate the necessary set of
hierarchical levels.
Its correctness can be formally proved with the following invariant:
\begin{proposition}[Invariant of \cref{alg:coarseBoundary}]
  \label{prop:invariantCoarseBoundary}
  \label{PROP:INVARIANTCOARSEBOUNDARY}
  After iteration $t$ of the algorithm ($t = 1, \dotsc, d$), it holds
  \begin{equation}
    \label{eq:coarseInvariant}
    \begin{split}
      L^{(t)}
      &= \{\ßl \in \NN^t \mid \norm{\ßl}_1 \le n - d + t\} \dotcup {}\\
      &\hphantom{{}={}} \big(\{\ßl \in \NN_0^t \setminus \NN^t \mid
      \norm{\mathop{\vec{\max}}(\ßl, \ß1)}_1 \le n-d+t-b+1\} \cup \{\ß0\}\big).
    \end{split}
  \end{equation}
\end{proposition}
\begin{proof}
  See \cref{sec:proofInvariantCoarseBoundary}.
\end{proof}
\begin{shortcorollary}
  \Cref{alg:coarseBoundary} is correct.
\end{shortcorollary}
\begin{proof}
  Follows immediately from \cref{prop:invariantCoarseBoundary}
  by setting $t = d$.
\end{proof}

\begin{algorithm}
  \begin{algorithmic}[1]
    \Function{$L_{n,d}^{\sparse(b)} =$ computeSGCoarseBoundary}{%
      $n$, $d$, $b$%
    }
      \State{$L^{(1)} \gets \{0, 1, \dotsc, n - d + 1\}$}
      \For{$t = 2, \dotsc, d$}
        \State{$L^{(t)} \gets \emptyset$}
        \For{$\ßl \in L^{(t-1)}$}
          \If{%
            $\norm{\mathop{\vec{\max}}(\ßl, \ß1)}_1 \le n - d + t - b$ or
            $\ßl = \ß0$%
          }%
          \label{line:algCoarseBoundary1}
            \State{$L^{(t)} \gets L^{(t)} \cup \{(\ßl, 0)\}$}
          \EndIf{}
          \If{$\ßl \in \NN^{t-1}$}
            \State{$l^\ast \gets n - d + t - \norm{\ßl}_1$}%
            \label{line:algCoarseBoundary2}
          \Else{}
            \State{%
              $l^\ast \gets n - d + t - b + 1 -
              \norm{\mathop{\vec{\max}}(\ßl, \ß1)}_1$%
            }%
            \label{line:algCoarseBoundary3}
          \EndIf{}
          \State{%
            $L^{(t)} \gets L^{(t)} \cup
            \{(\ßl, l_t) \mid l_t = 1, \dotsc, l^\ast\}$%
          }
        \EndFor{}
      \EndFor{}
      \State{\Return{$L^{(d)}$}}
    \EndFunction{}
  \end{algorithmic}
  \caption{%
    Generation of the level set $L_{n,d}^{\sparse(b)}$ corresponding
    to the sparse grid $\Omega_{n,d}^{\sparse(b)}$ with coarse boundaries.
    Inputs are the level $n \ge d$, the dimensionality $d \in \NN$, and
    the boundary parameter $b \in \NN$.%
  }
  \label{alg:coarseBoundary}
\end{algorithm}

\todo{add table with numbers}

\todo{add note about hierarchization with hat functions}

\subsubsection{Sparse Grids Without Boundary Points and Modified Bases}

\todo{write}

\blindtext{}