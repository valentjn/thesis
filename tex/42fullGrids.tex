\section{Hierarchization on Full Grids (Unidirectional Principle)}
\label{sec:42fullGrids}

If $\sgset$ is a full grid $\fgset{\*l}$
(see \cref{sec:21nodalSpaces}),
the well-known \emph{\up}
can be used to apply $\linop$ to input data $\vlinin$.
As shown in \cref{fig:unidirectionalPrinciple}, the idea of the \up
is to apply the corresponding one-dimensional operators on the
one-dimensional subgrids (the \emph{poles}) of $\sgset$,
which is repeated for all dimensions.
In this section, we first formulate the \up for
general linear operators $\linop$ and then prove its correctness for
the case $\linop = \intpmat^{-1}$ of hierarchization.
The correctness for the general case of arbitrary tensor product operators
will follow from the considerations in \cref{sec:45spatAdaptiveUP}.

\begin{figure}
  \includegraphics{unidirectionalPrinciple_1}%
  \caption[%
    Unidirectional principle%
  ]{%
    Application of a linear operator $\linop$
    on two-dimensional sparse grid data with the unidirectional principle.
    First, the univariate operator $\linop_1$ is applied on
    the input data $\vlinin$
    along poles of the first dimension $x_1$ \emph{(left)}.
    All grid points of the same color are part of the same pole
    (equivalence classes of ``$\samepole{t}$'' in
    \cref{alg:unidirectionalPrinciple}).
    Second, the univariate operator $\linop_2$ is applied on the
    resulting intermediate data $\vlinout[(1)]$
    along poles of the second dimension $x_2$ \emph{(center)}.
    This gives the final values $\vlinout = \linop[\vlinin]$ \emph{(right)}.%
  }%
  \label{fig:unidirectionalPrinciple}%
\end{figure}

\paragraph{Unidirectional principle and its correctness}

The \up is given in \cref{alg:unidirectionalPrinciple}.
The algorithm is given a permutation $(t_1, \dotsc, t_d)$ of $(1, \dotsc, d)$
that specifies the order of dimensions in which the \up should be applied.
We denote with $\linop_{t_j}$ the one-dimensional version of $\linop$
applied in dimension $t_j$ ($j = 1, \dotsc, d$).
With the following invariant, we can prove the correctness of the \up.

\begin{algorithm}
  \begin{algorithmic}[1]
    \Function{$\vlinout =$ unidirectionalPrinciple}{%
      $\liset$, $(t_1, \dotsc, t_d)$, $\vlinin$%
    }
      \State{$\vlinout[(0)] \gets \vlinin$}
      \For{$j = 1, \dotsc, d$}
        \State{%
          Define equivalence relation $\samepole{t_j}$ on $\liset$
          as $\*k \samepole{t_j} \*k' \iff \*k_{-t_j} = \*k'_{-t_j}$%
        }
        \For{$\liset_\mathrm{pole} \in \eqclasses{\liset}{\samepole{t_j}}$}
          \State{%
            $(\linout[(j)]{\*k})_{\*k \in \liset_\mathrm{pole}} \gets
            \linop_{t_j}
            \Big[(\linout[(j-1)]{\*k})_{\*k \in \liset_\mathrm{pole}}\Big]$%
          }
          \Comment{apply 1D operator on pole}%
          \label{line:algUnidirectionalPrinciple1}
        \EndFor{}
      \EndFor{}
      \State{$\vlinout \gets \vlinout[(d)]$}
    \EndFunction{}
  \end{algorithmic}
  \caption[%
    Unidirectional principle%
  ]{%
    Application of a tensor product operator $\linop$ with
    the unidirectional principle.
    Inputs are the set $\liset$ of grid indices,
    the permutation $(t_1, \dotsc, t_d)$ specifying the order in which
    the one-dimensional operators $\linop_{t_j}$ should be applied, and
    the vector $\vlinin = (\linin{\*k})_{\*k \in \liset}$ of input data.
    The output is the vector $\vlinout = (\linout{\*k})_{\*k \in \liset}$
    of output data.%
  }%
  \label{alg:unidirectionalPrinciple}%
\end{algorithm}

\begin{proposition}[%
  invariant of \cref{alg:unidirectionalPrinciple} for hierarchization%
]
  \label{prop:invariantUnidirectionalPrinciple}
  Let $\linop$ be the hierarchization operator on a full grid,
  i.e.,
  $\linop = \intpmat^{-1}$,
  $\vlinin = (\fcnval{\*k})_{\*k \in \liset}$,
  $\vlinout = (\surplus{\*k})_{\*k \in \liset}$,
  $\linop_{t_j}$ is the 1D interpolation operator $(\intpmat_{t_j})^{-1}$, and
  $\liset = \{\*0, \dotsc, \*2^\*l\}$
  corresponds to a full grid $\fgset{\*l}$ of level $\*l$.
  After iteration $j$ of \cref{alg:unidirectionalPrinciple}
  ($j = 1, \dotsc, d$), it holds for $T := (t_1, \dotsc, t_j)$
  \begin{equation}
    \sum_{\*k_T=\*0}^{\*2^{\*l_T}}
    \surplus[(j)]{(\*k_T,\*k'_{-T})} \basis{\*k_T}(\gp{\*k'_T})
    = \fcnval{\*k'},\quad
    \*k' = \*0, \dotsc, \*2^\*l,
  \end{equation}
  where $(\*k_T,\*k'_{-T})$ is defined to be the index $\*k''$
  with $\*k''_T := \*k_T$ and $\*k''_{-T} := \*k'_{-T}$.
\end{proposition}

\begin{proof}
  We prove the assertion by induction over $j = 1, \dotsc, d$.
  We set $T' := (t_1, \dotsc, t_{j-1})$,
  $T := (t_1, \dotsc, t_{j-1}, t_j)$,
  and we exploit the tensor product structure of the basis
  to write the \lhs of the assertion for $j$
  and arbitrary $\*k' = \*0, \dotsc, \*2^\*l$ as
  \begin{align}
    \sum_{\*k_T=\*0}^{\*2^{\*l_T}}
    \surplus[(j)]{(\*k_T,\*k'_{-T})} \basis{\*k_T}(\gp{\*k'_T})
    &= \sum_{\*k_{T'}=\*0}^{\*2^{\*l_{T'}}}
    \basis{\*k_{T'}}(\gp{\*k'_{T'}}) \cdot
    \sum_{k_{t_j}=0}^{2^{l_{t_j}}}
    \surplus[(j)]{(\*k_T,\*k'_{-T})} \basis{k_{t_j}}(\gp{k'_{t_j}}).\\
    \intertext{%
      If we choose the equivalence class
      $\liset_\mathrm{pole} := \clint{(\*k_T,\*k'_{-T})}_{\samepole{t_j}}$
      ($\*k_T$ arbitrary),
      then the sum over $k_{t_j}$ equals
      $\sum_{\*k \in \liset_\mathrm{pole}}
      \surplus[(j)]{\*k} \basis{k_{t_j}}(\gp{k'_{t_j}})
      %= ((\linop_{t_j})^{-1}
      %\Big[(\surplus[(j)]{\*k})_{\*k \in \liset_\mathrm{pole}}\Big])_{k'_{t_j}}
      %= ((\surplus[(j-1)]{\*k})_{\*k \in \liset_\mathrm{pole}})_{k'_{t_j}}
      = \surplus[(j-1)]{(\*k_{T'},\*k'_{-T'})}$ ($\ast$)
      by the 1D interpolation operator $T_{t_j}$
      (\cref{line:algUnidirectionalPrinciple1} of
      \cref{alg:unidirectionalPrinciple}).
      We can conclude that the \lhs equals%
    }
    &= \sum_{\*k_{T'}=\*0}^{\*2^{\*l_{T'}}}
    \surplus[(j-1)]{(\*k_{T'},\*k'_{-T'})}
    \basis{\*k_{T'}}(\gp{\*k'_{T'}}),
  \end{align}
  which, by the induction hypothesis, equals $\fcnval{\*k'}$ as desired
  (if $j > 1$).
  The same reasoning for $(\ast)$ can be used
  to establish the base case for $j = 1$.
\end{proof}

\begin{shortcorollary}[correctness of \cref{alg:unidirectionalPrinciple}]
  \label{cor:algUnidirectionalPrincipleCorrectness}
  \Cref{alg:unidirectionalPrinciple}
  is correct for hierarchization on full grids.
\end{shortcorollary}

\begin{proof}
  We apply \cref{prop:invariantUnidirectionalPrinciple} to obtain
  $\sum_{\*k=\*0}^{\*2^\*l}
  \surplus[(j)]{\*k} \basis{\*k}(\gp{\*k'})
  = \fcnval{\*k'}$
  for all $\*k' = \*0, \dotsc, \*2^\*l$, i.e.,
  the $\surplus[(j)]{\*k}$ are the correct interpolation coefficients
  according to \eqref{eq:hierarchizationProblem}.
\end{proof}

\paragraph{Complexity}

We compare the complexity of the \up for hierarchization compared
to directly solving the system \eqref{eq:hierarchizationSLE} of
linear equations.
If we assume that $d$ is constant and that
$\linop$ and $\linop_{t_j}$ apply Gaussian elimination to
solve the multivariate and univariate systems, respectively,
then directly solving \eqref{eq:hierarchizationSLE} takes
$\landauO{\ngp^2 (\ngp + d)}$ time%
\footnote{%
  $\landauO{\ngp^2 d}$ for calculating $\intpmat$ and
  $\landauO{\ngp^3}$ for solving the system.
}
and
$\landauO{\ngp^2}$ memory.
In contrast, the \up only requires
$\landauO{\ngp \sum_t \ngp_t^2}$ time%
\footnote{%
  There are $\ngp/\ngp_t$ poles in the
  $t$-th iteration of \cref{alg:unidirectionalPrinciple}.
  Each pole requires the solution of an $\ngp_t \times \ngp_t$ linear system,
  which takes $\landauO{\ngp_t^3}$ time.
},
if $\ngp_t$ is the grid size
$\setsize{\{k_t \mid \*k \in \liset\}}$ in dimension $t = 1, \dotsc, d$,
and $\landauO{\max_t N_t^2}$ memory.
The dependency from the univariate grid sizes $\ngp_t$ instead of $\ngp$
makes the \up significantly less computationally expensive.
As already mentioned,
the \up is even more efficient for the piecewise linear case
where the univariate interpolation operators can be applied
in-place.
Hence, it only needs $\landauO{Nd}$ time and
$\landauO{N}$ memory in this case.
