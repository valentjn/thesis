\breakpagebeforenextheadingtrue
\section{Dynamic Portfolio Choice Models}
\label{sec:81models}

\minitoc[-0.5mm]{76mm}{4}

\noindent
In this section, we give a mathematical framework for
dynamic portfolio choice models,
briefly mention related literature briefly, and
explain where B-splines on sparse grids come into play.
\Cref{tbl:glossaryFinance} summarizes the symbols
that are introduced in this chapter.

\begin{table}
  \setnumberoftableheaderrows{0}%
  \newcommand*{\pnst}[1]{\printnotationsymbol{#1}&\printnotationtext{#1}}%
  \newcommand*{\pnsta}[1]{%
    \printnotationsymbol{#1}&\multicolumn{3}{l}{\printnotationtext{#1}}%
  }%
  \begin{tabular}{%
    >{\kern\tabcolsep}=l+l<{\kern5mm}+l+l<{\kern5mm}+l+l<{\kern\tabcolsep}%
  }
    \toprulec
    \pnst{t}&            \pnst{\wealth}&      \pnst{\utilityfcn}\\
    \pnst{\state}&       \pnst{\consume}&     \pnst{\statefcn}\\
    \pnst{\policy}&      \pnst{\bond}&        \pnst{\valuefcn}\\
    \pnst{\stochastic}&  \pnst{\stock}&       \pnst{\optpolicyfcn}\\
    \pnst{\riskav}&      \pnsta{\cetvalueintp}\\
    \pnst{\patience}&    \pnst{\buysell}&     $\hat{({\cdot})}$&Normalized\\
    \pnst{\bondreturn}&  \pnst{\stockreturn}& \pnst{\wealthratio}\\
    \pnst{\tac}&         &&                   &\\
    \bottomrulec
  \end{tabular}%
  \caption[Glossary for dynamic portfolio choice models]{%
    Glossary of the notation for dynamic portfolio choice models.%
  }%
  \label{tbl:glossaryFinance}%
\end{table}



\subsection{Bellman Equation}
\label{sec:811bellmanEquation}

\paragraph{Utility maximization}

Dynamic portfolio choice models aim to maximize the expected
\term{discounted utility} over the lifetime of the individual.
If we neglect stochastic factors, then these models solve
\begin{equation}
  \label{eq:utilityMaximization}
  (\optpolicyfcn_0, \dotsc, \optpolicyfcn_T)
  = \vecargmax_{\policy_0, \dotsc, \policy_T}
  \sum_{t=0}^T \patience^t \utilityfcn(\consume_t(\state_t, \policy_t))
  \quad\text{s.t. specific constraints.}
\end{equation}
Here, $\state_t \in \clint{\*0, \*1} \subset \real^d$ and
$\policy_t \in \real^{m_{\policy}}$
are the state and policy of time $t = 0, \dotsc, T$, respectively.
The constraints ensure that for instance, we do not spend more money
than we actually have.
Starting from a given initial state $\state_0$,
the state $\state_t$ of time $t > 0$ can be computed from
$\state_0$ and $\policy_0, \dotsc, \policy_{t-1}$ with a
\term{state transition function} $(\state_t, \policy_t) \mapsto \state_{t+1}$.
As shown in \cref{fig:dynamicPortfolioChoice},
in each time step, a fraction of the available wealth
is invested in \term{consumption} $\consume_t$,
which can be computed from the state $\state_t$ and the policy $\policy_t$.
We rate the consumption with a \term{utility function}
$\utilityfcn(\consume_t)$.
A common choice for $\utilityfcn$ is the \term{\crra utility}
$\utilityfcn(\consume_t) := c_t^{1-\riskav}/(1-\riskav)$
with the \term{risk aversion} $\riskav \in \real \setminus \{1\}$.
Positive and negative values correspond to risk-averse and risk-affine
individuals, respectively.
The factor $\patience \in \pohint{0, 1}$ is the \term{patience}
or \term{time discount factor.}

\begin{SCfigure}
  \includegraphics{dynamicPortfolioChoice_1}%
  \caption[Dynamic portfolio choice models]{%
    Dynamic portfolio choice models.
    The available wealth $\wealth_t$ is invested
    into bonds ($\bond_t$) and two stocks ($\stock_{t,1}, \stock_{t,2}$).
    The rest is consumed ($\consume_t$), resulting in the utility
    $\utilityfcn(\consume_t)$.
    In the last time step $T$ \emph{(far right),}
    the whole wealth is consumed, if we do not take inheritance
    into account.%
  }%
  \label{fig:dynamicPortfolioChoice}%
\end{SCfigure}

\paragraph{Limitations of naive utility maximization}

When solving the utility maximization problem \eqref{eq:utilityMaximization},
there are two issues.
First, solving \eqref{eq:utilityMaximization} for all times $t$ at once
implies solving a $(T+1) m_{\policy}$-dimensional optimization problem,
which is usually computationally infeasible.
Second, \eqref{eq:utilityMaximization} does not take stochastic variables
$\stochastic_t$ such as stock return rates into account.
These variables influence the state transition, i.e.,
$(\state_t, \policy_t, \stochastic_t) \mapsto \state_{t+1}$.
Consequently, $\state_t$ cannot be computed from $\state_0$ and
$\policy_0, \dotsc, \policy_{t-1}$ alone,
which complicates the solution of \eqref{eq:utilityMaximization}
even in the expected value.

\paragraph{Bellman principle}

To solve the first issue,
Bellman's principle of optimality \cite{Bellman57Dynamic}
can be applied to problems like
\eqref{eq:utilityMaximization} that are said to have
\term{optimal substructure.}
The principle states that the optimal policy for all times $t = 0, \dotsc, T$
is also optimal with respect to $t = 1, \dotsc, T$, i.e.,
\begin{equation}
  \max_{\policy_0, \dotsc, \policy_T}
  \sum_{t=0}^T \patience^t \utilityfcn(\consume_t(\state_t, \policy_t))
  = \max_{\policy_0} \left(
    \utilityfcn(\consume_0(\state_0, \policy_0))
    + \patience \max_{\policy_1, \dotsc, \policy_T}
    \sum_{t=1}^T \patience^{t-1} \utilityfcn(\consume_t(\state_t, \policy_t))
  \right),
\end{equation}
where we omitted the constraints for brevity.
The inner maximum problem over $\policy_1, \dotsc, \policy_T$
has the same structure as the original problem on the \lhs.
With the \term{value function}
$\valuefcn_t\colon \clint{\*0, \*1} \to \real$,
$\valuefcn_t(\state_t) :=
\max_{\policy_0, \dotsc, \policy_T}
\sum_{t'=t}^T \patience^{t'-t}
\utilityfcn(\consume_{t'}(\state_{t'}, \policy_{t'}))$, this can be rewritten as
\begin{equation}
  \label{eq:simpleBellman}
  \valuefcn_0(\state_0)
  = \max_{\policy_0} \left(
    \utilityfcn(\consume_0(\state_0, \policy_0)) +
    \patience \valuefcn_1(\state_1)
  \right)
  \quad\text{s.t. specific constraints,}
\end{equation}
where $\state_1$ is the result of the state transition
from $(\state_0, \policy_0)$.

\paragraph{General Bellman equation}

If we formulate \eqref{eq:simpleBellman} for arbitrary times $t$ and
consider constraints, state transition, and stochastic variables,
we obtain the \term{Bellman equation:}
\begin{subequations}
  \label{eq:generalBellman}
  \begin{gather}
    \valuefcn_t(\state_t)
    = \max_{\policy_t} \left(
      \utilityfcn(\consume_t(\state_t, \policy_t)) +
      \patience \expectation[t]{
        \valuefcn_{t+1}(\statefcn_t(\state_t, \policy_t, \stochastic_t))
      }
    \right),\quad
    t = 0, \dotsc, T,\\
    \policy_t \in \real^{m_{\policy}}\;\;\text{s.t.}\;\;
    \ineqconfun_t(\state_t, \policy_t) \le \*0,
  \end{gather}
\end{subequations}
where $\valuefcn_{T+1} :\equiv 0$ for simplicity,
$\statefcn_t\colon \clint{\*0, \*1} \times \real^{m_{\policy}} \times
\stochdomain \to \clint{\*0, \*1}$,
$(\state_t, \policy_t, \stochastic_t) \mapsto \state_{t+1}$,
is the \term{state transition function,}
$\ineqconfun_t\colon \clint{\*0, \*1} \times \real^{m_{\policy}} \to
\real^{m_{\ineqconfun}}$ is the \term{constraint function,}
and the expected value is
\begin{equation}
  \expectation[t]{
    \valuefcn_{t+1}(\statefcn_t(\state_t, \policy_t, \stochastic_t))
  }
  := \int_{\stochdomain}
  \valuefcn_{t+1}(\statefcn_t(\state_t, \policy_t, \stochastic_t))
  P_{t,\stochastic}(\stochastic_t) \diff{}\stochastic_t
\end{equation}
with the probability density function
$P_{t,\stochastic}\colon \stochdomain \to \nonnegreal$ of $\stochastic_t$.
We denote the location of the maximum of \eqref{eq:generalBellman}
as the optimal policy $\optpolicyfcn_t$,
which may be regarded as a function
$\optpolicyfcn_t\colon \clint{\*0, \*1} \to \real^{m_{\policy}}$,
$\state_t \mapsto \optpolicyfcn_t(\state_t)$.

\paragraph{Discrete states}

The entries $\stateentry_{t,j}$ of the state
$\state_t \in \clint{\*0, \*1}$ are continuous.
Markov-chain \term{discrete states} $\discrstate_t \in \discrstdomain$
such as alive/dead
(i.e., $\discrstdomain$ is the Cartesian product of finite sets)
can be incorporated into \eqref{eq:generalBellman}:
The objective function of $\valuefcn_t(\state_t, \discrstate_t)$ then equals
$\utilityfcn(\consume_t(\state_t, \discrstate_t, \policy_t)) +
\patience \expectation[t]{
  \valuefcn_{t+1}(\statefcn_t(\state_t, \discrstate_t,
  \policy_t, \stochastic_t), \discrstate_{t+1}) \mid \discrstate_t
}$,
where the expectation is given by
$\sum_{\discrstate_{t+1} \in \discrstdomain}
P_{t+1,\discrstate}(\discrstate_{t+1} \mid \discrstate_t)
\int_{\stochdomain}
\valuefcn_{t+1}(\statefcn_t(\state_t, \discrstate_t,
\policy_t, \stochastic_t), \discrstate_{t+1})
P_{t,\stochastic}(\stochastic_t) \diff{}\stochastic_t$
with the \term{discrete state transition probability}
$P_{t+1,\discrstate}(\discrstate_{t+1} \mid \discrstate_t) \in \clint{0, 1}$.
For the sake of clarity, we omit discrete states in the following,
although they could easily be taken into account.

\paragraph{Dynamic programming scheme}

The Bellman equation \eqref{eq:generalBellman} can be solved
with a dynamic programming scheme backwards in time.
Starting from the solution $\valuefcn_T$ and $\optpolicyfcn_T$
of time $T$, which is determined by maximizing the utility
for the terminal time step,
we can determine $\valuefcn_t$ and $\optpolicyfcn_t$
from $\valuefcn_{t+1}$ and $\optpolicyfcn_{t+1}$
for $t = T - 1, \dotsc, T - 2, \dotsc, 0$ with the Bellman equation.
This way, we only have to solve $T+1$ separate $m_{\policy}$-dimensional
optimization problems instead of a single large
$(T+1) m_{\policy}$-dimensional problem.
Often, the terminal solutions $\valuefcn_T$ and $\optpolicyfcn_T$
are explicitly known.
For example, if we neglect inheritance,
then the optimal terminal solution is to consume the whole wealth $\wealth_T$
(which is usually part of the state $\state_T$)
in the last time step $T$.
%Otherwise, the remaining wealth would be wasted,
%resulting in a non-optimal decision.
Hence, we usually have
\begin{equation}
  \valuefcn_T(\state_T) = \wealth_T,\qquad
  \optpolicyfcn_T(\state_T) = \utilityfcn(\wealth_T).
\end{equation}

\paragraph{Implementation and interpolation}

For the implementation of \eqref{eq:generalBellman},
we discretize the state space $\clint{\*0, \*1}$ into
$\ngp_t$ grid points $\state_t^{(k)}$, $k = 1, \dotsc, \ngp_t$,
and we tabulate the values of $\valuefcn_t$ and $\optpolicyfcn_t$
at $\state_t^{(k)}$ for all $t = 0, \dotsc, T$ and $k = 1, \dotsc, \ngp_t$.
However, in general, the next state
$\statefcn_t(\state_t^{(k)}, \policy_t, \stochastic_t)$ does not correspond
to a grid point $\state_{t+1}^{(k')}$,
which cannot lookup the value of $\valuefcn_{t+1}$
at $\statefcn_t(\state_t^{(k)}, \policy_t, \stochastic_t)$.
Therefore, we have to interpolate $\valuefcn_t$ and $\optpolicyfcn_t$
at the grid points to obtain interpolants
$\valueintp_t$ and $\optpolicyintp_t$, respectively:
\begin{equation}
  \label{eq:gridBellman}
  \valueintp_t(\state_t^{(k)})
  = \max_{\policy_t} \left(
    \utilityfcn(\consume_t(\state_t^{(k)}, \policy_t)) +
    \patience \expectation[t]{
      \valueintp_{t+1}(\statefcn_t(\state_t^{(k)}, \policy_t, \stochastic_t))
    }
  \right),\quad
  k = 1, \dotsc, \ngp_t,
\end{equation}
with $\optpolicyintp_t(\state_t^{(k)}) = \vecargmax_{\policy_t} \cdots$.
As $\valueintp_{t+1}$ on the \rhs is only an approximation to
$\valuefcn_{t+1}$, the new values $\valueintp_t(\state_t^{(k)})$
on the \lhs are only an approximation as well.



\subsection{Solution with B-Spline Surrogates on Sparse Grids}
\label{sec:812surrogates}

\paragraph{Sparse grids and related work}

Conventional schemes for interpolation of $\valuefcn_t$
suffer from the curse of dimensionality:
If the number $d$ of state entries in $\state_t$ is already moderately
large (e.g., $d \ge 4$),
then classical full grid approaches require millions of
grid points even for medium-sized univariate grids
(e.g., $\ngp_t \ge 32$).
This quickly exceeds the available computational budget,
as we have to solve an expensive optimization problem for each grid point.
It seems obvious to instead use interpolation approaches based on
spatially adaptive sparse grids.

Recently, sparse grids have found increasing interest in the
solution of dynamic models in finance
\multicite{Brumm17Using,Judd14Smolyak,Schober18Solving,Winschel10Solving}.
For example in \cite{Brumm17Using},
discrete choices in the value iteration are computed
using piecewise linear basis functions on spatially adaptive sparse grids.
Judd et al.\ \cite{Judd14Smolyak} use global polynomials
on sparse Clenshaw--Curtis grids for the interpolation of
higher-dimensional economic models.
Schober \cite{Schober18Solving} employs sparse grids for the interpolation
of dynamic portfolio choice models, but uses the piecewise linear basis
functions.

\paragraph{B-splines on sparse grids for dynamic portfolio choice models}

The shortcomings of the two approaches of piecewise linear functions
\multicite{Brumm17Using,Schober18Solving} or global polynomials
\cite{Judd14Smolyak} are evident:
First,
the piecewise linear basis is not continuously differentiable,
impeding convergence of interpolation errors
(see \cref{sec:541interpolation}) and prohibiting the use
of gradient-based optimization methods to solve \eqref{eq:gridBellman}.
The reason for the latter is that gradient-based optimizers require
the derivatives of the objective function of \eqref{eq:gridBellman}
with respect to $\policyentry_{t,j}$, $j = 1, \dotsc, m_{\policy}$, i.e.,
\begin{equation}
  \begin{split}
    &\utilityfcn'(\consume_t(\state_t^{(k)}, \policy_t))
    \partialderiv{\partialdiff{}\policyentry_{t,j}}{c_t}\paren{
      \state_t^{(k)}, \policy_t
    }\\
    &{} + \patience \expectation[t]{
      \tr{
        \paren*{
          \gradient{\state_{t+1}}{\valueintp_{t+1}}\paren{
            \statefcn_t(\state_t^{(k)}, \policy_t, \stochastic_t)
          }
        }
      }
      \partialderiv{\partialdiff{}\policyentry_{t,j}}{\statefcn_t}\paren{
        \state_t^{(k)}, \policy_t, \stochastic_t
      }
    },
  \end{split}
\end{equation}
which involves the gradient
$\gradient{\state_{t+1}}{\valueintp_{t+1}}$ of the
value function interpolant $\valueintp_{t+1}$.
Second,
global polynomials only work well on Clenshaw--Curtis grids
with Chebyshev-distributed nodes due to Runge's phenomenon.

In this thesis, we use higher-order B-splines as basis functions for
the interpolation of $\valuefcn_t$ and $\optpolicyfcn_t$.
The advantages of the new method are obvious:
First, B-splines of degree $p > 1$ are continuously differentiable,
increasing the order of convergence and enabling
gradient-based optimization for \eqref{eq:gridBellman}.
Second, B-splines are defined for arbitrary knot sequences,
thus leading to a greater flexibility when compared to global polynomials.
