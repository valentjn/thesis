\section{Numerical Results}
\label{sec:54results}

The following numerical experiments can be roughly divided into two parts.
First, we study interpolation errors for the various test functions
to assess the effects of the hierarchical B-spline bases introduced in
\cref{chap:20sparseGrids,chap:30BSplines} on the interpolation.
Second, we consider optimization errors, i.e.,
the optimality gaps $\objfun(\xoptappr) - \objfun(\xopt)$
of the calculated approximations $\xoptappr$
of the parameter $\xopt$ for which the objective function $\objfun$
is minimal.



\subsection{Interpolation Error and Decay of Surpluses}
\label{sec:541interpolation}

\paragraph{Interpolation error for different test functions}

\Cref{fig:resultsInterpolationErrorTestFunctions} shows the
relative $\Ltwo$ interpolation error
$\tfrac{\normLtwo{\objfun - \sgintp}}{\normLtwo{\objfun}}$
of sparse grid interpolants $\sgintp$ to
different objective functions $\objfun$
(approximated via Monte-Carlo quadrature using
$10^4$ uniformly pseudo-random samples).
The interpolation is performed on regular sparse grids of increasing level
using hierarchical not-a-knot B-splines $\bspl[\nak]{\*l,\*i}{p}$
of degree $p = 1, 3, 5$.
As a visual aid, the plots include gray lines that indicate different
orders of convergence.

\begin{figure}
  \includegraphics{resultsInterpolationLegend_1}\\[2mm]%
  \includegraphics{resultsInterpolation_2}%
  \hfill%
  \includegraphics{resultsInterpolation_4}%
  \\[2mm]%
  \includegraphics{resultsInterpolation_6}%
  \hfill%
  \includegraphics{resultsInterpolation_8}%
  \caption[Relative interpolation error for different test functions]{%
    Relative $\Ltwo$ interpolation error
    $\normLtwo{\objfun - \sgintp}/\normLtwo{\objfun}$
    for different test functions $\objfun$ \emph{(colors)}
    using hierarchical not-a-knot B-splines
    $\bspl[\nak]{\*l,\*i}{p}$ of different degree $p$ \emph{(line styles)} on
    regular sparse grids $\regsgset{n}{d}$ of different levels $n$.%
  }%
  \label{fig:resultsInterpolationErrorTestFunctions}%
\end{figure}

It is already known that
if the objective function is sufficiently smooth
(i.e., at least as smooth as the employed spline basis),
the $\Ltwo$ error of spline interpolants of order $p$ on
$d$-dimensional regular sparse grids of level $n$
asymptotically behaves like
$\landauO{\ms{n}^{p+1} (\log_2 \ms{n}^{-1})^{d-1}}
= \landauO{2^{-(p+1)n} n^{d-1}}$ for $n \to \infty$ \cite{Sickel11Spline}.
We can numerically verify this fact easily with
\cref{fig:resultsInterpolationErrorTestFunctions},
in which we obtain the asserted orders of convergence
for the continuously differentiable bivariate functions that
are continuously differentiable.
For the functions Sch06 and Sch22, which have a non-differentiable kink,
only linear convergence can be achieved regardless of the B-spline degree.

The region, where the asymptotic behavior dominates, largely depends
on the objective function at hand.
Functions like Bra02 and Ack with many small oscillations
require more interpolation points than ``smoother'' functions like
GoP and Alp02.
This is also the case for all functions in higher dimensionalities,
as more interpolation points are necessary to sufficiently explore the domain
(curse of dimensionality).
In \cref{fig:resultsInterpolationErrorTestFunctions}, this can already be seen
for $d \ge 3$.
This is not a consequence of employing higher-order B-splines for
the hierarchical basis.
However, it seems that higher-order B-splines lead to a slight increase
in the interpolation error in the pre-asymptotic range.

\paragraph{Interpolation error for different basis functions}

In \cref{fig:resultsInterpolationErrorBasisFunctions},
we fix the objective function and study the effect of the choice
of hierarchical basis functions on the interpolation error.
Shown are eight types of hierarchical B-spline bases as introduced in
\cref{chap:20sparseGrids,chap:30BSplines} for the three degrees
$p = 1, 3, 5$.
Note that some of the lines exactly overlap, which is indicated in
the figure.

For $p = 1$, the non-modified and modified bases coincide,
respectively.
For higher degrees, the modified bases show worse results than
the corresponding non-modified versions for the same level $n$.
However, modified bases need significantly less grid points
(no boundary points),
which means that a direct comparison based on the sparse grid level $n$
is somewhat flawed.
%
In addition, we see that the not-a-knot bases coincide exactly for $p > 1$,
as they span the same space for dimensionally adaptive sparse grids.
Only with the not-a-knot boundary conditions, we obtain the true
theoretical order of convergence (which is $p + 1$ for degree $p$).
Otherwise, only quadratic convergence can be achieved regardless of $p$,
albeit with a smaller constant (offset).

\begin{SCfigure}
  \begin{minipage}{102mm}%
    \hspace*{10mm}%
    \includegraphics{resultsInterpolationLegend_2}\\[2mm]%
    \includegraphics{resultsInterpolation_10}%
  \end{minipage}%
  \caption[Relative interpolation error for different basis functions]{%
    Relative $\Ltwo$ interpolation error
    $\normLtwo{\objfun - \sgintp}/\normLtwo{\objfun}$
    for the bivariate Alp02 function ($d = 2$)
    using different hierarchical basis functions
    $\basis{\*l,\*i}$ \emph{(colors)}
    of different degree $p$ \emph{(marker styles)} and
    regular sparse grids $\regsgset{n}{d}$ of different levels $n$.\\
    The basis functions shown here involve
    standard \emph{(no superscript)},
    not-a-knot \emph{(nak)},
    modified \emph{(mod)},
    fundamental \emph{(fs)}, and
    weakly fundamental \emph{(wfs)}
    splines as well as the combinations
    introduced in \cref{chap:20sparseGrids,chap:30BSplines}.%
  }%
  \label{fig:resultsInterpolationErrorBasisFunctions}%
\end{SCfigure}

\paragraph{Pointwise interpolation error}

The importance of not-a-knot boundary conditions is also evident
when considering plots of the pointwise interpolation error as in
\cref{fig:resultsInterpolationErrorPointwise}.
Obviously, the interpolation error grows for the standard hierarchical
B-spline basis $\bspl{\*l,\*i}{p}$
as we move towards the boundary of the domain $\clint{\*0, \*1}$,
before dropping to zero or near-zero values at or near boundary grid points.
With not-a-knot B-splines $\bspl[\nak]{\*l,\*i}{p}$,
the interpolation error is consistently low in the whole domain.
For comparison, modified B-splines $\bspl[\modified]{\*l,\*i}{p}$
incur even worse issues near the boundary, since the corresponding sparse grids
do not contain boundary points.

\begin{figure}
  \includegraphics{resultsInterpolationPointwise_4}\\[2mm]%
  \subcaptionbox{%
    $\bspl{\*l,\*i}{p}\vphantom{\bspl[\nak]{\*l,\*i}{p}}$%
  }[48mm]{%
    \includegraphics{resultsInterpolationPointwise_1}%
  }%
  \hfill%
  \subcaptionbox{%
    $\bspl[\nak]{\*l,\*i}{p}$%
  }[48mm]{%
    \includegraphics{resultsInterpolationPointwise_2}%
  }%
  \hfill%
  \subcaptionbox{%
    $\bspl[\modified]{\*l,\*i}{p}$%
  }[48mm]{%
    \includegraphics{resultsInterpolationPointwise_3}%
  }%
  \caption[Pointwise interpolation error]{%
    Pointwise interpolation error
    $\abs{\objfun(\*x) - \sgintp(\*x)}$ on a logarithmic scale
    for the bivariate GoP function ($d = 2$)
    using different hierarchical basis functions
    $\basis{\*l,\*i}$ \emph{(left, center, right)} of degree $p = 3$ on
    the regular sparse grid $\regsgset{n}{d}$ of level $n = 7$.%
  }%
  \label{fig:resultsInterpolationErrorPointwise}%
\end{figure}

\paragraph{Decay of surpluses}

In the piecewise linear case ($p = 1$),
the hierarchical surpluses $\surplus{\*l,\*i}$
can be represented as the $\Ltwo$ inner product of
the corresponding hat function $\bspl{\*l,\*i}{1}$ with the
mixed second derivative
$\partialderiv[2d]{\partialdiff x_1^2 \dotsm \partialdiff x_d^2}{\objfun}$
of the objective function $\objfun$,
if $\*l \ge \*1$ and this derivative exists and is continuous
(see \cref{eq:surplusIntegral}).
Consequently, one can prove that
$\abs{\surplus{\*l,\*i}} \le 2^{-d} 2^{-2\normone{\*l}}
\norm[\infty]{
  \partialderiv[2d]{\partialdiff x_1^2 \dotsm \partialdiff x_d^2}{\objfun}
}$ \cite{Bungartz04Sparse},
i.e., the absolute value of the hierarchical surpluses
decays in quadratic order with the level sum $\normone{\*l}$.
This relation can be used to estimate the convergent range
of the corresponding interpolation error (\cref{%
  fig:resultsInterpolationErrorTestFunctions,%
  fig:resultsInterpolationErrorBasisFunctions%
}).
A generalization of this estimate to higher B-spline degrees $p > 1$
is not straightforward, as the surpluses $\surplus{\*l,\*i}$
then also depend on function values $\objfun(\gp{\*l',\*i'})$ at
grid points of finer levels $\*l' \ge \*l$.

The decay of surpluses can be seen in \cref{fig:resultsDecaySurpluses},
which shows the mean absolute value of surpluses corresponding to
grid points grouped by their level sum $\normone{\*l}$.
Due to the dependency of coarse-level surpluses on fine-level grid points
for $p > 1$,
we have to fix the level $n$ of the regular sparse grid for this analysis.
\Cref{fig:resultsDecaySurpluses} suggests that the
absolute value of the surpluses decays with order $p + 1$ for
B-spline degree $p$, although there is no theoretical evidence supporting
this claim.
Higher B-spline degrees seem to imply that
$\abs{\surplus{\*l,\*i}}$ generally grows for $\normone{\*l}$
in the pre-asymptotic range.

\begin{figure}
  \includegraphics{resultsInterpolationLegend_1}\\[2mm]%
  \includegraphics{resultsInterpolation_1}%
  \hfill%
  \includegraphics{resultsInterpolation_3}%
  %\\[2mm]%
  %\includegraphics{resultsInterpolation_5}%
  %\hfill%
  %\includegraphics{resultsInterpolation_7}%
  \caption[Decay of surpluses for different test functions]{%
    Mean absolute value of surpluses by level sum $\normone{\*l}$
    for different test functions $\objfun$ \emph{(colors)}
    using hierarchical not-a-knot B-splines
    $\bspl[\nak]{\*l,\*i}{p}$ of different degree $p$ \emph{(line styles)} on
    the regular sparse grid $\regsgset{n}{d}$ of level $n = 11$.%
  }%
  \label{fig:resultsDecaySurpluses}%
\end{figure}



\subsection{Optimization Error}
\label{sec:542optimization}

\paragraph{Optimality gaps and displacements}

With the method described in \cref{sec:52method},
we want to find an approximation $\xoptappr$ of the
global minimum $\xopt$ of some objective function $\objfun$
using optimization of a B-spline surrogate $\sgintp$ of $\objfun$
on sparse grids.
Obviously, the more accurate the sparse grid surrogate is,
the better the approximation $\xoptappr$ will be.
In the following plots,
we show the optimality gaps $\objfun(\xoptappr) - \objfun(\xopt)$
in terms of function value.%
\footnote{%
  In order to calculate the optimality gap,
  it is crucial to determine $\objfun(\xopt)$ as exact as possible.
  Otherwise, the optimality gap might either not converge to zero
  or it might even become negative.%
}
The results are sensitive to even small displacements
of the objective function, i.e.,
the results may change for the
function $\*x \mapsto \objfun(\*x - \*a)$
instead of $\*x \mapsto \objfun(\*x)$ for $\*x \in \clint{\*0, \*1}$
and some small $\*a \in \real^d$.%
\footnote{%
  By using the formulas in \cref{chap:a20testProblems},
  all test functions $\objfun$ in \cref{sec:53testProblems}
  can be extended such that they can be evaluated at $\*x - \*a$
  for all $\*x \in \clint{\*0, \*1}$, if $\*a \in \real^d$ is small enough.
  Note that we set $a_t$ to zero if a non-zero displacement in
  the $t$-th component would change the location of the global minimum.%
}
Therefore, the optimization processes for each of the
data points for the following \cref{%
  fig:resultsOptimizationUnconstrainedTestFunctions,%
  fig:resultsOptimizationConstrainedTestFunctions%
}
were repeated five times with replacements $\*a$
whose entries $a_t$ were independent and identically distributed Gaussian
pseudo-random with zero mean and a standard deviation of $0.01$.
The optimality gaps shown in the figures of this section were computed
as the mean of the five runs to increase confidence in the results.

\paragraph{Unconstrained optimization}

\Cref{fig:resultsOptimizationUnconstrainedTestFunctions}
shows the optimality gaps for different test functions $\objfun$
over the number $\ngpMax$ of allowed evaluations of $\objfun$
For the continuously differentiable functions Bra02, GoP, Ack, and Alp02
in $d = 2$ dimensions (top row),
the optimization of the corresponding cubic B-spline surrogates (solid lines)
performs significantly better than using piecewise linear basis functions
(dashed lines).
The reason is two-fold:
First, by using higher-order basis functions, the surrogates are more accurate
in general as seen in the discussion of the interpolation error in
\cref{sec:541interpolation}.
Second, the availability of surrogate gradients accelerates the
convergence of the employed optimization methods.
For some test functions, B-splines even give better results than
the direct optimization of the objective function (dotted lines).

\begin{figure}
  \includegraphics{resultsOptimizationLegend_1}\\[2mm]%
  \includegraphics{resultsOptimizationUnconstrained_1}%
  \hfill%
  \includegraphics{resultsOptimizationUnconstrained_2}%
  \\[2mm]%
  \includegraphics{resultsOptimizationUnconstrained_3}%
  \hfill%
  \includegraphics{resultsOptimizationUnconstrained_4}%
  \\[2mm]%
  \includegraphics{resultsOptimizationUnconstrained_5}%
  \hfill%
  \includegraphics{resultsOptimizationUnconstrained_6}%
  \caption[Optimality gaps for different objective functions (unconstrained)]{%
    Optimality gaps $\objfun(\xoptappr) - \objfun(\xopt)$ between
    the function value at the approximated optimum $\xoptappr$ and
    the minimal function value at the actual optimum $\xopt$
    over the number $\ngpMax$ of objective function evaluations
    for different unconstrained
    objective functions $\objfun$ \emph{(colors).}
    Shown are the optimization results of the B-spline surrogate
    \emph{(solid lines),}
    the optimization results of the piecewise linear surrogate
    \emph{(dashed),} and
    the optimization results of the actual objective function
    \emph{(dotted)} as described in \cref{sec:52method}.%
  }%
  \label{fig:resultsOptimizationUnconstrainedTestFunctions}%
\end{figure}

For the test functions Sch06 and Sch22 with discontinuous derivatives,
the advantage of higher-order B-splines is not as evident (Sch22) or
does not even exist (Sch06).
However, in low dimensions $d \le 4$, B-splines are still able
to achieve a slight advantage compared to the piecewise linear basis.
In higher dimensionalities $d \ge 6$ (bottom row),
convergence speeds visibly decrease for all methods shown in
\cref{fig:resultsOptimizationUnconstrainedTestFunctions},
although for some objective functions, B-splines are still able
to perform better than the comparison methods
(most notably for the Ack function).

\paragraph{Constrained optimization}

\Cref{fig:resultsOptimizationConstrainedTestFunctions}
shows the result for the two constrained optimization problems.
The objective function value $\objfun(\xoptappr)$
at the approximated optimum $\xoptappr$ should not only
be as small as possible, but $\xoptappr$ should also be feasible, i.e.,
$\ineqconfun(\xoptappr) \le \*0$.
Hence, we also plot the maximal violation
$\norm[\infty]{\nonnegpart{\ineqconfun(\xoptappr)}}$
of the constraints in the respective optimal points.

\begin{figure}
  \includegraphics{resultsOptimizationLegend_2}\\[2mm]%
  \subcaptionbox{%
    G08%
  }[72mm]{%
    \includegraphics{resultsOptimizationConstrained_1}%
  }%
  \hfill%
  \subcaptionbox{%
    G04Sq%
  }[72mm]{%
    \includegraphics{resultsOptimizationConstrained_2}%
  }%
  \caption[Optimality gaps for different objective functions (constrained)]{%
    Optimality gaps $\objfun(\xoptappr) - \objfun(\xopt)$ between
    the function value at the approximated optimum $\xoptappr$ and
    the minimal function value at the actual optimum $\xopt$
    over the number $\ngpMax$ of objective function evaluations
    for different constrained optimization problems
    \emph{(left vertical axes).}
    Shown are the optimization results of the B-spline surrogate
    \emph{(solid lines),}
    the optimization results of the piecewise linear surrogate
    \emph{(dashed),} and
    the optimization results of the actual objective function
    \emph{(dotted)} as described in \cref{sec:52method}.
    Additionally, the constraint violation
    $\norm[\infty]{\nonnegpart{\ineqconfun(\xoptappr)}}$
    at the approximated optimum $\xoptappr$ is plotted
    \emph{(right vertical axes).}%
  }%
  \label{fig:resultsOptimizationConstrainedTestFunctions}%
\end{figure}

For the bivariate G08 problem, the hierarchical B-splines surrogates
perform better than the direct gradient-free optimization of the problem
for $\ngpMax \le 3500$ objective function evaluations
and better than the piecewise linear surrogate for $\ngpMax \ge 300$
objective function evaluations.
All calculated points are feasible
(note that for plotting reasons,
\cref{fig:resultsOptimizationConstrainedTestFunctions} shows
$\max(\norm[\infty]{\nonnegpart{\ineqconfun(\xoptappr)}}, 10^{-16})$
instead of the true constraint violation).

The range of the objective function of the five-dimensional G04Sq problem
is larger than that of G08.
This results in generally higher optimality gaps
$\objfun(\xoptappr) - \objfun(\xopt)$ as we do not normalize them
with respect to the range.
The B-spline surrogate results in good approximations $\xoptappr$
of $\xopt$ already for $\ngpMax = 1000$ with an optimality gap of
around one.
Both comparison methods result in optimality gaps that are
seven orders of magnitude higher.
Additionally, the corresponding values of constraint violation
are between $10^{-10}$ and $10^{-6}$, i.e.,
the constraints are numerically met.
In contrast, the optimizers struggle more for
the comparison methods (optimization of the linear surrogate and
of objective function) to meet the constraints,
as the values of constraint violation partly exceed $10^{-3}$.
The availability of gradients seems to allow the constrained optimization
algorithm to better enforce the feasibility of the
resulting points $\xoptappr$.



\subsection{Complexity of Hierarchization}
\label{sec:543complexity}

\blindtext{}

\begin{figure}
  \includegraphics{complexityFundamental_3}\\[2mm]%
  \subcaptionbox{%
    Runtime%
  }[72mm]{%
    \includegraphics{complexityFundamental_1}%
  }%
  \hfill%
  \subcaptionbox{%
    Memory consumption%
  }[72mm]{%
    \includegraphics{complexityFundamental_2}%
  }%
  \caption[TODO]{%
    TODO%
  }%
  \label{fig:TODO1}%
\end{figure}

\begin{figure}
  \includegraphics{complexityWeaklyFundamental_4}\\[2mm]%
  \subcaptionbox{%
    $\gamma = 0.05$%
  }[48mm]{%
    \includegraphics{complexityWeaklyFundamental_1}%
  }%
  \hfill%
  \subcaptionbox{%
    $\gamma = 0.15$%
  }[48mm]{%
    \includegraphics{complexityWeaklyFundamental_2}%
  }%
  \hfill%
  \subcaptionbox{%
    $\gamma = 0.25$%
  }[48mm]{%
    \includegraphics{complexityWeaklyFundamental_3}%
  }%
  \caption[TODO]{%
    TODO%
  }%
  \label{fig:TODO2}%
\end{figure}
