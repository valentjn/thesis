\section{Numerical Results}
\label{sec:54results}

The following numerical experiments can be roughly divided into two parts.
First, we study interpolation errors for the various test functions
to assess the effects of the hierarchical B-spline bases introduced in
\cref{chap:20sparseGrids,chap:30BSplines} on the interpolation.
Second, we consider optimization errors, i.e.,
the optimality gaps $\objfun(\xoptappr) - \objfun(\xopt)$
of the calculated approximations $\xoptappr$
of the parameter $\xopt$ for which the objective function $\objfun$
is minimal.



\subsection{Interpolation Error and Decay of Surpluses}
\label{sec:541interpolation}

\paragraph{Interpolation error for different test functions}

\Cref{fig:resultsInterpolationErrorTestFunctions} shows the
relative $\Ltwo$ interpolation error
$\tfrac{\normLtwo{\objfun - \sgintp}}{\normLtwo{\objfun}}$
of sparse grid interpolants $\sgintp$ to
different objective functions $\objfun$
(approximated via Monte-Carlo quadrature using
$10^4$ uniformly pseudo-random samples).
The interpolation is performed on regular sparse grids of increasing level
using hierarchical not-a-knot B-splines $\bspl[\nak]{\*l,\*i}{p}$
of degree $p = 1, 3, 5$.
As a visual aid, the plots include gray lines that indicate different
orders of convergence.

\begin{figure}
  \includegraphics{resultsInterpolationLegend_1}\\[2mm]%
  \includegraphics{resultsInterpolation_2}%
  \hfill%
  \includegraphics{resultsInterpolation_4}%
  \\[2mm]%
  \includegraphics{resultsInterpolation_6}%
  \hfill%
  \includegraphics{resultsInterpolation_8}%
  \caption[Relative interpolation error for different test functions]{%
    Relative $\Ltwo$ interpolation error
    $\normLtwo{\objfun - \sgintp}/\normLtwo{\objfun}$
    for different test functions $\objfun$ \emph{(colors)}
    using hierarchical not-a-knot B-splines
    $\bspl[\nak]{\*l,\*i}{p}$ of different degree $p$ \emph{(line styles)} on
    regular sparse grids $\regsgset{n}{d}$ of different levels $n$.%
  }%
  \label{fig:resultsInterpolationErrorTestFunctions}%
\end{figure}

It is already known that
if the objective function is sufficiently smooth
(i.e., at least as smooth as the employed spline basis),
the $\Ltwo$ error of spline interpolants of order $p$ on
$d$-dimensional regular sparse grids of level $n$
asymptotically behaves like
$\landauO{\ms{n}^{p+1} (\log_2 \ms{n}^{-1})^{d-1}}
= \landauO{2^{-(p+1)n} n^{d-1}}$ for $n \to \infty$ \cite{Sickel11Spline}.
We can numerically verify this fact easily with
\cref{fig:resultsInterpolationErrorTestFunctions},
in which we obtain the asserted orders of convergence
for the continuously differentiable bivariate functions that
are continuously differentiable.
For the functions Sch06 and Sch22, which have a non-differentiable kink,
only linear convergence can be achieved regardless of the B-spline degree.

The region, where the asymptotic behavior dominates, largely depends
on the objective function at hand.
Functions like Bra02 and Ack with many small oscillations
require more interpolation points than ``smoother'' functions like
GoP and Alp02.
This is also the case for all functions in higher dimensionalities,
as more interpolation points are necessary to sufficiently explore the domain
(curse of dimensionality).
In \cref{fig:resultsInterpolationErrorTestFunctions}, this can already be seen
for $d \ge 3$.
This is not a consequence of employing higher-order B-splines for
the hierarchical basis.
However, it seems that higher-order B-splines lead to a slight increase
in the interpolation error in the pre-asymptotic range.

\paragraph{Interpolation error for different basis functions}

In \cref{fig:resultsInterpolationErrorBasisFunctions},
we fix the objective function and study the effect of the choice
of hierarchical basis functions on the interpolation error.
Shown are eight types of hierarchical B-spline bases as introduced in
\cref{chap:20sparseGrids,chap:30BSplines} for the three degrees
$p = 1, 3, 5$.
Note that some of the lines exactly overlap, which is indicated in
the figure.

For $p = 1$, the non-modified and modified bases coincide,
respectively.
For higher degrees, the modified bases show worse results than
the corresponding non-modified versions for the same level $n$.
However, modified bases need significantly less grid points
(no boundary points),
which means that a direct comparison based on the sparse grid level $n$
is somewhat flawed.
%
In addition, we see that the not-a-knot bases coincide exactly for $p > 1$,
as they span the same space for dimensionally adaptive sparse grids.
Only with the not-a-knot boundary conditions, we obtain the true
theoretical order of convergence (which is $p + 1$ for degree $p$).
Otherwise, only quadratic convergence can be achieved regardless of $p$,
albeit with a smaller constant (offset).

\begin{SCfigure}
  \begin{minipage}{102mm}%
    \hspace*{10mm}%
    \includegraphics{resultsInterpolationLegend_2}\\[2mm]%
    \includegraphics{resultsInterpolation_10}%
  \end{minipage}%
  \caption[Relative interpolation error for different basis functions]{%
    Relative $\Ltwo$ interpolation error
    $\normLtwo{\objfun - \sgintp}/\normLtwo{\objfun}$
    for the bivariate Alp02 function ($d = 2$)
    using different hierarchical basis functions
    $\basis{\*l,\*i}$ \emph{(colors)}
    of different degree $p$ \emph{(marker styles)} and
    regular sparse grids $\regsgset{n}{d}$ of different levels $n$.\\
    The basis functions shown here involve
    standard \emph{(no superscript)},
    not-a-knot \emph{(nak)},
    modified \emph{(mod)},
    fundamental \emph{(fs)}, and
    weakly fundamental \emph{(wfs)}
    splines as well as the combinations
    introduced in \cref{chap:20sparseGrids,chap:30BSplines}.%
  }%
  \label{fig:resultsInterpolationErrorBasisFunctions}%
\end{SCfigure}

\paragraph{Pointwise interpolation error}

The importance of not-a-knot boundary conditions is also evident
when considering plots of the pointwise interpolation error as in
\cref{fig:resultsInterpolationErrorPointwise}.
Obviously, the interpolation error grows for the standard hierarchical
B-spline basis $\bspl{\*l,\*i}{p}$
as we move towards the boundary of the domain $\clint{\*0, \*1}$,
before dropping to zero or near-zero values at or near boundary grid points.
With not-a-knot B-splines $\bspl[\nak]{\*l,\*i}{p}$,
the interpolation error is consistently low in the whole domain.
For comparison, modified B-splines $\bspl[\modified]{\*l,\*i}{p}$
incur even worse issues near the boundary, since the corresponding sparse grids
do not contain boundary points.

\begin{figure}
  \includegraphics{resultsInterpolationPointwise_4}\\[2mm]%
  \subcaptionbox{%
    $\bspl{\*l,\*i}{p}\vphantom{\bspl[\nak]{\*l,\*i}{p}}$%
  }[48mm]{%
    \includegraphics{resultsInterpolationPointwise_1}%
  }%
  \hfill%
  \subcaptionbox{%
    $\bspl[\nak]{\*l,\*i}{p}$%
  }[48mm]{%
    \includegraphics{resultsInterpolationPointwise_2}%
  }%
  \hfill%
  \subcaptionbox{%
    $\bspl[\modified]{\*l,\*i}{p}$%
  }[48mm]{%
    \includegraphics{resultsInterpolationPointwise_3}%
  }%
  \caption[Pointwise interpolation error]{%
    Pointwise interpolation error
    $\abs{\objfun(\*x) - \sgintp(\*x)}$ on a logarithmic scale
    for the bivariate GoP function ($d = 2$)
    using different hierarchical basis functions
    $\basis{\*l,\*i}$ \emph{(left, center, right)} of degree $p = 3$ on
    the regular sparse grid $\regsgset{n}{d}$ of level $n = 7$.%
  }%
  \label{fig:resultsInterpolationErrorPointwise}%
\end{figure}

\paragraph{Decay of surpluses}

In the piecewise linear case ($p = 1$),
the hierarchical surpluses $\surplus{\*l,\*i}$
can be represented as the $\Ltwo$ inner product of
the corresponding hat function $\bspl{\*l,\*i}{1}$ with the
mixed second derivative
$\partialderiv[2d]{\partialdiff x_1^2 \dotsm \partialdiff x_d^2}{\objfun}$
of the objective function $\objfun$,
if $\*l \ge \*1$ and this derivative exists and is continuous
(see \cref{eq:surplusIntegral}).
Consequently, one can prove that
$\abs{\surplus{\*l,\*i}} \le 2^{-d} 2^{-2\normone{\*l}}
\norm[\infty]{
  \partialderiv[2d]{\partialdiff x_1^2 \dotsm \partialdiff x_d^2}{\objfun}
}$ \cite{Bungartz04Sparse},
i.e., the absolute value of the hierarchical surpluses
decays in quadratic order with the level sum $\normone{\*l}$.
This relation can be used to estimate the convergent range
of the corresponding interpolation error (\cref{%
  fig:resultsInterpolationErrorTestFunctions,%
  fig:resultsInterpolationErrorBasisFunctions%
}).
A generalization of this estimate to higher B-spline degrees $p > 1$
is not straightforward, as the surpluses $\surplus{\*l,\*i}$
then also depend on function values $\objfun(\gp{\*l',\*i'})$ at
grid points of finer levels $\*l' \ge \*l$.

The decay of surpluses can be seen in \cref{fig:resultsDecaySurpluses},
which shows the mean absolute value of surpluses corresponding to
grid points grouped by their level sum $\normone{\*l}$.
Due to the dependency of coarse-level surpluses on fine-level grid points
for $p > 1$,
we have to fix the level $n$ of the regular sparse grid for this analysis.
\Cref{fig:resultsDecaySurpluses} suggests that the
absolute value of the surpluses decays with order $p + 1$ for
B-spline degree $p$, although there is no theoretical evidence supporting
this claim.
Higher B-spline degrees seem to imply that
$\abs{\surplus{\*l,\*i}}$ generally grows for $\normone{\*l}$
in the pre-asymptotic range.

\begin{figure}
  \includegraphics{resultsInterpolationLegend_1}\\[2mm]%
  \includegraphics{resultsInterpolation_1}%
  \hfill%
  \includegraphics{resultsInterpolation_3}%
  %\\[2mm]%
  %\includegraphics{resultsInterpolation_5}%
  %\hfill%
  %\includegraphics{resultsInterpolation_7}%
  \caption[Decay of surpluses for different test functions]{%
    Mean absolute value of surpluses by level sum $\normone{\*l}$
    for different test functions $\objfun$ \emph{(colors)}
    using hierarchical not-a-knot B-splines
    $\bspl[\nak]{\*l,\*i}{p}$ of different degree $p$ \emph{(line styles)} on
    the regular sparse grid $\regsgset{n}{d}$ of level $n = 11$.%
  }%
  \label{fig:resultsDecaySurpluses}%
\end{figure}



\subsection{Optimization Error}
\label{sec:542optimization}

\paragraph{Unconstrained optimization}

\blindtext{}

\paragraph{Constrained optimization}

\blindtext{}



\subsection{Complexity of Hierarchization}
\label{sec:543complexity}

\blindtext{}
