\section{Approximating Elasticity Tensors}
\label{sec:62tensors}

\paragraph{Optimization process}

During the solution process of \eqref{eq:topoOptProblemDiscrete},
optimization algorithms typically
evaluate the objective function $\compliance(\*x^{(1)}, \dotsc, \*x^{(M)})$
at iteratively different \term{micro-cell parameter combinations}
$(\*x^{(1)}, \dotsc, \*x^{(M)}) \in (\real^d)^M$.
Every evaluation of $\compliance$ corresponds to the solution of a
macro-problem.
However, the elasticity tensors $\etensor^{(q)}$ of all $M$ macro-cells
need to be known to solve the macro-problem.
Hence, in every optimization iteration, it is necessary to solve
one macro-problem and $M$ micro-cell problems,
both with the \fem.
This naive approach has two major drawbacks, which we explain in
the following.



\subsection{Drawbacks of the Naive Approach}
\label{sec:621drawbacks}

\paragraph{Drawback 1: Computation time}

First, this approach is typically computationally infeasible.
The computation of a single elasticity tensor usually takes seconds to
minutes.
All $M$ micro-cell problems per optimization iterations
can be solved in parallel without any
communication (embarrassingly parallel).
However, $M$ is usually in the range of thousands and
there are thousands or tens of thousands optimization iterations
(the optimization problem is $(d \cdot M)$-dimensional!).
This implies that the overall computation may still take
several days to complete.

\paragraph{Drawback 2: Approximation of gradients}

Second, most optimization algorithms require gradients of the
objective function and of the constraints, i.e.,%
%\footnote{%
%  By convention, the gradient $\gradient{\*x}{f}(\*x)$ of a
%  scalar-valued function $f$ is the column vector of partial derivatives.
%  For vector-valued functions $\*f$, the gradient $\gradient{\*x}{\*f}(\*x)$
%  is defined as the transposed Jacobian of $\*f$, i.e.,
%  the matrix whose columns are the gradients $\gradient{\*x}{f_j}(\*x)$ of
%  single components of $\*f$.
%  If $\*f$ is matrix-valued, then its gradient is defined as the
%  gradient of its vectorization.%
%}
\begin{equation}
  \partialderiv{\partialdiff{} x_t}{\etensor^{(q)}}(\*x^{(q)}),\quad
  \partialderiv{\partialdiff{} x_t}{\dens^{(q)}}(\*x^{(q)}),\qquad
  q = 1, \dotsc, M,\quad
  t = 1, \dotsc, d,
\end{equation}
where $\etensor^{(q)}$ is the elasticity tensor and
$\dens^{(q)}$ is the density of the $q$-th macro-cell.
However, in general, both gradients are unavailable and
have to be approximated by finite differences.
This introduces new error sources and
increases the number of elasticity tensors to be evaluated
by the factor of $2d$ (central differences),
further increasing the time-to-solution.
Additionally, the number of optimization iterations to
achieve convergence might increase,
if there are discontinuities in the objective function
or its gradient
(which could already be caused by the inexact solution of the \fem).
If we need Hessians or other higher-order derivatives,
then all the mentioned issues even worsen.



\subsection{B-Splines on Sparse Grids for Topology Optimization}
\label{sec:622BSplines}

\paragraph{Elasticity tensor function}

As a remedy, we replace the costly evaluation of the
elasticity tensors $\etensor^{(q)}$ with cheap surrogates.
We assume that all macro-cells use the same micro-cell model.
Consequently, the elasticity tensor $\etensor^{(q)}$ of the $q$-th macro cell
with the micro-cell parameter $\*x^{(q)} \in \clint{\*0, \*1}$
can be written as the value $\etensor(\*x^{(q)})$ of some function
$\etensor\colon \clint{\*0, \*1} \to \real^{6 \times 6}$
(assuming that $\dimdomain = 3$) at the point $\*x^{(q)}$.
In the following,
$\etensor\colon \clint{\*0, \*1} \to \real^m$
gives $m \in \nat$ values from which the symmetric elasticity tensor
can be uniquely reconstructed,
e.g., $m = 6$ for $\dimdomain = 2$ and $m = 21$ for $\dimdomain = 3$.
The vector-valued and the matrix-valued version of $\etensor$
will be used interchangeably.

\paragraph{Elasticity tensor surrogate}

The idea is now to use B-splines on sparse grids to approximate
the elasticity tensor function $\etensor$.
In contrast to the theoretical framework that we derived in
\cref{chap:20sparseGrids,chap:30BSplines,chap:40hierarchization},
the function to be interpolated is not scalar-valued, but vector-valued.
This means that we have to construct $m$ sparse grid interpolants
$\etensorentry[\sparse]{j}$
for the $m$ components $\etensorentry{j}$ of $\etensor$ ($j = 1, \dotsc, m$).
Note that one could generate different sparse grids for the
different components $\etensorentry[\sparse]{j}$.
However, it is not possible to evaluate only specific entries of $\etensor$
without also evaluating all other entries,
which means that we would waste computation time by selecting only
a subset of the calculated entries.
Therefore, we use the same grid for all components.

Additionally, we approximate the density $\dens^{(q)}$
of the $q$-th macro-cell with a surrogate using
B-splines on the same sparse grid for reasons of implementation,
resulting in $m + 1$ sparse grid interpolants in total.
From a theoretical perspective, this is not necessary,
since the density can be calculated explicitly with simple formulas
for most micro-cell models, independently of evaluations of the
elasticity tensor.

\paragraph{Advantages}

Our approach has multiple obvious advantages:
\begin{itemize}
  \item
  The sparse grid interpolant $\etensor[\sparse]$ has to be generated only
  once in an \term{offline step} before the optimization algorithm starts.
  During the optimization \term{(online phase)},
  only inexpensive evaluations of $\etensor[\sparse]$ are performed,
  which saves much time.
  
  \item
  Sparse grids help to reduce the curse of dimensionality, which prohibits
  conventional full grid interpolation methods if $d > 4$.
  
  \item
  With spatially adaptive sparse grids and a suitable refinement criterion,
  we can spend more grid points in regions of interest of $\etensor$,
  e.g., regions with large oscillations.
  
  \item
  By using B-splines as basis functions,
  the interpolant $\etensor[\sparse]$ will be smoother and more accurate.
  In addition, we can calculate its derivatives
  $\tpartialderiv{\partialdiff{} x_t}{\etensor[\sparse]}(\*x^{(q)})$
  fast and explicitly,
  accelerating the speed of convergence of the optimization method.
\end{itemize}



\subsection{Cholesky Factor Interpolation}
\label{sec:623cholesky}

\paragraph{Positive definiteness of elasticity tensors}

Unfortunately, just replacing the elasticity tensor with
the B-spline surrogate often does not lead to correct results in practice.
Experiments show that only for most regular sparse grids and
for some spatially adaptive sparse grids \cite{Valentin16Hierarchical},
the optimization algorithm converges to an optimal point.
The optimization algorithm crashes for most spatially adaptive grids,
not able to find any meaningful optimum.
%
The root of the problem proves to be that for specific
$\*x \in \clint{\*0, \*1}$,
the interpolated elasticity tensors $\etensor[\sparse](\*x)$ are not
positive definite.
However, indefinite or even negative definite tensors $\etensor[\sparse]$
would mimic unphysical behavior.%
\footnote{%
  In the scalar case, this is analogous to Hooke's law for linear springs,
  where the force $F = kx$ needed to displace the end of a spring
  (fixed at the other end) by $x$ is proportional to $x$.
  The proportionality constant $k$ (which corresponds to the elasticity tensor)
  has to be positive.%
}
Hence, it is imperative for the optimization process that
the interpolated elasticity tensors are \spd.

\paragraph{Positive definiteness of sparse grid interpolants}

However, interpolation on sparse grids per se does not preserve
positive definiteness.
A counterexample is shown in \cref{fig:cholesky1},
which displays the minimal eigenvalue of the elasticity tensor surrogate
resulting from interpolation on a regular sparse grid.
The positivity of the diagonal is a necessary condition
for positive definiteness.
This means that small oscillations of the interpolant of some entries
already make the whole elasticity tensor non-positive-definite.

\begin{figure}
  \subcaptionbox{%
    Minimal eigenvalue of $\etensor[\sparse](\*x)$%
    \label{fig:cholesky1}%
  }[65mm]{%
    \includegraphics{cholesky_1}%
  }%
  \hfill%
  \subcaptionbox{%
    Minimal eigenvalue of $\etensor[\chol,\sparse](\*x)$%
    \label{fig:cholesky2}%
  }[65mm]{%
    \includegraphics{cholesky_2}%
  }%
  \hfill\hfill%
  \raisebox{2.2mm}{\includegraphics{cholesky_3}}%
  \caption[%
    Minimal eigenvalue of interpolated elasticity tensors%
  ]{%
    Minimal eigenvalue \emph{(colored contour lines)}
    of elasticity tensor surrogates
    for the ``cross'' model ($\dimdomain = 2$, $d = 2$)
    and cubic hierarchical B-splines $\bspl{l,i}{p}$ ($p = 3$) on
    the regular sparse grid $\coarseregsgset{n}{d}{b}$ \emph{(dots)}
    with $n = 7$ and $b = 4$.
    \emph{Left:} The minimal eigenvalue of $\etensor[\sparse](\*x)$
    becomes negative in some regions \emph{\textcolor{C1}{(red areas)}}
    of the domain $\clint{\*0, \*1}$,
    indicating that $\etensor[\sparse](\*x)$ is not positive definite.
    \emph{Right:} The minimal eigenvalue of $\etensor[\chol,\sparse](\*x)$
    is non-negative in the whole domain $\clint{\*0, \*1}$.%
  }%
  \label{fig:cholesky}%
\end{figure}

These oscillations occur preferably near the boundary of the domain
$\clint{\*0, \*1}$, such that there are larger regions
where the interpolated tensor is not positive definite anymore.
The reason is two-fold:
First, sparse grids notoriously have a bias for the center of the domain,
as they place only few points near the boundary \cite{Pflueger10Spatially},
which leads to a loss of interpolation accuracy near the boundary,
compared to the center of $\clint{\*0, \*1}$.
Second, both the minimal eigenvalue of $\etensor(\*x)$ and the norm of its
gradient with respect to $\*x$ is small near $x_1 = 0$ or $x_2 = 0$.
Consequently, the ``surface'' of $\etensor(\*x)$ is rather flat in these
regions and almost vanishes, making it easy for the eigenvalues of a
surrogate function $\etensor[\sparse](\*x)$ to drop below zero.

Note that for most micro-cell models,
the optimization algorithm often queries the objective function
$\compliance(\*x^{(1)}, \dotsc, \*x^{(M)})$ for micro-cell parameter
combinations where many of the points $\*x^{(j)}$ are near the boundary
of $\clint{\*0, \*1}$.
This is because many of the macro-cells will either be empty or
fully filled with material, which usually corresponds to micro-cell
parameters near zero or one, respectively.
Thus, $\etensor[\sparse]$ is often evaluated in the regions of indefiniteness,
which further worsens the issue.

\paragraph{Positivity-preserving methods}

Even in one dimension, it cannot be guaranteed that the interpolant of
positive data remains positive,
which is a key problem in the estimation of probability densities
\multicite{Pflueger10Spatially,Franzelin16From,Griebel10Finite}.
Just clamping the interpolated values via $\max(\cdot, 0)$ often
does not help:
In our application, the tensor may still be indefinite;
additionally, the calculated gradients of the interpolants do not match
the actual gradients anymore.
In density estimation, clamping a density-like function changes its
integral, making it necessary to calculate a normalization constant
\cite{Franzelin17Data}.

One possible workaround is applying an injective and continuous transformation
$T\colon \posreal \to \real$ on the density values (e.g., $\ln$),
then interpolating the resulting values, and finally
applying the inverse transformation $T^{-1}\colon T(\posreal) \to \posreal$
on the interpolated values (e.g., $\exp$).
%In density estimation, this method might introduce new unwanted modes into the
%probability distribution.
For the piecewise linear basis on sparse grids, another approach has been
developed recently \cite{Franzelin17Data},
where the positivity of the function is maintained
by inserting additional points into the sparse grid.
In the context of spline approximation,
positivity-preserving approximation schemes based on so-called
quasi-interpolation are known \cite{Hoellig13Approximation}.
For our application, where we do not only need to preserve positivity,
but positive definiteness,
it is conceivable that one could apply these positivity-preserving
methods in the eigenspace,
interpolating the positive eigenvalues.

\paragraph{Interpolation of Cholesky factors}

Instead, we pursue a different, more canonical
approach based on Cholesky factorization:

\usenotation{Ëtr}

\begin{proposition}[Cholesky factorization]
  For every \spd matrix $\etensor \in \real^{6 \times 6}$,
  there is a unique upper triangular matrix
  $\cholfactor \in \real^{6 \times 6}$
  with positive diagonal entries such that
  \begin{equation}
    \etensor
    = \cholfactor^\tr \cholfactor.
  \end{equation}
\end{proposition}

\begin{proof}
  See \cite{Benoit24Note} or \cite{Freund07Stoer}.
\end{proof}

\usenotation{Ëchol}
In one dimension, the Cholesky factorization is equivalent
to the application of a transformation $T$ as above by choosing
$T := \sqrt{\cdot}$ and $T^{-1} = (\cdot)^2$.
Our approach is as follows:
\begin{enumerate}
  \item
  Define $\cholfactor\colon \clint{\*0, \*1} \to \real^{6 \times 6}$
  as the Cholesky factor of
  $\etensor\colon \clint{\*0, \*1} \to \real^{6 \times 6}$, i.e.,
  $\etensor(\*x) = \cholfactor(\*x)^\tr \cholfactor(\*x)$
  for all $\*x \in \clint{\*0, \*1}$.
  
  \item
  During the grid generation (offline phase),
  evaluate $\etensor(\gp{\*l,\*i})$ at the grid points $\gp{\*l,\*i}$,
  compute the Cholesky factors $\cholfactor(\gp{\*l,\*i})$ of
  $\etensor(\gp{\*l,\*i})$,
  and interpolate them instead of the elasticity tensors
  to obtain an interpolant
  $\cholfactor[\sparse]\colon \clint{\*0, \*1} \to \real^{6 \times 6}$.
  
  \item
  During the optimization (online phase),
  every time the value $\etensor(\*x)$ of an elasticity tensor is needed,
  the interpolant $\cholfactor[\sparse](\*x)$ is evaluated and we return
  \begin{equation}
    \etensor[\chol,\sparse](\*x)
    := \cholfactor[\sparse](\*x)^\tr \cholfactor[\sparse](\*x).
  \end{equation}
\end{enumerate}

\paragraph{Advantages of Cholesky factor interpolation}

As shown in \cref{fig:cholesky2},
the resulting elasticity tensor surrogate $\etensor[\chol,\sparse]$
is positive semidefinite on the whole domain and
positive definite almost everywhere:
The surrogate $\etensor[\chol,\sparse](\*x)$ is singular if and only if
$\cholfactor[\sparse](\*x)$ is singular, which is only the case on a
null set of $\clint{\*0, \*1}$.

Another advantage of this approach is that not only the
positive definiteness, but also the explicit differentiability
of the surrogate $\etensor[\chol,\sparse]$ is preserved.
The gradient can be computed easily and fast with the product rule:
\begin{equation}
  \partialderiv{\partialdiff{} x_t}{\etensor[\chol,\sparse]}(\*x)
  = \cholfactor[\sparse](\*x)^\tr \cdot
  \partialderiv{\partialdiff{} x_t}{\cholfactor[\sparse]}(\*x) +
  \partialderiv{\partialdiff{} x_t}{\cholfactor[\sparse]}(\*x)^\tr \cdot
  \cholfactor[\sparse](\*x),\quad
  t = 1, \dotsc, d,
\end{equation}
% e.g. R = [[r11, r12],
%           [r21, r22]]
% ==> E = R^T * R = [[r11^2+r21^2,     r11*r12+r21*r22],
%                    [r11*r12+r21*r22, r12^2+r22^2]]
% ==> dE = [[2*r11*dr11+2*r21*dr21, r11*dr12+r12*dr11+r21*dr22+r22*dr21],
%           [r11*dr12+r12*dr11+r21*dr22+r22*dr21, 2*r12*dr12+2*r22*dr22]]
% dR = [[dr11, dr12],
%       [dr21, dr22]]
% dR^T = [[dr11, dr21],
%         [dr12, dr22]]
% R = [[r11, r12],
%      [r21, r22]]
% R^T = [[r11, r21],
%        [r12, r22]]
% ==> dR^T * R = [[r11*dr11+r21*dr21, r12*dr11+r22*dr21],
%                 [r11*dr12+r21*dr22, r12*dr12+r22*dr22]]
% ==> R^T * dR = [[r11*dr11+r21*dr21, r11*dr12+r21*dr22],
%                 [r12*dr11+r22*dr21, r12*dr12+r22*dr22]]
% ==> R^T * dR + dR^T * R
% = [[2*r11*dr11+2*r21*dr21, r11*dr12+r12*dr11+r21*dr22+r22*dr21],
%    [r11*dr12+r12*dr11+r21*dr22+r22*dr21, 2*r12*dr12+2*r22*dr22]]
% = dE
where both $\cholfactor[\sparse](\*x)$ and
$\partialderiv{\partialdiff{} x_t}{\cholfactor[\sparse]}(\*x)$
are known (interpolant and its derivative using B-splines on sparse grids).
As discussed above,
this is key for the applicability of gradient-based optimization.
















































