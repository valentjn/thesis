\section{Approximating Elasticity Tensors}
\label{sec:62tensors}

\paragraph{Optimization process}

During the solution process of \eqref{eq:topoOptProblemDiscrete},
optimization algorithms typically
evaluate the objective function $\compliance(\*x^{(1)}, \dotsc, \*x^{(M)})$
at iteratively different \term{micro-cell parameter combinations}
$(\*x^{(1)}, \dotsc, \*x^{(M)}) \in (\real^d)^M$.
Every evaluation of $\compliance$ corresponds to the solution of a
macro-problem.
However, the elasticity tensors $\etensor_j$ of all $M$ macro-cells
need to be known to solve the macro-problem.
Hence, in every optimization iteration, it is necessary to solve
one macro-problem and $M$ micro-cell problems,
both with the \fem.
This naive approach has two major drawbacks, which we explain in
the following.



\subsection{Drawbacks of the Naive Approach}
\label{sec:621drawbacks}

\paragraph{Drawback 1: Computation time}

First, this approach is typically computationally infeasible.
The computation of a single elasticity tensor usually takes seconds to
minutes.
All $M$ micro-cell problems per optimization iterations
can be solved in parallel without any
communication (embarrassingly parallel).
However, $M$ is usually in the range of thousands and
there are thousands or tens of thousands optimization iterations
(the optimization problem is $(d \cdot M)$-dimensional!).
This implies that the overall computation may still take
several days to complete.

\paragraph{Drawback 2: Approximation of gradients}

Second, most optimization algorithms require gradients of the
objective function and of the constraints, i.e.,%
\footnote{%
  By convention, the gradient $\nabla_{\*x} f(\*x)$ of a
  scalar-valued function $f$ is the column vector of partial derivatives.
  For vector-valued functions $\*f$, the gradient $\nabla_{\*x} \*f(\*x)$
  is defined as the transposed Jacobian of $\*f$, i.e.,
  the matrix whose columns are the gradients $\nabla_{\*x} f_j(\*x)$ of
  single components of $\*f$.
  If $\*f$ is matrix-valued, then its gradient is defined as the
  gradient of its vectorization.%
}
\begin{equation}
\nabla_{\*x^{(j)}} \etensor_j(\*x^{(j)}),\quad
\nabla_{\*x^{(j)}} \dens_j(\*x^{(j)}),\qquad
j = 1, \dotsc, M,
\end{equation}
where $\etensor_j$ is the elasticity tensor and
$\dens_j$ is the density of the $j$-th macro-cell.
However, in general, both gradients are unavailable and
have to be approximated by finite differences.
This introduces new error sources and
increases the number of elasticity tensors to be evaluated
by the factor of $2d$ (central differences),
further increasing the time-to-solution.
Additionally, the number of optimization iterations to
achieve convergence might increase,
if there are discontinuities in the objective function
or its gradient
(which could already be caused by the inexact solution of the \fem).
If we need Hessians or other higher-order derivatives,
then all the mentioned issues even worsen.



\subsection{B-Splines on Sparse Grids for Topology Optimization}
\label{sec:622BSplines}

\blindtext{}



\subsection{Cholesky Factor Interpolation}
\label{sec:623cholesky}

\blindtext{}















































