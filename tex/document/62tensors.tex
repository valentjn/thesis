\section{Approximating Elasticity Tensors}
\label{sec:62tensors}

\paragraph{Optimization process}

During the solution process of \eqref{eq:topoOptProblemDiscrete},
optimization algorithms typically
evaluate the objective function $\compliance(\*x^{(1)}, \dotsc, \*x^{(M)})$
at iteratively different \term{micro-cell parameter combinations}
$(\*x^{(1)}, \dotsc, \*x^{(M)}) \in (\real^d)^M$.
Every evaluation of $\compliance$ corresponds to the solution of a
macro-problem.
However, the elasticity tensors $\etensor^{(q)}$ of all $M$ macro-cells
need to be known to solve the macro-problem.
Hence, in every optimization iteration, it is necessary to solve
one macro-problem and $M$ micro-cell problems,
both with the \fem.
This naive approach has two major drawbacks, which we explain in
the following.



\subsection{Drawbacks of the Naive Approach}
\label{sec:621drawbacks}

\paragraph{Drawback 1: Computation time}

First, this approach is typically computationally infeasible.
The computation of a single elasticity tensor usually takes seconds to
minutes.
All $M$ micro-cell problems per optimization iterations
can be solved in parallel without any
communication (embarrassingly parallel).
However, $M$ is usually in the range of thousands and
there are thousands or tens of thousands optimization iterations
(the optimization problem is $(d \cdot M)$-dimensional!).
This implies that the overall computation may still take
several days to complete.

\paragraph{Drawback 2: Approximation of gradients}

Second, most optimization algorithms require gradients of the
objective function and of the constraints, i.e.,%
\footnote{%
  By convention, the gradient $\nabla_{\*x} f(\*x)$ of a
  scalar-valued function $f$ is the column vector of partial derivatives.
  For vector-valued functions $\*f$, the gradient $\nabla_{\*x} \*f(\*x)$
  is defined as the transposed Jacobian of $\*f$, i.e.,
  the matrix whose columns are the gradients $\nabla_{\*x} f_j(\*x)$ of
  single components of $\*f$.
  If $\*f$ is matrix-valued, then its gradient is defined as the
  gradient of its vectorization.%
}
\begin{equation}
  \nabla_{\*x^{(q)}} \etensor^{(q)}(\*x^{(q)}),\quad
  \nabla_{\*x^{(q)}} \dens^{(q)}(\*x^{(q)}),\qquad
  q = 1, \dotsc, M,
\end{equation}
where $\etensor^{(q)}$ is the elasticity tensor and
$\dens^{(q)}$ is the density of the $q$-th macro-cell.
However, in general, both gradients are unavailable and
have to be approximated by finite differences.
This introduces new error sources and
increases the number of elasticity tensors to be evaluated
by the factor of $2d$ (central differences),
further increasing the time-to-solution.
Additionally, the number of optimization iterations to
achieve convergence might increase,
if there are discontinuities in the objective function
or its gradient
(which could already be caused by the inexact solution of the \fem).
If we need Hessians or other higher-order derivatives,
then all the mentioned issues even worsen.



\subsection{B-Splines on Sparse Grids for Topology Optimization}
\label{sec:622BSplines}

\paragraph{Elasticity tensor function}

As a remedy, we replace the costly evaluation of the
elasticity tensors $\etensor^{(q)}$ with cheap surrogates.
We assume that all macro-cells use the same micro-cell model.
Consequently, the elasticity tensor $\etensor^{(q)}$ of the $q$-th macro cell
with the micro-cell parameter $\*x^{(q)} \in \clint{\*0, \*1}$
can be written as the value $\etensor(\*x^{(q)})$ of some function
$\etensor\colon \clint{\*0, \*1} \to \real^{3 \times 3}$
(for $\dimdomain = 2$) or
$\etensor\colon \clint{\*0, \*1} \to \real^{6 \times 6}$
(for $\dimdomain = 3$) at the point $\*x^{(q)}$.
In the following,
$\etensor\colon \clint{\*0, \*1} \to \real^m$
gives $m \in \nat$ values from which the symmetric elasticity tensor
can be uniquely reconstructed,
e.g., $m = 6$ for $\dimdomain = 2$ and $m = 21$ for $\dimdomain = 3$.
The vector-valued and the matrix-valued version of $\etensor$
will be used interchangeably.

\paragraph{Elasticity tensor surrogate}

The idea is now to use B-splines on sparse grids to approximate
the elasticity tensor function $\etensor$.
In contrast to the theoretical framework that we derived in
\cref{chap:20sparseGrids,chap:30BSplines,chap:40hierarchization},
the function to be interpolated is not scalar-valued, but vector-valued.
This means that we have to construct $m$ sparse grid interpolants
$\etensorentrysgintp{j}$
for the $m$ components $\etensorentry{j}$ of $\etensor$ ($j = 1, \dotsc, m$).
Note that one could generate different sparse grids for the
different components $\etensorentrysgintp{j}$.
However, it is not possible to evaluate only specific entries of $\etensor$
without also evaluating all other entries,
which means that we would waste computation time by selecting only
a subset of the calculated entries.
Therefore, we use the same grid for all components.

Additionally, we approximate the density $\dens^{(q)}$
of the $q$-th macro-cell with a surrogate using
B-splines on the same sparse grid for reasons of implementation,
resulting in $m + 1$ sparse grid interpolants in total.
From a theoretical perspective, this is not necessary,
since the density can be calculated explicitly with simple formulas
for most micro-cell models, independently of evaluations of the
elasticity tensor.

\paragraph{Advantages}

Our approach has multiple obvious advantages:
\begin{itemize}
  \item
  The sparse grid interpolant $\etensorsgintp$ has to be generated only
  once in an \term{offline step} before the optimization algorithm starts.
  During the optimization \term{(online phase)},
  only inexpensive evaluations of $\etensorsgintp$ are performed,
  which saves much time.
  
  \item
  Sparse grids help to reduce the curse of dimensionality, which prohibits
  conventional full grid interpolation methods if $d > 4$.
  
  \item
  With spatially adaptive sparse grids and a suitable refinement criterion,
  we can spend more grid points in regions of interest of $\etensor$,
  e.g., regions with large oscillations.
  
  \item
  By using B-splines as basis functions,
  the interpolant $\etensorsgintp$ will be smoother and more accurate.
  In addition, we can calculate its gradient
  $\nabla_{\*x^{(q)}} \etensorsgintp(\*x^{(q)})$ fast and explicitly,
  accelerating the speed of convergence of the optimization method.
\end{itemize}



\subsection{Cholesky Factor Interpolation}
\label{sec:623cholesky}

Unfortunately, just replacing the elasticity tensor with
the B-spline surrogate often does not lead to correct results in practice.
Experiments show that only for most regular sparse grids and
for some spatially adaptive sparse grids \cite{Valentin16Hierarchical},
the optimization algorithm converges to an optimal point.
The optimization algorithm crashes for most spatially adaptive grids,
not able to find any meaningful optimum.

\paragraph{Positive definiteness of elasticity tensors}

The root of the problem proves to be that for specific
$\*x \in \clint{\*0, \*1}$,
the interpolated elasticity tensors $\etensorsgintp(\*x)$ are not
positive definite.
However, indefinite or even negative definite tensors $\etensorsgintp$
would mimic unphysical behavior.%
\footnote{%
  In the scalar case, this is analogous to Hooke's law for linear springs,
  where the force $F = kx$ needed to displace the end of a spring
  (fixed at the other end) by $x$ is proportional to $x$.
  The proportionality constant $k$ (which corresponds to the elasticity tensor)
  has to be positive.%
}
Therefore, it is imperative for the optimization process that
the interpolated elasticity tensors are \spd.

\blindtext{}

\begin{figure}
  \subcaptionbox{%
    Minimal eigenvalue of $\etensorsgintp(\*x)$%
  }[65mm]{%
    \includegraphics{cholesky_1}%
  }%
  \hfill%
  \subcaptionbox{%
    Minimal eigenvalue of $\etensorcholsgintp(\*x)$%
  }[65mm]{%
    \includegraphics{cholesky_2}%
  }%
  \hfill\hfill%
  \raisebox{2.2mm}{\includegraphics{cholesky_3}}%
  \caption[%
    Minimal eigenvalue of interpolated elasticity tensors%
  ]{%
    Minimal eigenvalue \emph{(colored contour lines)}
    of elasticity tensor surrogates
    for the ``cross'' model ($\dimdomain = 2$, $d = 2$)
    and cubic hierarchical B-splines $\bspl{l,i}{p}$ ($p = 3$) on
    the regular sparse grid $\coarseregsgset{n}{d}{b}$ \emph{(dots)}
    with $n = 7$ and $b = 4$.
    \emph{Left:} The minimal eigenvalue of $\etensorsgintp(\*x)$
    becomes negative in some regions \emph{\textcolor{C1}{(red areas)}}
    of the domain $\clint{\*0, \*1}$,
    indicating that $\etensorsgintp(\*x)$ is not positive definite.
    \emph{Right:} The minimal eigenvalue of $\etensorcholsgintp(\*x)$
    is non-negative in the whole domain $\clint{\*0, \*1}$.%
  }%
  \label{fig:cholesky}%
\end{figure}

\paragraph{Interpolation of Cholesky factors}

\usenotation{Ã‹chol}

\blindtext{}















































