\section{Algorithms}
\label{sec:82algorithms}

\minitoc{83mm}{7}

\noindent
This section gives an overview of the algorithms that
were used to implement the solution process of
the discretized Bellman equation \eqref{eq:gridBellman}.



\subsection{General Structure}
\label{sec:821generalStructure}

The general approach to solve dynamic portfolio choice models is as follows:
\begin{enumerate}
  \item
  Generation of value function interpolants $\valueintp_t$
  (\texttt{solveValueFunction}, \cref{alg:financeSolveValueFunction})
  
  \item
  Generation of optimal policy interpolants $\optpolicyintp_t$
  (\texttt{solvePolicy}, \cref{alg:financeSolvePolicy})
  
  \item
  Computation of Euler equation errors
  (\texttt{computeEulerErrors}, \cref{alg:computeEulerErrors})
\end{enumerate}
The separation of the solution processes for
the value function interpolants $\valueintp_t$
and the optimal policy interpolants $\optpolicyintp_t$
enables to generate different spatially adaptive sparse grids
for the value function and the optimal policies.
This is useful if the shapes of value function and optimal policies
have different characteristics.

In the following \cref{%
  sec:822solveValueFunction,%
  sec:823optimization,%
  sec:824quadrature,%
  sec:825interpolation,%
  sec:826gridGeneration%
}, we describe the algorithmic details of
\texttt{solveValueFunction} (step 1).
The treatment of the other steps \texttt{solvePolicy} (step 2) and
\texttt{computeEulerErrors} (step 3) follows with
\cref{sec:827solvePolicy} and \cref{sec:828eulerEquationErrors},
respectively.

%\paragraph{Interpolants}

We track two interpolants $\valueintp[1]_t$ and $\valueintp[p]_t$
for each $t = 0, \dotsc, T$.
The former interpolates the value function data at the grid points
with the hierarchical piecewise linear basis
(used for the surplus-based grid generation),
while the latter interpolates the data with hierarchical B-splines
of higher order $p > 1$.
Each $\valueintp[\ast]_t$ ($\ast \in \{1, p\}$)
additionally stores the grid points $\state_t^{(k)}$
and the optimal policies $\optpolicyintp_t(\state_t^{(k)})$
at the grid points ($k = 1, \dotsc, \ngp_t$).
For simplicity, we do not pass them as separate data
to the algorithms.



\subsection{Solution for the Value Function}
\label{sec:822solveValueFunction}

\paragraph{\texttt{solveValueFunction} algorithm}

\Cref{alg:financeSolveValueFunction} shows \texttt{solveValueFunction},
generating the value function interpolants
$\valueintp[1]_t$ and $\valueintp[p]_t$ ($t = 0, \dotsc, T$).
The algorithm follows a simple optimize--refine--interpolate scheme:
First, the Bellman equation \eqref{eq:gridBellman} is solved
on an initial sparse grid (\texttt{optimize}).
Then, we \texttt{refine} the grid spatially adaptively.
Finally, the resulting grid data are \texttt{interpolate}d
with hierarchical higher-order B-splines.

\begin{algorithm}
  \begin{algorithmic}[1]
    \Function{%
      $\text{%
        $((\valueintp[1]_t, \valueintp[p]_t))_{t=0,\dotsc,T}$%
      } = \texttt{solveValueFunction}$%
    }{%
      \hspace*{0mm}%
    }
      \State{$\valueintp[p]_{T+1} \gets \emptyset$}
      \Comment{dummy variable (is not used)}%
      \For{$t = T, T - 1, \dotsc, 0$}
        \State{%
          $\valueintp[1]_t \gets \text{%
            Initial regular sparse grid with no values%
          }$%
        }
        \State{%
          $\valueintp[1]_t \gets
          \texttt{optimize($t$, $\valueintp[1]_t$, $\valueintp[p]_{t+1}$)}$%
        }
        \State{%
          $\valueintp[1]_t \gets
          \texttt{refine($t$, $\valueintp[1]_t$, $\valueintp[p]_{t+1}$)}$%
        }
        \State{%
          $\valueintp[p]_t \gets
          \texttt{interpolate($\valueintp[1]_t$)}$%
        }
      \EndFor{}
    \EndFunction{}
  \end{algorithmic}
  \caption[%
    Generation of value function interpolants (\texttt{solveValueFunction})%
  ]{%
    Generation of value function interpolants.
    The output is the higher-order B-spline interpolant $\valueintp[p]_t$
    for all $t = 0, \dotsc, T$.%
  }%
  \label{alg:financeSolveValueFunction}%
\end{algorithm}

At the beginning of every iteration $t$,
the grid of the piecewise linear interpolant is reset
to an initial, possibly regular sparse grid.
It is also possible to reuse the grid from the
previous iteration $t + 1$.
Nevertheless, the results we then obtain become worse,
likely due to the different characteristics of $\valueintp[1]_t$
for different $t$ (e.g., kinks).

The higher-order B-spline interpolant
$\valueintp[p]_{t+1}$ of the previous iteration $t+1$ is used
for the \rhs of the Bellman equation \eqref{eq:gridBellman},
if $t < T$.
In the first iteration $t = T$,
there is no such interpolant.
However,
%it is not needed anyway, as
the exact terminal solution $\valuefcn_T$ is assumed to be known.



\subsection{Optimization}
\label{sec:823optimization}

\paragraph{\texttt{optimize} algorithm}

The \texttt{optimize} step can be seen in \cref{alg:financeOptimize}.
This algorithm accepts in $\valueintp[1]_t$
a spatially adaptive sparse grid
$\gridset{t}{\sparse}
= \{\state_t^{(k)} \mid k = 1, \dotsc, \ngp_t\}$,
where the function values $\valueintp[1]_t(\state_t^{(k)})$
may already be known for some grid points $\state_t^{(k)}$.
\texttt{optimize} computes the missing value function values.
For $t = T$, we assume that the exact terminal solution
$\valuefcn_T$ is known and can be computed explicitly by some function
\texttt{computeKnownTerminalSolution}.%
\footnote{%
  In any case, the terminal solution be be computed as the
  solution of the corresponding single-time optimization problem,
  i.e., $\valuefcn_T(\state_T^{(k)})
  = \max_{\policy_T} \utilityfcn(\consume_T(\state_T^{(k)}, \policy_T))$.%
}
Otherwise, for $t < T$, we solve the Bellman equation
\eqref{eq:gridBellman} by using the higher-order B-spline interpolant
$\valueintp[p]_{t+1}$ of the previous iteration $t + 1$
(\texttt{optimizeSinglePoint}).
The computations for the different $\state_t^{(k)}$ are
independent of each other,
which means that they can be computed in parallel \cite{Horneff16Efficient}.%
\footnote{%
  Such a problem is usually referred to as \term{embarrassingly parallel.}%
}
After generating all missing data,
we update the hierarchical surpluses of the
piecewise linear interpolant $\valueintp[1]_t$
to interpolate the new data at all grid points of $\gridset{t}{\sparse}$.

\begin{algorithm}
  \begin{algorithmic}[1]
    \Function{$\valueintp[1]_t = \texttt{optimize}$}{%
      $t$, $\valueintp[1]_t$, $\valueintp[p]_{t+1}$%
    }
      \State{%
        $(\state_t^{(k)})_{k=1,\dotsc,\ngp_t}
        \gets \text{grid of $\valueintp[1]_t$}$%
      }
      \For{$k = 1, \dotsc, \ngp_t$}
        \If{$\valueintp[1]_t(\state_t^{(k)})$ not previously computed}
          \IfOneLine{$t = T$}{%
            $\valueintp[1]_T(\state_T^{(k)})
            \gets \texttt{computeKnownTerminalSolution($\state_T^{(k)}$)}$%
          }
          \ElseOneLine{%
            $\valueintp[1]_t(\state_t^{(k)})
            \gets \texttt{%
              optimizeSinglePoint(%
                $t$, $\state_t^{(k)}$, $\valueintp[p]_{t+1}$%
              )%
            }$%
          }
        \EndIf{}
      \EndFor{}
      \State{%
        Re-interpolate
        $(\valueintp[1]_t(\state_t^{(k)}))_{k=1,\dotsc,\ngp_t}$
        with piecewise linear functions%
      }
    \EndFunction{}
  \end{algorithmic}
  \caption[Evaluation of the value function (\texttt{optimize})]{%
    Evaluation of the value function at all
    grid points $\state_t^{(k)}$ of $\valueintp[1]_t$
    at which the value function has not been evaluated yet.
    Inputs are
    the time $t$,
    the piecewise linear interpolant $\valueintp[1]_t$
    of the current iteration $t$ (with the underlying sparse grid and
    corresponding function values, possibly unset), and
    the higher-order B-spline interpolant $\valueintp[p]_{t+1}$
    of the previous iteration $t + 1$
    (not used if $t = T$).
    The output is the updated piecewise linear interpolant $\valueintp[1]_t$,
    where all missing function values at grid points have been computed.%
  }%
  \label{alg:financeOptimize}%
\end{algorithm}

\paragraph{Certainty-equivalent transformation}

For utility functions of \crra-type, i.e., of the form
$\utilityfcn(\consume_t) = \consume_t^{1-\riskav}/(1-\riskav)$,
the curvature of the objective function in the Bellman equation
\eqref{eq:gridBellman} can be very high
(depending on the risk aversion parameter $\riskav$),
which may impede convergence of the optimizer.
As a remedy, we transform the value function $\valueintp_t$ with the
\term{certainty-equivalent transformation}
$\valueintp_t \mapsto \cetvalueintp_t
:= ((1 - \riskav) \valueintp_t)^{1/(1-\riskav)}$ if $\riskav > 0$.
\Cref{eq:gridBellman} then becomes
\begin{equation}
  \label{eq:gridBellmanCET}
  \cetvalueintp[1]_t(\state_t^{(k)})
  = \max_{\policy_t} \left(
    \left(
      \consume_t(\state_t^{(k)}, \policy_t)^{1-\riskav} +
      \patience \expectation[t]{
        (
          \cetvalueintp[p]_{t+1}(
            \statefcn_t(\state_t^{(k)}, \policy_t, \stochastic_t)
          )
        )^{1-\riskav}
      }
    \right)^{1/(1-\riskav)}
  \right),
\end{equation}
since for $\riskav > 0$,
$(\cdot)^{1/(1-\riskav)}$ is strictly monotonously decreasing and
$(1-\riskav) < 0$.
%and hence,
%$(\max ((1-\riskav) (\cdot)))^{1/(1-\riskav)}
%= ((1-\riskav) \min (\cdot))^{1/(1-\riskav)}
%= (1-\riskav)^{1/(1-\riskav)} \max ((\cdot)^{1/(1-\riskav)})$.

\paragraph{State space transformation}

Depending on the dynamic portfolio lifecycle model at hand,
it might be beneficial to transform the state space as well
with a \term{state space transformation}
$\statetrafofcn_t\colon \clint{\*0, \*1} \to \real^d$,
for instance, to increase the density of grid points for low
wealth values $\wealth_t$,
where the value function might have a high curvature.
The untransformed states $\state_t^{(k)}$ are then still contained in
the unit cube $\clint{\*0, \*1}$,
and the occurrences of $\state_t^{(k)}$ in \cref{eq:gridBellmanCET}
are replaced with $\statetrafofcn_t(\state_t^{(k)})$.



\subsection{Quadrature}
\label{sec:824quadrature}

In an implementation,
the expectation in \cref{eq:gridBellmanCET} cannot be exactly computed;
it has to be approximated by a quadrature rule
\begin{equation}
  \label{eq:bellmanQuadrature}
  \expectation[t]{
    (
      \cetvalueintp[p]_{t+1}(
        \statefcn_t(\state_t^{(k)}, \policy_t, \stochastic_t)
      )
    )^{1-\riskav}
  }
  \approx \sum_{j=1}^{m_\zeta} \quadweight^{(j)}
  (
    \cetvalueintp[p]_{t+1}(
      \statefcn_t(\state_t^{(k)}, \policy_t, \stochastic_t^{(j)})
    )
  )^{1-\riskav}
\end{equation}
for some weights $\quadweight^{(j)} \in \real$ and
quadrature points $\stochastic_t^{(j)} \in \stochdomain$
($j = 1, \dotsc, m_\zeta$).
Since the stochastic domain $\stochdomain \subset \real^{m_{\stochastic}}$
might be high-dimensional as well,
full grid quadrature rules suffer from the curse of dimensionality.
Therefore, we use sparse grid quadrature rules based
on Gauss--Hermite quadrature \cite{Gerstner98Numerical}.
Note that this sparse grid in the stochastic space $\stochdomain$
is independent of the sparse grid in the state space $\clint{\*0, \*1}$.



\subsection{Interpolation and Extrapolation}
\label{sec:825interpolation}

\paragraph{Sparse grid interpolation}

As already mentioned,
$\valueintp[1]_t$ is constructed as the sparse grid interpolant
of the grid data $\state_t^{(k)}$ ($k = 1, \dotsc, \ngp_t$)
using the hierarchical piecewise linear basis.
For $\valueintp[p]_t$,
we use cubic hierarchical weakly fundamental not-a-knot splines
(see \cref{sec:454wfs}).
The not-a-knot boundary conditions help to decrease the
interpolation error (see \cref{sec:541interpolation}),
while the weakly fundamental property eases the hierarchization
complexity by enabling us to use the unidirectional principle
(see \cref{sec:45spatAdaptiveUP,sec:543complexity}).

\paragraph{Extrapolation}

Unfortunately, for many dynamic portfolio choice models,
state transition is not a function
$\statefcn_t\colon \clint{\*0, \*1} \times \real^{m_{\policy}} \times
\stochdomain \to \clint{\*0, \*1}$:
It may happen that for some quadrature points
$\stochastic_t^{(j)} \in \stochdomain$ in \eqref{eq:bellmanQuadrature},
we have
\begin{equation}
  \statefcn_t(\state_t^{(k)}, \policy_t, \stochastic_t^{(j)})
  \notin \clint{\*0, \*1},
\end{equation}
i.e., $\statefcn_t(\state_t^{(k)}, \policy_t, \stochastic_t^{(j)})$
is not a feasible state.
Hence, we are not able to evaluate the value function interpolant
$\cetvalueintp[p]_{t+1}(
  \statefcn_t(\state_t^{(k)}, \policy_t, \stochastic_t^{(j)})
)$, as it is only defined on $\clint{\*0, \*1}$.
Scaling of the domain is not an option due to the dynamic nature of
the problem.
Modifying infeasible states by cropping them to $\clint{\*0, \*1}$
is not straightforward, as this introduces new errors and
the modified state should satisfy the model constraints.

Instead, we extend the interpolant $\cetvalueintp[p]_{t+1}$
to $\real^d$ by extrapolation methods as shown in
\cref{alg:financeExtrapolate}.
One can choose between three extrapolation \texttt{type}s:
\texttt{constant}, \texttt{linear}, and \texttt{quadratic}.
First, we crop the evaluation point
$\state_{t+1} \in \real^d \setminus \clint{\*0, \*1}$ to
a point $\state_{t+1}^\mathrm{in} \in \clint{\*0, \*1}$.
For \texttt{constant} extrapolation,
we just use $\cetvalueintp[p]_{t+1}(\state_{t+1}^\mathrm{in})$
for the extrapolated value $\cetvalueintp[p]_{t+1}(\state_{t+1})$.
In contrast, \texttt{linear} extrapolation is based on the
Taylor approximation
\begin{subequations}
  \begin{align}
    \cetvalueintp[p]_{t+1}(\state_{t+1})
    &= \cetvalueintp[p]_{t+1}(\state_{t+1}^\mathrm{in}) +
    \tr{(\gradient{\state}{\cetvalueintp[p]_{t+1}}(\state_{t+1}^\mathrm{in}))}
    (\state_{t+1} - \state_{t+1}^\mathrm{in})\\
    &= \cetvalueintp[p]_{t+1}(\state_{t+1}^\mathrm{in}) +
    \sum_{s=1}^d
    \partialderiv{\partialdiff{}\stateentry_s}{\cetvalueintp[p]_{t+1}}(
      \state_{t+1}^\mathrm{in}
    )
    (\stateentry_{t+1,s} - \stateentry_{t+1,s}^\mathrm{in})\\
    &\approx \cetvalueintp[p]_{t+1}(\state_{t+1}^\mathrm{in}) +
    \sum_{s=1}^d
    \sigma_s \frac{
      \cetvalueintp[p]_{t+1}(\state_{t+1}^\mathrm{in}) -
      \cetvalueintp[p]_{t+1}(
        \state_{t+1}^\mathrm{in} - \sigma_s \extpwidth \stdbasis{s}
      )
    }{\extpwidth}
    (\stateentry_{t+1,s} - \stateentry_{t+1,s}^\mathrm{in}),
  \end{align}
\end{subequations}
where
$\sigma_s := 1$, if $\stateentry_{t+1,s} > 1$,
$\sigma_s := -1$, if $\stateentry_{t+1,s} < 0$, and
$\sigma_s := 0$, otherwise,
$\extpwidth \in \pohint{0, 1}$ is the mesh size for the finite differences,
and $\stdbasis{s}$ is the $s$-th standard basis vector.
\texttt{quadratic} extrapolation extrapolates linearly as well,
but uses three (instead of two) points
in $\clint{\*0, \*1}$ to estimate the partial derivative
(i.e., the slope of the value function in the $s$-th direction).
True quadratic extrapolation would require to estimate the
Hessian $\hessian{\state}{\cetvalueintp[p]_{t+1}}$,
for which we would need value function values at
$\binom{d}{2} + 2d + 1$ points (which is quadratic in $d$).

\begin{algorithm}
  \begin{algorithmic}[1]
    \Function{$\cetvalueintp[p]_{t+1}(\state_{t+1}) = \texttt{extrapolate}$}{%
      $\cetvalueintp[p]_{t+1}$, $\state_{t+1}$%
    }
      \State{%
        $\state_{t+1}^\mathrm{in}
        \gets \vecmin(\vecmax(\state_{t+1}, \*0), \*1)$%
      }
      \Comment{%
        nearest point in $\clint{\*0, \*1}$ with respect to $\norm[2]{\cdot}$%
      }%
      \State{%
        $\cetvalueintp[p]_{t+1}(\state_{t+1})
        \gets \cetvalueintp[p]_{t+1}(\state_{t+1}^\mathrm{in})$%
      }
      \If{type $\not=$ constant}
        \For{$s = 1, \dotsc, d$}
          \State{%
            $\sigma_s
            \gets [\stateentry_{t+1,s} > 1] - [\stateentry_{t+1,s} < 0]$%
          }
          \Comment{%
            $1$ if $\stateentry_{t+1,s} > 1$,\;
            $-1$ if $\stateentry_{t+1,s} < 0$,\;
            $0$ otherwise%
          }%
          \If{$\sigma_s \not= 0$}
            \If{type $=$ linear}
              \State{%
                $v_s \gets (
                  \cetvalueintp[p]_{t+1}(\state_{t+1}^\mathrm{in}) -
                  \cetvalueintp[p]_{t+1}(
                    \state_{t+1}^\mathrm{in} -
                    \sigma_s \extpwidth \stdbasis{s}
                  )
                )/\extpwidth$%
              }
            \ElsIf{type $=$ quadratic}
              \State{%
                $D
                \gets \{
                  \stateentry_{t+1,s}^\mathrm{in} - 2 \sigma_s \extpwidth,\;\;
                  \stateentry_{t+1,s}^\mathrm{in} - \sigma_s \extpwidth,\;\;
                  \stateentry_{t+1,s}^\mathrm{in}
                \}$%
              }
              \State{%
                $c_2 \stateentry_s^2 + c_1 \stateentry_s + c_0
                \gets \text{%
                  parabola interpolating
                  $\cetvalueintp[p]_{t+1}(\stateentry_s, \*\state_{t+1,-s})$
                  at $D$%
                }$%
              }
              \label{line:algFinanceExtrapolateParabola}
              \State{%
                $v_s \gets 2 c_2 \cdot \stateentry_{t+1,s}^\mathrm{in} + c_1$%
              }
            \EndIf{}
            \State{%
              $\cetvalueintp[p]_{t+1}(\state_{t+1})
              \gets \cetvalueintp[p]_{t+1}(\state_{t+1}) +
              \sigma_s v_s \cdot
              (\stateentry_{t+1,s} - \stateentry_{t+1,s}^\mathrm{in})$%
            }
          \EndIf{}
        \EndFor{}
      \EndIf{}
    \EndFunction{}
  \end{algorithmic}
  \caption[Extrapolation of value function interpolants]{%
    Evaluation of $\cetvalueintp[p]_{t+1}$ at $\state_{t+1}$
    by extrapolation.
    Inputs are
    the value function interpolant $\cetvalueintp[p]_{t+1}$
    of the next iteration $t + 1$ and
    the state $\state_{t+1} \in \real^d \setminus \clint{\*0, \*1}$
    at which to evaluate the value function.
    The output is
    the value function value $\cetvalueintp[p]_{t+1}(\state_{t+1})$
    at $\state_{t+1}$.
    Note that $\cetvalueintp[p]_{t+1}(\stateentry_s, \*\state_{t+1,-s})$
    in \cref{line:algFinanceExtrapolateParabola} is short for
    the univariate function $\cetvalueintp[p]_{t+1}(
      \state_{t+1,1}, \dotsc, \state_{t+1,s-1},\; {\cdot},\;
      \state_{t+1,s+1}, \dotsc, \state_{t+1,d}
    )$.
    The parameter $\extpwidth$ is the mesh size of the finite differences.%
  }%
  \label{alg:financeExtrapolate}%
\end{algorithm}



\subsection{Grid Generation}
\label{sec:826gridGeneration}

\paragraph{\texttt{refine} algorithm}

%The piecewise linear interpolant is used for the surplus-based
%grid generation,
%as the surpluses are easier to compute in the piecewise linear case,
%and they are more meaningful
%due to the integral representation formula \eqref{eq:surplusIntegral}.

\todo{specify level of initial sparse grid}

\dummytext[1]{}

\begin{algorithm}
  \begin{algorithmic}[1]
    \Function{$\valueintp[1]_t = \texttt{refine}$}{%
      $t$, $\valueintp[1]_t$, $\valueintp[p]_{t+1}$%
    }
      \For{$j = 1, \dotsc, \norefine_t$}
        \State{%
          Refine all grid points of $\valueintp[1]_t$ whose
          surplus is $< \refinetol_t$%
        }
        \IfOneLine{grid of $\valueintp[1]_t$ did not change}{\Break}
        \State{%
          $\valueintp[1]_t \gets
          \texttt{optimize($t$, $\valueintp[1]_t$, $\valueintp[p]_{t+1}$)}$%
        }
      \EndFor{}
    \EndFunction{}
  \end{algorithmic}
  \caption[TODO]{%
    TODO%
  }%
  \label{alg:financeRefine}%
\end{algorithm}

\paragraph{Gradient grids}

\dummytext[1]{}



\subsection{Solution for Optimal Policies}
\label{sec:827solvePolicy}

\dummytext[1]{}



\subsection{Euler Equation Errors}
\label{sec:828eulerEquationErrors}

\dummytext[1]{}
