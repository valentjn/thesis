\section{Algorithms}
\label{sec:82algorithms}

\minitoc{83mm}{7}

\noindent
This section gives an overview of the algorithms that
were used to implement the solution process of
the discretized Bellman equation \eqref{eq:gridBellman}.



\subsection{General Structure}
\label{sec:821generalStructure}

The general approach to solve dynamic portfolio choice models is as follows:
\begin{enumerate}
  \item
  Generation of value function interpolants $\valueintp_t$
  %(\texttt{solveValueFunction}, \cref{alg:financeSolveValueFunction})
  
  \item
  Generation of optimal policy interpolants $\optpolicyintp_t$
  %(\texttt{solvePolicies}, \cref{alg:financeSolvePolicies})
  
  \item
  Post-processing, e.g., Monte Carlo simulation
  %or computation of Euler equation errors
  %(\cref{sec:828postProecessing})
\end{enumerate}
The separation of the solution processes for
the value function interpolants $\valueintp_t$
and the optimal policy interpolants $\optpolicyintp_t$
enables to generate different spatially adaptive sparse grids
for the value function and the optimal policies.
This is useful if the shapes of value function and optimal policies
have different characteristics.

In the following \cref{%
  sec:822solveValueFunction,%
  sec:823optimization,%
  sec:824quadrature,%
  sec:825interpolation,%
  sec:826gridGeneration%
}, we describe the algorithmic details of
\texttt{solveValueFunction} (step 1).
The treatment of the other steps \texttt{solvePolicy} (step 2) and
post-processing (step 3) follows with
\cref{sec:827solvePolicies} and \cref{sec:828postProecessing},
respectively.

%\paragraph{Interpolants}

We track two interpolants $\valueintp[1]_t$ and $\valueintp[p]_t$
for each $t = 0, \dotsc, T$.
The former interpolates the value function data at the grid points
with the hierarchical piecewise linear basis
(used for the surplus-based grid generation),
while the latter interpolates the data with hierarchical B-splines
of higher order $p > 1$.
Each $\valueintp[\ast]_t$ ($\ast \in \{1, p\}$)
additionally stores the grid points $\state_t^{(k)}$
and the optimal policies $\optpolicyintp_t(\state_t^{(k)})$
at the grid points ($k = 1, \dotsc, \ngp_t$).
For simplicity, we do not pass them as separate data
to the algorithms.



\subsection{Solution for the Value Function}
\label{sec:822solveValueFunction}

\paragraph{\texttt{solveValueFunction} algorithm}

\Cref{alg:financeSolveValueFunction} shows \texttt{solveValueFunction},
generating the value function interpolants
$\valueintp[1]_t$ and $\valueintp[p]_t$ ($t = 0, \dotsc, T$).
The algorithm follows a simple optimize--refine--interpolate scheme:
First, the Bellman equation \eqref{eq:gridBellman} is solved
on an initial sparse grid (\texttt{optimize}).
Then, we \texttt{refine} the grid spatially adaptively.
Finally, the resulting grid data are \texttt{interpolate}d
with hierarchical higher-order B-splines.

\begin{algorithm}
  \begin{algorithmic}[1]
    \Function{%
      $\text{%
        $(\valueintp[p]_t)_{t=0,\dotsc,T}$%
      } = \texttt{solveValueFunction}$%
    }{%
      \hspace*{0mm}%
    }
      \State{$\valueintp[p]_{T+1} \gets \emptyset$}
      \Comment{dummy variable (is not used)}%
      \For{$t = T, T - 1, \dotsc, 0$}
        \State{%
          $\valueintp[1]_t \gets \text{%
            Initial regular sparse grid with no values%
          }$%
        }
        \State{%
          $\valueintp[1]_t \gets
          \texttt{optimize($t$, $\valueintp[1]_t$, $\valueintp[p]_{t+1}$)}$%
        }
        \State{%
          $\valueintp[1]_t \gets
          \texttt{refine($t$, $\valueintp[1]_t$, $\valueintp[p]_{t+1}$)}$%
        }
        \State{%
          $\valueintp[p]_t \gets
          \texttt{interpolate($\valueintp[1]_t$)}$%
        }
      \EndFor{}
    \EndFunction{}
  \end{algorithmic}
  \caption[%
    Generation of value function interpolants (\texttt{solveValueFunction})%
  ]{%
    Generation of value function interpolants.
    The output is the higher-order B-spline interpolant $\valueintp[p]_t$
    for all $t = 0, \dotsc, T$.%
  }%
  \label{alg:financeSolveValueFunction}%
\end{algorithm}

At the beginning of every iteration $t$,
the grid of the piecewise linear interpolant is reset
to an initial, possibly regular sparse grid.
It is also possible to reuse the grid from the
previous iteration $t + 1$.
Nevertheless, the results we then obtain become worse,
likely due to the different characteristics of $\valueintp[1]_t$
for different $t$ (e.g., kinks).

The higher-order B-spline interpolant
$\valueintp[p]_{t+1}$ of the previous iteration $t+1$ is used
for the \rhs of the Bellman equation \eqref{eq:gridBellman},
if $t < T$.
In the first iteration $t = T$,
there is no such interpolant.
However,
%it is not needed anyway, as
the exact terminal solution $\valuefcn_T$ is assumed to be known.



\subsection{Optimization}
\label{sec:823optimization}

\paragraph{\texttt{optimize} algorithm}

The \texttt{optimize} step can be seen in \cref{alg:financeOptimize}.
This algorithm accepts in $\valueintp[1]_t$
a spatially adaptive sparse grid
$\gridset{t}{\sparse}
= \{\state_t^{(k)} \mid k = 1, \dotsc, \ngp_t\}$,
where the function values $\valueintp[1]_t(\state_t^{(k)})$
may already be known for some grid points $\state_t^{(k)}$.
\texttt{optimize} computes the missing value function values.
For $t = T$, we assume that the exact terminal solution
$\valuefcn_T$ is known and can be computed explicitly by some function
\texttt{computeKnownTerminalSolution}.%
\footnote{%
  In any case, the terminal solution be be computed as the
  solution of the corresponding single-time optimization problem,
  i.e., $\valuefcn_T(\state_T^{(k)})
  = \max_{\policy_T} \utilityfcn(\consume_T(\state_T^{(k)}, \policy_T))$.%
}
Otherwise, for $t < T$, we solve the Bellman equation
\eqref{eq:gridBellman} by using the higher-order B-spline interpolant
$\valueintp[p]_{t+1}$ of the previous iteration $t + 1$
(\texttt{optimizeSinglePoint}).
The computations for the different $\state_t^{(k)}$ are
independent of each other,
which means that they can be computed in parallel \cite{Horneff16Efficient}.%
\footnote{%
  Such a problem is usually referred to as \term{embarrassingly parallel.}%
}
After generating all missing data,
we update the hierarchical surpluses of the
piecewise linear interpolant $\valueintp[1]_t$
to interpolate the new data at all grid points of $\gridset{t}{\sparse}$.

\begin{algorithm}
  \begin{algorithmic}[1]
    \Function{$\valueintp[1]_t = \texttt{optimize}$}{%
      $t$, $\valueintp[1]_t$, $\valueintp[p]_{t+1}$%
    }
      \State{%
        $(\state_t^{(k)})_{k=1,\dotsc,\ngp_t}
        \gets \text{grid of $\valueintp[1]_t$}$%
      }
      \For{$k = 1, \dotsc, \ngp_t$}
        \If{$\valueintp[1]_t(\state_t^{(k)})$ not previously computed}
          \IfOneLine{$t = T$}{%
            $\valueintp[1]_T(\state_T^{(k)})
            \gets \texttt{computeKnownTerminalSolution($\state_T^{(k)}$)}$%
          }
          \ElseOneLine{%
            $\valueintp[1]_t(\state_t^{(k)})
            \gets \texttt{%
              optimizeSinglePoint(%
                $t$, $\state_t^{(k)}$, $\valueintp[p]_{t+1}$%
              )%
            }$%
          }
        \EndIf{}
      \EndFor{}
      \State{%
        Re-interpolate
        $(\valueintp[1]_t(\state_t^{(k)}))_{k=1,\dotsc,\ngp_t}$
        with piecewise linear functions%
      }
    \EndFunction{}
  \end{algorithmic}
  \caption[Evaluation of the value function (\texttt{optimize})]{%
    Evaluation of the value function at all
    grid points $\state_t^{(k)}$ of $\valueintp[1]_t$
    at which the value function has not been evaluated yet.
    Inputs are
    the time $t$,
    the piecewise linear interpolant $\valueintp[1]_t$
    of the current iteration $t$ (with the underlying sparse grid and
    corresponding function values, possibly unset), and
    the higher-order B-spline interpolant $\valueintp[p]_{t+1}$
    of the previous iteration $t + 1$
    (not used if $t = T$).
    The output is the updated piecewise linear interpolant $\valueintp[1]_t$,
    where all missing function values at grid points have been computed.%
  }%
  \label{alg:financeOptimize}%
\end{algorithm}

\paragraph{Certainty-equivalent transformation}

For utility functions of \crra-type, i.e., of the form
$\utilityfcn(\consume_t) = \consume_t^{1-\riskav}/(1-\riskav)$,
the curvature of the objective function in the Bellman equation
\eqref{eq:gridBellman} can be very high
(depending on the risk aversion parameter $\riskav$),
which may impede convergence of the optimizer.
As a remedy, we transform the value function $\valueintp_t$ with the
\term{certainty-equivalent transformation}
$\valueintp_t \mapsto \cetvalueintp_t
:= ((1 - \riskav) \valueintp_t)^{1/(1-\riskav)}$ if $\riskav > 0$.
\Cref{eq:gridBellman} then becomes
\begin{equation}
  \label{eq:gridBellmanCET}
  \cetvalueintp[1]_t(\state_t^{(k)})
  = \max_{\policy_t} \left(
    \left(
      \consume_t(\state_t^{(k)}, \policy_t)^{1-\riskav} +
      \patience \expectation[t]{
        (
          \cetvalueintp[p]_{t+1}(
            \statefcn_t(\state_t^{(k)}, \policy_t, \stochastic_t)
          )
        )^{1-\riskav}
      }
    \right)^{1/(1-\riskav)}
  \right),
\end{equation}
since for $\riskav > 0$,
$(\cdot)^{1/(1-\riskav)}$ is strictly monotonously decreasing and
$(1-\riskav) < 0$.
For the sake of consistency, the notation in this section
does not distinguish between $\valueintp[\ast]_t$ and $\cetvalueintp[\ast]_t$.
%and hence,
%$(\max ((1-\riskav) (\cdot)))^{1/(1-\riskav)}
%= ((1-\riskav) \min (\cdot))^{1/(1-\riskav)}
%= (1-\riskav)^{1/(1-\riskav)} \max ((\cdot)^{1/(1-\riskav)})$.

\paragraph{State space transformation}

Depending on the dynamic portfolio lifecycle model at hand,
it might be beneficial to transform the state space as well
with a \term{state space transformation}
$\statetrafofcn_t\colon \clint{\*0, \*1} \to \real^d$,
for instance, to increase the density of grid points for low
wealth values $\wealth_t$,
where the value function might have a high curvature.
The untransformed states $\state_t^{(k)}$ are then still contained in
the unit cube $\clint{\*0, \*1}$,
and the occurrences of $\state_t^{(k)}$ in \cref{eq:gridBellmanCET}
are replaced with $\statetrafofcn_t(\state_t^{(k)})$.



\subsection{Quadrature}
\label{sec:824quadrature}

In an implementation,
the expectation in \cref{eq:gridBellmanCET} cannot be exactly computed;
it has to be approximated by a quadrature rule
\begin{equation}
  \label{eq:bellmanQuadrature}
  \expectation[t]{
    (
      \cetvalueintp[p]_{t+1}(
        \statefcn_t(\state_t^{(k)}, \policy_t, \stochastic_t)
      )
    )^{1-\riskav}
  }
  \approx \sum_{j=1}^{m_\zeta} \quadweight^{(j)}
  (
    \cetvalueintp[p]_{t+1}(
      \statefcn_t(\state_t^{(k)}, \policy_t, \stochastic_t^{(j)})
    )
  )^{1-\riskav}
\end{equation}
for some weights $\quadweight^{(j)} \in \real$ and
quadrature points $\stochastic_t^{(j)} \in \stochdomain$
($j = 1, \dotsc, m_\zeta$).
Since the stochastic domain $\stochdomain \subset \real^{m_{\stochastic}}$
might be high-dimensional as well,
full grid quadrature rules suffer from the curse of dimensionality.
Therefore, we use sparse grid quadrature rules based
on Gauss--Hermite quadrature \cite{Gerstner98Numerical}.
Note that this sparse grid in the stochastic space $\stochdomain$
is independent of the sparse grid in the state space $\clint{\*0, \*1}$.



\subsection{Interpolation and Extrapolation}
\label{sec:825interpolation}

\paragraph{Sparse grid interpolation}

As already mentioned,
$\valueintp[1]_t$ is constructed as the sparse grid interpolant
of the grid data $\state_t^{(k)}$ ($k = 1, \dotsc, \ngp_t$)
using the hierarchical piecewise linear basis.
For $\valueintp[p]_t$,
we use cubic hierarchical weakly fundamental not-a-knot splines
(see \cref{sec:454wfs}).
The not-a-knot boundary conditions help to decrease the
interpolation error (see \cref{sec:541interpolation}),
while the weakly fundamental property eases the hierarchization
complexity by enabling us to use the unidirectional principle
(see \cref{sec:45spatAdaptiveUP,sec:543complexity}).

\paragraph{Extrapolation}

Unfortunately, for many dynamic portfolio choice models,
state transition is not a function
$\statefcn_t\colon \clint{\*0, \*1} \times \real^{m_{\policy}} \times
\stochdomain \to \clint{\*0, \*1}$:
It may happen that
$\statefcn_t(\state_t^{(k)}, \policy_t, \stochastic_t^{(j)})
\notin \clint{\*0, \*1}$
for some quadrature points
$\stochastic_t^{(j)} \in \stochdomain$ in \eqref{eq:bellmanQuadrature},
i.e., $\statefcn_t(\state_t^{(k)}, \policy_t, \stochastic_t^{(j)})$
is not a feasible state.
Hence, we might not be able to evaluate the value function interpolant
$\cetvalueintp[p]_{t+1}(
  \statefcn_t(\state_t^{(k)}, \policy_t, \stochastic_t^{(j)})
)$, as it is only defined on $\clint{\*0, \*1}$.
Scaling of the domain is not an option due to the dynamic nature of
the problem.
%Modifying infeasible states by cropping them to $\clint{\*0, \*1}$
%is not straightforward, as this introduces new errors and
%the modified state should satisfy the model constraints.

\paragraph{\texttt{extrapolate} algorithm}

Instead, we extend the interpolant $\cetvalueintp[p]_{t+1}$
to $\real^d$ by extrapolation methods as shown in
\cref{alg:financeExtrapolate}.
One can choose between three extrapolation \texttt{type}s:
\texttt{constant}, \texttt{linear}, and \texttt{quadratic}.
First, we crop the evaluation point
$\state_{t+1} \in \real^d \setminus \clint{\*0, \*1}$ to
a point $\state_{t+1}^\mathrm{in} \in \clint{\*0, \*1}$.
For \texttt{constant} extrapolation,
we just use $\cetvalueintp[p]_{t+1}(\state_{t+1}^\mathrm{in})$
for the extrapolated value $\cetvalueintp[p]_{t+1}(\state_{t+1})$.
In contrast, \texttt{linear} extrapolation is based on the
linear Taylor approximation
\begin{subequations}
  \begin{align}
    \cetvalueintp[p]_{t+1}(\state_{t+1})
    &\approx \cetvalueintp[p]_{t+1}(\state_{t+1}^\mathrm{in}) +
    \tr{(\gradient{\state}{\cetvalueintp[p]_{t+1}}(\state_{t+1}^\mathrm{in}))}
    (\state_{t+1} - \state_{t+1}^\mathrm{in})\\
    &= \cetvalueintp[p]_{t+1}(\state_{t+1}^\mathrm{in}) +
    \sum_{s=1}^d
    \partialderiv{\partialdiff{}\stateentry_s}{\cetvalueintp[p]_{t+1}}(
      \state_{t+1}^\mathrm{in}
    )
    (\stateentry_{t+1,s} - \stateentry_{t+1,s}^\mathrm{in})\\
    &\approx \cetvalueintp[p]_{t+1}(\state_{t+1}^\mathrm{in}) +
    \sum_{s=1}^d
    \sigma_s \frac{
      \cetvalueintp[p]_{t+1}(\state_{t+1}^\mathrm{in}) -
      \cetvalueintp[p]_{t+1}(
        \state_{t+1}^\mathrm{in} - \sigma_s \extpwidth \stdbasis{s}
      )
    }{\extpwidth}
    (\stateentry_{t+1,s} - \stateentry_{t+1,s}^\mathrm{in}),
  \end{align}
\end{subequations}
where
$\sigma_s := 1$, if $\stateentry_{t+1,s} > 1$,
$\sigma_s := -1$, if $\stateentry_{t+1,s} < 0$, and
$\sigma_s := 0$, otherwise,
$\extpwidth \in \pohint{0, 1}$ is the mesh size for the finite differences,
and $\stdbasis{s}$ is the $s$-th standard basis vector.
\texttt{quadratic} extrapolation extrapolates linearly as well,
but uses three (instead of two) points
in $\clint{\*0, \*1}$ to estimate the partial derivative
(i.e., the slope of the value function in the $s$-th direction).
True quadratic extrapolation would require to estimate the
Hessian $\hessian{\state}{\cetvalueintp[p]_{t+1}}$,
for which we would need value function values at
$\binom{d}{2} + 2d + 1$ points (which is quadratic in $d$).

\begin{algorithm}
  \begin{algorithmic}[1]
    \Function{$\cetvalueintp[p]_{t+1}(\state_{t+1}) = \texttt{extrapolate}$}{%
      $\cetvalueintp[p]_{t+1}$, $\state_{t+1}$%
    }
      \State{%
        $\state_{t+1}^\mathrm{in}
        \gets \vecmin(\vecmax(\state_{t+1}, \*0), \*1)$%
      }
      \Comment{%
        nearest point in $\clint{\*0, \*1}$ with respect to $\norm[2]{\cdot}$%
      }%
      \State{%
        $\cetvalueintp[p]_{t+1}(\state_{t+1})
        \gets \cetvalueintp[p]_{t+1}(\state_{t+1}^\mathrm{in})$%
      }
      \If{type $\not=$ constant}
        \For{$s = 1, \dotsc, d$}
          \State{%
            $\sigma_s
            \gets [\stateentry_{t+1,s} > 1] - [\stateentry_{t+1,s} < 0]$%
          }
          \Comment{%
            $1$ if $\stateentry_{t+1,s} > 1$,\;
            $-1$ if $\stateentry_{t+1,s} < 0$,\;
            $0$ otherwise%
          }%
          \If{$\sigma_s \not= 0$}
            \If{type $=$ linear}
              \State{%
                $v_s \gets (
                  \cetvalueintp[p]_{t+1}(\state_{t+1}^\mathrm{in}) -
                  \cetvalueintp[p]_{t+1}(
                    \state_{t+1}^\mathrm{in} -
                    \sigma_s \extpwidth \stdbasis{s}
                  )
                )/\extpwidth$%
              }
            \ElsIf{type $=$ quadratic}
              \State{%
                $D
                \gets \{
                  \stateentry_{t+1,s}^\mathrm{in} - 2 \sigma_s \extpwidth,\;\;
                  \stateentry_{t+1,s}^\mathrm{in} - \sigma_s \extpwidth,\;\;
                  \stateentry_{t+1,s}^\mathrm{in}
                \}$%
              }
              \State{%
                $c_2 \stateentry_s^2 + c_1 \stateentry_s + c_0
                \gets \text{%
                  parabola interpolating
                  $\cetvalueintp[p]_{t+1}(\stateentry_s, \*\state_{t+1,-s})$
                  at $D$%
                }$%
              }
              \label{line:algFinanceExtrapolateParabola}
              \State{%
                $v_s \gets 2 c_2 \cdot \stateentry_{t+1,s}^\mathrm{in} + c_1$%
              }
            \EndIf{}
            \State{%
              $\cetvalueintp[p]_{t+1}(\state_{t+1})
              \gets \cetvalueintp[p]_{t+1}(\state_{t+1}) +
              \sigma_s v_s \cdot
              (\stateentry_{t+1,s} - \stateentry_{t+1,s}^\mathrm{in})$%
            }
          \EndIf{}
        \EndFor{}
      \EndIf{}
    \EndFunction{}
  \end{algorithmic}
  \caption[%
    Extrapolation of value function interpolants (\texttt{extrapolate})%
  ]{%
    Evaluation of $\cetvalueintp[p]_{t+1}$ at $\state_{t+1}$
    by extrapolation.
    Inputs are
    the value function interpolant $\cetvalueintp[p]_{t+1}$
    of the next iteration $t + 1$ and
    the state $\state_{t+1} \in \real^d \setminus \clint{\*0, \*1}$
    at which to evaluate the value function.
    The output is
    the value function value $\cetvalueintp[p]_{t+1}(\state_{t+1})$
    at $\state_{t+1}$.
    Note that $\cetvalueintp[p]_{t+1}(\stateentry_s, \*\state_{t+1,-s})$
    in \cref{line:algFinanceExtrapolateParabola} is short for
    the univariate function $\cetvalueintp[p]_{t+1}(
      \state_{t+1,1}, \dotsc, \state_{t+1,s-1},\; {\cdot},\;
      \state_{t+1,s+1}, \dotsc, \state_{t+1,d}
    )$.
    The parameter $\extpwidth$ is the mesh size of the finite differences.%
  }%
  \label{alg:financeExtrapolate}%
\end{algorithm}



\subsection{Grid Generation}
\label{sec:826gridGeneration}

\paragraph{\texttt{refine} algorithm}

\Cref{alg:financeRefine} shows how to generate the spatially adaptive
sparse grid in \texttt{solveValueFunction}
(\cref{alg:financeSolveValueFunction}).
The criterion is the common surplus-based refinement criterion
\cite{Pflueger10Spatially}.
We use the piecewise linear interpolant is used for the surplus-based
grid generation,
as the surpluses are easier to compute in the piecewise linear case,
and they are more meaningful
due to the integral representation formula \eqref{eq:surplusIntegral}.
Parameters for \cref{alg:financeRefine} are
the tolerance $\refinetol_t \in \nonnegreal$,
by which the set of grid points to be refined is determined, and
the number $\norefine_t \in \natz$ of refinement iterations.
These parameters depend on the time $t$,
since it might be beneficial to change the adaptivity of the
grid over time.

\begin{algorithm}
  \begin{algorithmic}[1]
    \Function{$\valueintp[1]_t = \texttt{refine}$}{%
      $t$, $\valueintp[1]_t$, $\valueintp[p]_{t+1}$%
    }
      \For{$j = 1, \dotsc, \norefine_t$}
        \State{%
          $\ngp_t
          \gets \text{number of grid points of $\valueintp[1]_t$}$%
        }
        \ForOneLine{$k = 1, \dotsc, \ngp_t$}{%
          $\surplus[(k)]{t}
          \gets \text{%
            surplus of $\state_t^{(k)}$ in $\valueintp[1]_t$%
          }$%
        }
        \State{%
          $K_\mathrm{refine}
          \gets \{k = 1, \dotsc, \ngp_t \mid
          \abs{\surplus[(k)]{t}} \ge \refinetol_t\}$%
        }
        \IfOneLine{$K_\mathrm{refine} = \emptyset$}{\Break}
        \State{%
          Refine all grid points in
          $\{\state_t^{(k)} \mid k \in K_\mathrm{refine}\}$%
        }
        \State{%
          $\valueintp[1]_t \gets
          \texttt{optimize($t$, $\valueintp[1]_t$, $\valueintp[p]_{t+1}$)}$%
        }
      \EndFor{}
    \EndFunction{}
  \end{algorithmic}
  \caption[Refinement of the value function (\texttt{refine})]{%
    In-place refinement of the value function $\valueintp[1]_t$.
    Inputs are
    the time $t$,
    the piecewise linear interpolant $\valueintp[1]_t$
    of the current iteration $t$, and
    the higher-order B-spline interpolant $\valueintp[p]_{t+1}$
    of the previous iteration $t + 1$
    (not used if $t = T$).
    The output is the updated piecewise linear interpolant $\valueintp[1]_t$
    with refined sparse grid.%
  }%
  \label{alg:financeRefine}%
\end{algorithm}

\paragraph{Gradient grids}

The classical surplus-refinement criterion puts its emphasis
in regions where the mixed second derivative
$\partialderiv[2d]{
  \partialdiff \stateentry_{t,1}^2 \dotsm \partialdiff \stateentry_{t,d}^2
}{
  \valueintp[1]_t
}$
of $\valueintp[1]_t$ has large absolute values, i.e.,
where $\valueintp[1]_t$ has large high-frequency oscillations.
In gradient-based optimization,
it might be advisable to apply this criterion also
to the partial derivatives
$\partialderiv{\partialdiff \stateentry_{t,s}}{\valueintp[1]_t}$
of $\valueintp[1]_t$ ($s = 1, \dotsc, d$),
since the optimizer depends on the accuracy of the gradient.
In this case, we have to track additional sparse grid interpolants
for every partial derivative
$\partialderiv{\partialdiff \stateentry_{t,s}}{\valueintp[1]_t}$
in \cref{alg:financeSolveValueFunction}.
This possibility is omitted of the algorithms in this section,
as this would complicate their presentation.



\subsection{Solution for Optimal Policies}
\label{sec:827solvePolicies}

\paragraph{\texttt{solvePolicies} algorithm}

After explaining the generation of the value function interpolants
$\valueintp[p]_t$ ($t = 0, \dotsc, T$),
we can move on with step 2 of the general structure of our method
(see \cref{sec:821generalStructure}):
which is the generation of optimal policy interpolants.
The corresponding \cref{alg:financeSolvePolicies} is similar to
\texttt{solveValueFunction} (\cref{alg:financeSolveValueFunction}),
except that it operates on the policy instead of
the value function interpolants.
The functions \texttt{optimize}, \texttt{refine}, and \texttt{interpolate}
have been replaced by corresponding policy versions
\texttt{optimizePolicy}, \texttt{refinePolicy}, and \texttt{interpolatePolicy},
that work very much like their value function counterpart.
\texttt{optimizePolicy} only has to generate new values,
if the initial regular sparse grid for the policies
is not contained in the grid of $\valueintp[p]_t$.
The policy grid is then refined and interpolated independently
of the value function grid.
The iterations are independent of each other,
which means that they can be parallelized.

\begin{algorithm}
  \begin{algorithmic}[1]
    \Function{%
      $\text{%
        $(\optpolicyintp_t)_{t=0,\dotsc,T}$%
      } = \texttt{solvePolicies}$%
    }{%
      $(\valueintp[p]_t)_{t=0,\dotsc,T}$%
    }
      \State{$\valueintp[p]_{T+1} \gets \emptyset$}
      \Comment{dummy variable (is not used)}%
      \For{$t = 0, \dotsc, T$}
        \State{%
          $\optpolicyintp[1]_t \gets \text{%
            Initial regular sparse grid, get values from $\valueintp[p]_t$%
          }$%
        }
        \State{%
          $\optpolicyintp[1]_t \gets
          \texttt{%
            optimizePolicy($t$, $\optpolicyintp[1]_t$, $\valueintp[p]_{t+1}$)%
          }$%
        }
        \State{%
          $\optpolicyintp[1]_t \gets
          \texttt{%
            refinePolicy($t$, $\optpolicyintp[1]_t$, $\valueintp[p]_{t+1}$)%
          }$%
        }
        \State{%
          $\optpolicyintp[p]_t \gets
          \texttt{interpolatePolicy($\optpolicyintp[1]_t$)}$%
        }
      \EndFor{}
    \EndFunction{}
  \end{algorithmic}
  \caption[%
    Generation of interpolants for optimal policies (\texttt{solvePolicies})%
  ]{%
    Generation of interpolants for optimal policies.
    The input is the higher-order B-spline interpolant $\valueintp[p]_t$
    of the value function for all $t = 0, \dotsc, T$.
    The output is the higher-order B-spline interpolant $\optpolicyintp[p]_t$
    of the optimal policies for all $t = 0, \dotsc, T$.%
  }%
  \label{alg:financeSolvePolicies}%
\end{algorithm}



\subsection{Post-Processing}
\label{sec:828postProecessing}

\paragraph{Monte Carlo simulation}

There are various ways to
assess if the resulting optimal policy B-spline interpolants
$(\optpolicyintp[p]_t)_{t=0,\dotsc,T}$
are reasonable.
One possibility is a Monte Carlo simulation,
where we calculate the mean optimal policy
\begin{equation}
  \optpolicymean_t
  = \frac{1}{m_\mathrm{MC}} \sum_{j=1}^{m_\mathrm{MC}} \policy_{t,(j)}
\end{equation}
for $m_\mathrm{MC} \in \nat$ individuals.
The optimal policies $\policy_{t,(j)}$ of the individuals
($t = 0, \dotsc, T$ and $j = 1, \dotsc, m_\mathrm{MC}$)
are determined by
\begin{subequations}
  \begin{align}
    \policy_{t,(j)}
    &:= \optpolicyintp[p]_t(\state_{t,(j)}),\\
    \state_{t,(j)}
    &:= \statefcn_{t-1}(
      \state_{t-1,(j)}, \policy_{t-1,(j)}, \stochastic_{t-1,(j)}
    ),\quad
    t > 0,\qquad
    \state_{t,(0)}
    \sim P_{0,\state},\\
    \stochastic_{t,(j)}
    &\centerhphantom{\sim}{\hspace*{1.6em}} P_{t,\stochastic},
  \end{align}
\end{subequations}
i.e., the initial state $\state_{t,(0)}$ and the
stochastic variables $\stochastic_{t,(j)}$ are samples of random variables.
The Monte Carlo simulation enables us to draw macro-economic conclusions.
(For instance, how does the amount of consumption typically evolve over time?)

%\paragraph{Euler equation errors}
%
%\dummytext[1]{}
