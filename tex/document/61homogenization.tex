\section{Homogenization and the Two-Scale Approach}
\label{sec:61homogenization}

We roughly follow the presentation given in
\multicite{%
  Huebner14Mehrdimensionale,%
  Valentin14Hierarchische,%
  Valentin16Hierarchical%
}.



\subsection{Homogenization}
\label{sec:611homogenization}

\paragraph{Density function}

Let $\domain \subset \real^{\dimdomain}$ be the optimization domain.
Usually, we assume $\dimdomain = 2$ or $\dimdomain = 3$,
although the method can be generalized to
arbitrary dimensionalities $\dimdomain \in \nat$.
Shapes and topologies are described by \term{density functions}
$\dens\colon \domain \to \clint{0, 1}$.
The function values $\dens(\tilde{\*x}) \in \clint{0, 1}$ tell if $\tilde{\*x}$
is contained in the object (value of one) or not (value of zero).
The \term{homogenization} approach also allows values between
zero and one, giving the physical density of the material in $\tilde{\*x}$.

\paragraph{Optimization of compliance values}

Furthermore, for every density function $\dens$,
let $\compliance(\dens)$ be an objective function value.
In our setting, which is shown in \cref{fig:topoOptExampleScenario},
we exert a force $\force$ (or multiple forces) on the object,
measure the resulting deformation, and
compute the \term{compliance} (i.e., the inverse of the stiffness) as
the objective function value~$\compliance(\dens)$:
\begin{equation}
  \compliance(\dens)
  = \int_{\domain} \innerprod[2]{\force}{\displacement_{\dens}(\tilde{\*x})}
  \diff\tilde{\*x},
\end{equation}
where the \term{displacement function}
$\displacement_{\dens}\colon \domain \to \real^{\dimdomain}$
depends on the density \cite{Huebner14Mehrdimensionale}.
We want to find the density function
with the minimal objective function value:
\begin{equation}
  \label{eq:topoOptProblemContinuous}
  \min_{\dens} \compliance(\dens).
\end{equation}
There are often uninteresting trivial solutions,
if we do not impose additional conditions.
For example, when minimizing compliance function values,
choosing $\dens :\equiv 1$
(i.e., filling the entire domain $\domain$ with material)
usually leads to the topology with the
highest stiffness and, thus, the smallest displacement and compliance value.
Therefore, we introduce the following volume constraint:
\begin{equation}
  \frac{\voldens{\dens}{\domain}}{\vol{\domain}} \le \densub,\quad
  \voldens{\dens}{\domain}
  := \int_{\domain} \dens(\tilde{\*x}) \diff\tilde{\*x},\quad
  \vol{\domain}
  := \voldens{1}{\domain},
\end{equation}
where $\vol{\domain} = \int_{\domain} 1 \diff\tilde{\*x}$
is the volume of the domain and
$\densub \in \clint{0, 1}$ is an upper bound on the volume fraction.

\begin{SCfigure}
  \includegraphics{topoOptScenarios_1}%
  \caption[%
    Example scenario for topology optimization%
  ]{%
    Example scenario for topology optimization.
    An object \emph{\textcolor{hellblau}{(light blue)}}
    is fixed on the left side
    of the domain $\domain$
    \emph{\textcolor{mittelblau!50}{(darker blue)}}
    and deformed by a force $\force$, resulting in a displaced object
    \emph{(dashed)}.
    The density function $\dens(\tilde{\*x})$ is one inside the object
    and zero outside.%
  }%
  \label{fig:topoOptExampleScenario}%
\end{SCfigure}



\subsection{Two-Scale Approach}
\label{sec:612twoScale}

\paragraph{Discretization and two-scale approach}

Of course, we cannot solve the problem \eqref{eq:topoOptProblemContinuous}
numerically,
as there are infinitely many density functions $\dens$.
%To be able to discretize the domain $\domain \subset \real^{\dimdomain}$
%more easily,
For simplicity, we assume that $\domain$ is some hyper-rectangle
$\clint{\tilde{\*a}, \tilde{\*b}}
= \clint{\tilde{a}_1, \tilde{b}_1} \times \dotsb \times
\clint{\tilde{a}_{\dimdomain}, \tilde{b}_{\dimdomain}}$;
if it is not, we can replace $\domain$ with its bounding box.
The domain $\domain$ can then be split into
$M_1 \times \dotsb \times M_{\dimdomain}$
equally sized and axis-aligned sub-hyper-rectangles,
which we call \term{macro-cells}.

In the \term{two-scale approach},
we assume the material of the macro-cells to be
repetitions of infinitesimally small periodic structures
(i.e., identical for each macro-cell),
called \term{micro-cells}.
These micro-cells have a specific shape, which is parametrized by
$d$ \term{micro-cell parameters} $x_1, \dotsc, x_d$.
The parameters are assumed to be normalized to values in the
unit interval $\clint{0, 1}$.
For instance, in two dimensions,
this shape may be an axis-aligned cross of two bars
with thicknesses $x_1$ and $x_2$, as shown in \cref{fig:twoScale}.

\begin{figure}
  \includegraphics{twoScale_1}%
  \caption[%
    Two-scale approach for topology optimization%
  ]{%
    Two-scale approach to discretize the homogenized topology
    optimization problem in two dimensions ($\dimdomain = 2$).
    \emph{Left:} The domain $\domain$ is subdivided into $M_1 \times M_2$
    macro-cells, each with its own material properties such as density
    \emph{(colors)}.
    \emph{Center:} Every macro-cell is the repetition of infinitesimally small
    periodic micro-cells.
    \emph{Right:} The shape of the structure in every micro-cell is
    described by a micro-cell model with $d$ parameters $x_1, \dotsc, x_d$.
    Here, the micro-cell model is a cross with two parameters
    that represent the thickness of each crossbar.%
  }%
  \label{fig:twoScale}%
\end{figure}

\paragraph{Elasticity tensors}

Note that while the shape of all micro-cells in one macro-cell is identical,
the micro-cell parameters corresponding to different macro-cells differ
in general.
This enables varying densities in different regions of $\domain$.
We denote the parameters of the $j$-th macro-cell
with $\*x^{(j)} = (x_1^{(j)}, \dotsc, x_d^{(j)}) \in
\clint{\*0, \*1} = \clint{0, 1}^d$,
where $j = 1, \dotsc, M$ and
$M := M_1 \dotsm M_{\dimdomain}$ is the number of macro-cells.
With linear elasticity,
one can compute so-called \term{elasticity tensors} $\etensor_j$,
which encode information about the material properties
of the different macro-cells.
The elasticity tensors can be written as symmetric matrices
in $\real^{3 \times 3}$ (for $\dimdomain = 2$) or
in $\real^{6 \times 6}$ (for $\dimdomain = 3$).%
\footnote{%
  In general, the elasticity tensor is a fourth-order tensor in
  $\real^{\dimdomain \times \dimdomain \times \dimdomain \times \dimdomain}$.
  One can reduce the size of the tensor by exploiting various symmetries
  \cite{Huebner14Mehrdimensionale}
  to obtain $6$ or $21$ stiffness coefficients
  in two or three dimensions, respectively,
  which can then be written as a symmetric matrix.%
}
They are usually computed as the solution of a \fem problem
\term{(micro-problem)}.
Once all elasticity tensors $\etensor_j$ are known,
we can compute the compliance value corresponding to the macro-shape
by solving another \fem problem \term{(macro-problem)}.
We refer to \cite{Allaire04Topology} and \cite{Huebner14Mehrdimensionale}
for more details.

\paragraph{Discretized optimization problem}

The new optimization problem following from the
two-scale discretization process has the form
\begin{subequations}
  \label{eq:topoOptProblemDiscrete}
  \begin{gather}
    \min J(\*x^{(1)}, \dotsc, \*x^{(M)}),\quad
    \*x^{(1)}, \dotsc, \*x^{(M)} \in \clint{0, 1}^d
    \quad\text{s.t.}\quad
    \densmean(\*x^{(1)}, \dotsc, \*x^{(M)}) \le \densub,\\
    \densmean(\*x^{(1)}, \dotsc, \*x^{(M)})
    := \frac{1}{M} \sum_{j=1}^M \dens(\*x^{(j)}).
  \end{gather}
\end{subequations}
Here, $\dens(\*x^{(j)}) \in \clint{0, 1}$ is the
density of the $j$-th macro cell with micro-cell parameter $\*x^{(j)}$
(i.e., the fraction of material volume of one micro-cell
with respect to its total volume)
and $\densmean(\*x^{(1)}, \dotsc, \*x^{(M)})$ is the resulting
total mean density.
This discretized optimization problem can now be implemented and
solved numerically.



\subsection{Optimization Process and Drawbacks of the Naive Approach}
\label{sec:613optimization}

\paragraph{Optimization process}

During the solution process of \eqref{eq:topoOptProblemDiscrete},
optimization algorithms typically
evaluate the objective function $\compliance(\*x^{(1)}, \dotsc, \*x^{(M)})$
at iteratively different \term{micro-cell parameter combinations}
$(\*x^{(1)}, \dotsc, \*x^{(M)}) \in (\real^d)^M$.
Every evaluation of $\compliance$ corresponds to the solution of a
macro-problem.
However, the elasticity tensors $\etensor_j$ of all $M$ macro-cells
need to be known to solve the macro-problem.
Hence, in every optimization iteration, it is necessary to solve
one macro-problem and $M$ micro-cell problems,
both with the \fem.
This naive approach has two major drawbacks.

\paragraph{Drawback 1: Computation time}

First, this approach is typically computationally infeasible.
The computation of a single elasticity tensor usually takes seconds to
minutes.
All $M$ micro-cell problems per optimization iterations
can be solved in parallel without any
communication (embarrassingly parallel).
However, $M$ is usually in the range of thousands and
there are thousands or tens of thousands optimization iterations
(the optimization problem is $(d \cdot M)$-dimensional!).
This implies that the overall computation may still take
several days to complete.

\paragraph{Drawback 2: Approximation of gradients}

Second, most optimization algorithms require gradients of the
objective function and of the constraints, i.e.,%
\footnote{%
  By convention, the gradient $\nabla_{\*x} f(\*x)$ of a
  scalar-valued function $f$ is the column vector of partial derivatives.
  For vector-valued functions $\*f$, the gradient $\nabla_{\*x} \*f(\*x)$
  is defined as the transposed Jacobian, i.e., the matrix whose columns
  are the gradients $\nabla_{\*x} f_j(\*x)$ of single components of
  $\*f$.%
}
\begin{equation}
  \nabla_{\*x^{(j)}} \etensor_j(\*x^{(j)}),\quad
  \nabla_{\*x^{(j)}} \dens_j(\*x^{(j)}),\qquad
  j = 1, \dotsc, M,
\end{equation}
where $\etensor_j$ is the elasticity tensor and
$\dens_j$ is the density of the $j$-th macro-cell.
However, in general, both gradients are unavailable and
have to be approximated by finite differences.
This introduces new error sources and
increases the number of elasticity tensors to be evaluated
by the factor of $2d$ (central differences),
further increasing the time-to-solution.
Additionally, the number of optimization iterations to
achieve convergence might increase,
if there are discontinuities in the objective function
or its gradient
(which could already be caused by the inexact solution of the \fem).
If we need Hessians or other higher-order derivatives,
then all the mentioned issues even worsen.
